[0m12:31:40.892529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10436f620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10537f890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10537fb10>]}


============================== 12:31:40.894649 | d1c8b2b9-71fe-4f18-87b9-c6d23418c3b4 ==============================
[0m12:31:40.894649 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:31:40.894872 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'invocation_command': 'dbt debug', 'printer_width': '80', 'quiet': 'False', 'version_check': 'True', 'warn_error': 'None', 'empty': 'None', 'use_colors': 'True', 'partial_parse': 'True', 'log_cache_events': 'False', 'static_parser': 'True', 'fail_fast': 'False', 'log_format': 'default', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'introspect': 'True', 'write_json': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs'}
[0m12:31:40.901798 [info ] [MainThread]: dbt version: 1.10.9
[0m12:31:40.901926 [info ] [MainThread]: python version: 3.13.0
[0m12:31:40.902013 [info ] [MainThread]: python path: /Users/artakerqeli/flights_dbt/.venv/bin/python3.13
[0m12:31:40.902090 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m12:31:41.192990 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:31:41.193183 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:31:41.193278 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:31:41.558967 [info ] [MainThread]: Using profiles dir at /Users/artakerqeli/.dbt
[0m12:31:41.559159 [info ] [MainThread]: Using profiles.yml file at /Users/artakerqeli/.dbt/profiles.yml
[0m12:31:41.559263 [info ] [MainThread]: Using dbt_project.yml file at /Users/artakerqeli/flights_dbt/flights_dbt/dbt_project.yml
[0m12:31:41.559349 [info ] [MainThread]: adapter type: databricks
[0m12:31:41.559430 [info ] [MainThread]: adapter version: 1.10.11
[0m12:31:41.592267 [info ] [MainThread]: Configuration:
[0m12:31:41.592472 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:31:41.592577 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:31:41.592655 [info ] [MainThread]: Required dependencies:
[0m12:31:41.592795 [debug] [MainThread]: Executing "git --help"
[0m12:31:41.615368 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:31:41.615788 [debug] [MainThread]: STDERR: "b''"
[0m12:31:41.615897 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:31:41.616018 [info ] [MainThread]: Connection:
[0m12:31:41.616147 [info ] [MainThread]:   host: https://dbc-8fd4d429-f882.cloud.databricks.com
[0m12:31:41.616238 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/2444393c45259518
[0m12:31:41.616319 [info ] [MainThread]:   catalog: main
[0m12:31:41.616392 [info ] [MainThread]:   schema: flights_dbt
[0m12:31:41.616703 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:31:41.686512 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m12:31:41.686767 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m12:31:41.686872 [debug] [MainThread]: Using databricks connection "debug"
[0m12:31:41.686962 [debug] [MainThread]: On debug: select 1 as id
[0m12:31:41.687039 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:32:53.507258 [error] [MainThread]: Encountered an error:

[0m12:32:53.529802 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
    ~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 1093, in _validate_conn
    conn.connect()
    ~~~~~~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connection.py", line 790, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
        sock=sock,
    ...<14 lines>...
        assert_fingerprint=self.assert_fingerprint,
    )
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connection.py", line 969, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
        sock=sock,
    ...<8 lines>...
        tls_in_tls=tls_in_tls,
    )
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/util/ssl_.py", line 480, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/util/ssl_.py", line 524, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        sock=sock,
        ^^^^^^^^^^
    ...<5 lines>...
        session=session
        ^^^^^^^^^^^^^^^
    )
    ^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py", line 1076, in _create
    self.do_handshake()
    ~~~~~~~~~~~~~~~~~^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py", line 1372, in do_handshake
    self._sslobj.do_handshake()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1020)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 488, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1020)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ~~~~^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/main.py", line 420, in debug
    results = task.run()
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/task/debug.py", line 144, in run
    connection_status = self.test_connection()
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/task/debug.py", line 465, in test_connection
    connection_result = self.attempt_connection(self.profile)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/task/debug.py", line 443, in attempt_connection
    adapter.debug_query()
    ~~~~~~~~~~~~~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/spark/impl.py", line 519, in debug_query
    self.execute("select 1 as id")
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/impl.py", line 322, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt_common/record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/base/impl.py", line 449, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/connections.py", line 293, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/connections.py", line 266, in add_query
    handle: DatabricksHandle = connection.handle
                               ^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/contracts/connection.py", line 96, in handle
    self._handle.resolve(self)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/contracts/connection.py", line 120, in resolve
    return self.opener(connection)
           ~~~~~~~~~~~^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/connections.py", line 421, in open
    return cls.retry_connection(
           ~~~~~~~~~~~~~~~~~~~~^
        connection,
        ^^^^^^^^^^^
    ...<4 lines>...
        retry_timeout=(timeout if timeout is not None else exponential_backoff),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/base/connections.py", line 237, in retry_connection
    connection.handle = connect()
                        ~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/connections.py", line 400, in connect
    conn = DatabricksHandle.from_connection_args(
        conn_args,
        is_cluster_http_path(databricks_connection.http_path, creds.cluster_id),
    )
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/handle.py", line 211, in from_connection_args
    conn = dbsql.connect(**conn_args)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/__init__.py", line 90, in connect
    return Connection(server_hostname, http_path, access_token, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/client.py", line 278, in __init__
    self._open_session_resp = self.thrift_backend.open_session(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        session_configuration, catalog, schema
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/thrift_backend.py", line 562, in open_session
    response = self.make_request(self._client.OpenSession, open_session_req)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/thrift_backend.py", line 479, in make_request
    response_or_error_info = attempt_request(attempt)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/thrift_backend.py", line 386, in attempt_request
    response = method(request)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/thrift_api/TCLIService/TCLIService.py", line 204, in OpenSession
    self.send_OpenSession(req)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/thrift_api/TCLIService/TCLIService.py", line 213, in send_OpenSession
    self._oprot.trans.flush()
    ~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/auth/thrift_http_client.py", line 186, in flush
    self.__resp = self.__pool.request(
                  ~~~~~~~~~~~~~~~~~~~^
        "POST",
        ^^^^^^^
    ...<5 lines>...
        retries=self.retry_policy,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        method, url, fields=fields, headers=headers, **urlopen_kw
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  [Previous line repeated 2 more times]
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 844, in urlopen
    retries.sleep()
    ~~~~~~~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/util/retry.py", line 363, in sleep
    self._sleep_backoff()
    ~~~~~~~~~~~~~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/util/retry.py", line 347, in _sleep_backoff
    time.sleep(backoff)
    ~~~~~~~~~~^^^^^^^^^
KeyboardInterrupt

[0m12:32:53.537719 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 72.675896, "process_in_blocks": "0", "process_kernel_time": 0.234508, "process_mem_max_rss": "220069888", "process_out_blocks": "0", "process_user_time": 0.966997}
[0m12:32:53.539917 [debug] [MainThread]: Command `dbt debug` failed at 12:32:53.539804 after 72.68 seconds
[0m12:32:53.540644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c04e780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c151a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0f8af0>]}
[0m12:32:53.541066 [debug] [MainThread]: Flushing usage events
[0m12:32:54.149056 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:33:53.225964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041e7620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051f7890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051f7b10>]}


============================== 12:33:53.228196 | d4c10c76-c798-4ec7-958e-7577bf491cb2 ==============================
[0m12:33:53.228196 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:33:53.228415 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'quiet': 'False', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'write_json': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'log_cache_events': 'False', 'warn_error': 'None', 'printer_width': '80', 'use_colors': 'True', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'empty': 'None', 'log_format': 'default', 'debug': 'False', 'no_print': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'indirect_selection': 'eager', 'fail_fast': 'False', 'use_experimental_parser': 'False'}
[0m12:33:53.234846 [info ] [MainThread]: dbt version: 1.10.9
[0m12:33:53.234961 [info ] [MainThread]: python version: 3.13.0
[0m12:33:53.235048 [info ] [MainThread]: python path: /Users/artakerqeli/flights_dbt/.venv/bin/python3.13
[0m12:33:53.235124 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m12:33:53.236197 [info ] [MainThread]: target not specified in profile 'flights_dbt', using 'default'
[0m12:33:53.236325 [info ] [MainThread]: Using profiles dir at /Users/artakerqeli/.dbt
[0m12:33:53.236403 [info ] [MainThread]: Using profiles.yml file at /Users/artakerqeli/.dbt/profiles.yml
[0m12:33:53.236474 [info ] [MainThread]: Using dbt_project.yml file at /Users/artakerqeli/flights_dbt/flights_dbt/dbt_project.yml
[0m12:33:53.267810 [info ] [MainThread]: Configuration:
[0m12:33:53.268006 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m12:33:53.268105 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:33:53.268189 [info ] [MainThread]: Required dependencies:
[0m12:33:53.268324 [debug] [MainThread]: Executing "git --help"
[0m12:33:53.283421 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:33:53.283760 [debug] [MainThread]: STDERR: "b''"
[0m12:33:53.283881 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:33:53.283979 [info ] [MainThread]: Connection test skipped since no profile was found
[0m12:33:53.284078 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:33:53.284157 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  The profile 'flights_dbt' does not have a target named 'default'. The valid target names for this profile are:
   - dev


[0m12:33:53.285857 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.090515874, "process_in_blocks": "0", "process_kernel_time": 0.102811, "process_mem_max_rss": "109641728", "process_out_blocks": "0", "process_user_time": 0.476147}
[0m12:33:53.286095 [debug] [MainThread]: Command `dbt debug` failed at 12:33:53.286047 after 0.09 seconds
[0m12:33:53.286280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104528c30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105375eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10509f570>]}
[0m12:33:53.286435 [debug] [MainThread]: Flushing usage events
[0m12:33:53.842705 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:36:14.660480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a3b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a4b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a4bb10>]}


============================== 12:36:14.662661 | a97de7c4-5104-4d23-ace4-ed132827ba57 ==============================
[0m12:36:14.662661 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:36:14.662915 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'empty': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'introspect': 'True', 'no_print': 'None', 'warn_error': 'None', 'quiet': 'False', 'target_path': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug --target dev', 'version_check': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'partial_parse': 'True', 'use_colors': 'True', 'log_cache_events': 'False', 'debug': 'False'}
[0m12:36:14.669709 [info ] [MainThread]: dbt version: 1.10.9
[0m12:36:14.669852 [info ] [MainThread]: python version: 3.13.0
[0m12:36:14.669945 [info ] [MainThread]: python path: /Users/artakerqeli/flights_dbt/.venv/bin/python3.13
[0m12:36:14.670023 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m12:36:14.957635 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:36:14.957817 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:36:14.957909 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:36:15.318203 [info ] [MainThread]: Using profiles dir at /Users/artakerqeli/.dbt
[0m12:36:15.318393 [info ] [MainThread]: Using profiles.yml file at /Users/artakerqeli/.dbt/profiles.yml
[0m12:36:15.318495 [info ] [MainThread]: Using dbt_project.yml file at /Users/artakerqeli/flights_dbt/flights_dbt/dbt_project.yml
[0m12:36:15.318582 [info ] [MainThread]: adapter type: databricks
[0m12:36:15.318665 [info ] [MainThread]: adapter version: 1.10.11
[0m12:36:15.351211 [info ] [MainThread]: Configuration:
[0m12:36:15.351393 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:36:15.351492 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:36:15.351573 [info ] [MainThread]: Required dependencies:
[0m12:36:15.351702 [debug] [MainThread]: Executing "git --help"
[0m12:36:15.368876 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:36:15.369250 [debug] [MainThread]: STDERR: "b''"
[0m12:36:15.369352 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:36:15.369445 [info ] [MainThread]: Connection:
[0m12:36:15.369565 [info ] [MainThread]:   host: https://dbc-8fd4d429-f882.cloud.databricks.com
[0m12:36:15.369648 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/2444393c45259518
[0m12:36:15.369731 [info ] [MainThread]:   catalog: main
[0m12:36:15.369806 [info ] [MainThread]:   schema: flights_dbt
[0m12:36:15.370060 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:36:15.411613 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m12:36:15.411876 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m12:36:15.411985 [debug] [MainThread]: Using databricks connection "debug"
[0m12:36:15.412069 [debug] [MainThread]: On debug: select 1 as id
[0m12:36:15.412147 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:36:58.364649 [error] [MainThread]: Encountered an error:

[0m12:36:58.383800 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
    ~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 1093, in _validate_conn
    conn.connect()
    ~~~~~~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connection.py", line 790, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
        sock=sock,
    ...<14 lines>...
        assert_fingerprint=self.assert_fingerprint,
    )
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connection.py", line 969, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
        sock=sock,
    ...<8 lines>...
        tls_in_tls=tls_in_tls,
    )
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/util/ssl_.py", line 480, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/util/ssl_.py", line 524, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        sock=sock,
        ^^^^^^^^^^
    ...<5 lines>...
        session=session
        ^^^^^^^^^^^^^^^
    )
    ^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py", line 1076, in _create
    self.do_handshake()
    ~~~~~~~~~~~~~~~~~^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py", line 1372, in do_handshake
    self._sslobj.do_handshake()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1020)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 488, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1020)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ~~~~^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/main.py", line 420, in debug
    results = task.run()
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/task/debug.py", line 144, in run
    connection_status = self.test_connection()
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/task/debug.py", line 465, in test_connection
    connection_result = self.attempt_connection(self.profile)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/task/debug.py", line 443, in attempt_connection
    adapter.debug_query()
    ~~~~~~~~~~~~~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/spark/impl.py", line 519, in debug_query
    self.execute("select 1 as id")
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/impl.py", line 322, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt_common/record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/base/impl.py", line 449, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/connections.py", line 293, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/connections.py", line 266, in add_query
    handle: DatabricksHandle = connection.handle
                               ^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/contracts/connection.py", line 96, in handle
    self._handle.resolve(self)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/contracts/connection.py", line 120, in resolve
    return self.opener(connection)
           ~~~~~~~~~~~^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/connections.py", line 421, in open
    return cls.retry_connection(
           ~~~~~~~~~~~~~~~~~~~~^
        connection,
        ^^^^^^^^^^^
    ...<4 lines>...
        retry_timeout=(timeout if timeout is not None else exponential_backoff),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/base/connections.py", line 237, in retry_connection
    connection.handle = connect()
                        ~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/connections.py", line 400, in connect
    conn = DatabricksHandle.from_connection_args(
        conn_args,
        is_cluster_http_path(databricks_connection.http_path, creds.cluster_id),
    )
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/handle.py", line 211, in from_connection_args
    conn = dbsql.connect(**conn_args)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/__init__.py", line 90, in connect
    return Connection(server_hostname, http_path, access_token, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/client.py", line 278, in __init__
    self._open_session_resp = self.thrift_backend.open_session(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        session_configuration, catalog, schema
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/thrift_backend.py", line 562, in open_session
    response = self.make_request(self._client.OpenSession, open_session_req)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/thrift_backend.py", line 479, in make_request
    response_or_error_info = attempt_request(attempt)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/thrift_backend.py", line 386, in attempt_request
    response = method(request)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/thrift_api/TCLIService/TCLIService.py", line 204, in OpenSession
    self.send_OpenSession(req)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/thrift_api/TCLIService/TCLIService.py", line 213, in send_OpenSession
    self._oprot.trans.flush()
    ~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/databricks/sql/auth/thrift_http_client.py", line 186, in flush
    self.__resp = self.__pool.request(
                  ~~~~~~~~~~~~~~~~~~~^
        "POST",
        ^^^^^^^
    ...<5 lines>...
        retries=self.retry_policy,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        method, url, fields=fields, headers=headers, **urlopen_kw
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  [Previous line repeated 1 more time]
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 844, in urlopen
    retries.sleep()
    ~~~~~~~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/util/retry.py", line 363, in sleep
    self._sleep_backoff()
    ~~~~~~~~~~~~~~~~~~~^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/urllib3/util/retry.py", line 347, in _sleep_backoff
    time.sleep(backoff)
    ~~~~~~~~~~^^^^^^^^^
KeyboardInterrupt

[0m12:36:58.390585 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 43.758865, "process_in_blocks": "0", "process_kernel_time": 0.210829, "process_mem_max_rss": "219463680", "process_out_blocks": "0", "process_user_time": 0.957066}
[0m12:36:58.391497 [debug] [MainThread]: Command `dbt debug` failed at 12:36:58.391402 after 43.76 seconds
[0m12:36:58.391869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117122650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117325490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1171c09e0>]}
[0m12:36:58.392161 [debug] [MainThread]: Flushing usage events
[0m12:37:12.721169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106663620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107833890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107833b10>]}


============================== 12:37:12.723397 | a7e5c171-cb60-4cfb-837f-1956bc029857 ==============================
[0m12:37:12.723397 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:37:12.723620 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'static_parser': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'printer_width': '80', 'invocation_command': 'dbt debug --target dev', 'log_format': 'default', 'cache_selected_only': 'False', 'quiet': 'False', 'no_print': 'None', 'target_path': 'None', 'partial_parse': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'fail_fast': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error': 'None', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'debug': 'False'}
[0m12:37:12.730465 [info ] [MainThread]: dbt version: 1.10.9
[0m12:37:12.730605 [info ] [MainThread]: python version: 3.13.0
[0m12:37:12.730694 [info ] [MainThread]: python path: /Users/artakerqeli/flights_dbt/.venv/bin/python3.13
[0m12:37:12.730773 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m12:37:13.021711 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:37:13.021892 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:37:13.021983 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:37:13.396599 [info ] [MainThread]: Using profiles dir at /Users/artakerqeli/.dbt
[0m12:37:13.396807 [info ] [MainThread]: Using profiles.yml file at /Users/artakerqeli/.dbt/profiles.yml
[0m12:37:13.396909 [info ] [MainThread]: Using dbt_project.yml file at /Users/artakerqeli/flights_dbt/flights_dbt/dbt_project.yml
[0m12:37:13.396993 [info ] [MainThread]: adapter type: databricks
[0m12:37:13.397078 [info ] [MainThread]: adapter version: 1.10.11
[0m12:37:13.430978 [info ] [MainThread]: Configuration:
[0m12:37:13.431188 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:37:13.431292 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:37:13.431373 [info ] [MainThread]: Required dependencies:
[0m12:37:13.431503 [debug] [MainThread]: Executing "git --help"
[0m12:37:13.445429 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:37:13.445801 [debug] [MainThread]: STDERR: "b''"
[0m12:37:13.445913 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:37:13.446020 [info ] [MainThread]: Connection:
[0m12:37:13.446150 [info ] [MainThread]:   host: https://dbc-8fd4d429-f882.cloud.databricks.com
[0m12:37:13.446235 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/2444393c45259518
[0m12:37:13.446313 [info ] [MainThread]:   catalog: main
[0m12:37:13.446470 [info ] [MainThread]:   schema: flights_dbt
[0m12:37:13.446829 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:37:13.491443 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m12:37:13.491699 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m12:37:13.491815 [debug] [MainThread]: Using databricks connection "debug"
[0m12:37:13.491905 [debug] [MainThread]: On debug: select 1 as id
[0m12:37:13.491988 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:37:14.347701 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0897b-1f15-1eb8-b11c-508973f96b97) - Created
[0m12:37:15.021201 [debug] [MainThread]: SQL status: OK in 1.530 seconds
[0m12:37:15.023160 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0897b-1f15-1eb8-b11c-508973f96b97, command-id=01f0897b-1f35-1855-aac6-63b7d1be8932) - Closing
[0m12:37:15.023919 [debug] [MainThread]: On debug: Close
[0m12:37:15.024186 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0897b-1f15-1eb8-b11c-508973f96b97) - Closing
[0m12:37:15.205766 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m12:37:15.206729 [info ] [MainThread]: [32mAll checks passed![0m
[0m12:37:15.213600 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 2.521132, "process_in_blocks": "0", "process_kernel_time": 0.206135, "process_mem_max_rss": "223903744", "process_out_blocks": "0", "process_user_time": 0.984439}
[0m12:37:15.214452 [debug] [MainThread]: Command `dbt debug` succeeded at 12:37:15.214191 after 2.52 seconds
[0m12:37:15.215092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e361ba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e32b410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e0978a0>]}
[0m12:37:15.215537 [debug] [MainThread]: Flushing usage events
[0m12:37:15.758381 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:37:46.769384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083d3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095ab890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095abb10>]}


============================== 12:37:46.771538 | ea1ff178-b290-44f3-b321-0929231c2cde ==============================
[0m12:37:46.771538 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:37:46.771768 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'log_format': 'default', 'log_cache_events': 'False', 'static_parser': 'True', 'empty': 'False', 'write_json': 'True', 'no_print': 'None', 'invocation_command': 'dbt run --target dev', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'debug': 'False', 'partial_parse': 'True', 'warn_error': 'None', 'target_path': 'None', 'quiet': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'version_check': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'use_colors': 'True', 'use_experimental_parser': 'False'}
[0m12:37:47.064092 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:37:47.064290 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:37:47.064393 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:37:47.494492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ea1ff178-b290-44f3-b321-0929231c2cde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088db360>]}
[0m12:37:47.512949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ea1ff178-b290-44f3-b321-0929231c2cde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7b6ad0>]}
[0m12:37:47.513188 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:37:47.563230 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:37:47.563556 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:37:47.563682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ea1ff178-b290-44f3-b321-0929231c2cde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff73550>]}
[0m12:37:48.031634 [error] [MainThread]: Encountered an error:
Compilation Error in model bronze_airports (models/bronze/bronze_airports.sql)
  Invalid inline model config
[0m12:37:48.034276 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2930819, "process_in_blocks": "0", "process_kernel_time": 0.207129, "process_mem_max_rss": "227524608", "process_out_blocks": "0", "process_user_time": 1.387277}
[0m12:37:48.034508 [debug] [MainThread]: Command `dbt run` failed at 12:37:48.034466 after 1.29 seconds
[0m12:37:48.034674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1103f89b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1103f85f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10945fe70>]}
[0m12:37:48.034793 [debug] [MainThread]: Flushing usage events
[0m12:37:48.664081 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:38:59.478559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103eab620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ec3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ec3b10>]}


============================== 12:38:59.480816 | fb6ef7bf-aaa0-4a3a-b662-617c80609b30 ==============================
[0m12:38:59.480816 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:38:59.481081 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'use_experimental_parser': 'False', 'version_check': 'True', 'cache_selected_only': 'False', 'use_colors': 'True', 'target_path': 'None', 'empty': 'False', 'quiet': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'log_cache_events': 'False', 'warn_error': 'None', 'static_parser': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'introspect': 'True', 'log_format': 'default', 'partial_parse': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'invocation_command': 'dbt run --target dev'}
[0m12:38:59.774776 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:38:59.774987 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:38:59.775088 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:39:00.203832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fb6ef7bf-aaa0-4a3a-b662-617c80609b30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041f3360>]}
[0m12:39:00.222677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fb6ef7bf-aaa0-4a3a-b662-617c80609b30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060cead0>]}
[0m12:39:00.222914 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:39:00.270763 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:39:00.271067 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:39:00.271191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fb6ef7bf-aaa0-4a3a-b662-617c80609b30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9d3550>]}
[0m12:39:00.744514 [error] [MainThread]: Encountered an error:
Compilation Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  Invalid inline model config
[0m12:39:00.746384 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2965463, "process_in_blocks": "0", "process_kernel_time": 0.21459, "process_mem_max_rss": "228032512", "process_out_blocks": "0", "process_user_time": 1.394365}
[0m12:39:00.746608 [debug] [MainThread]: Command `dbt run` failed at 12:39:00.746571 after 1.30 seconds
[0m12:39:00.746762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be548c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be55e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d77e70>]}
[0m12:39:00.746877 [debug] [MainThread]: Flushing usage events
[0m12:39:01.374334 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:39:52.537511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ad3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088fbb10>]}


============================== 12:39:52.539871 | 88df382a-16ea-4351-bd91-ffb6db00bbd5 ==============================
[0m12:39:52.539871 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:39:52.540093 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'empty': 'False', 'partial_parse': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'target_path': 'None', 'no_print': 'None', 'debug': 'False', 'write_json': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'log_format': 'default', 'invocation_command': 'dbt run --target dev', 'introspect': 'True', 'version_check': 'True', 'printer_width': '80', 'static_parser': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'log_cache_events': 'False', 'warn_error': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True'}
[0m12:39:52.831746 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:39:52.831939 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:39:52.832040 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:39:53.248750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '88df382a-16ea-4351-bd91-ffb6db00bbd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10792b360>]}
[0m12:39:53.267364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '88df382a-16ea-4351-bd91-ffb6db00bbd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a902ad0>]}
[0m12:39:53.267590 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:39:53.316379 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:39:53.316686 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:39:53.316805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '88df382a-16ea-4351-bd91-ffb6db00bbd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119853550>]}
[0m12:39:53.778029 [error] [MainThread]: Encountered an error:
Compilation Error in model bronze_flights (models/bronze/bronze_flights.sql)
  Invalid inline model config
[0m12:39:53.780536 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.271917, "process_in_blocks": "0", "process_kernel_time": 0.211394, "process_mem_max_rss": "227622912", "process_out_blocks": "0", "process_user_time": 1.375579}
[0m12:39:53.780771 [debug] [MainThread]: Command `dbt run` failed at 12:39:53.780735 after 1.27 seconds
[0m12:39:53.780932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119cd5c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119cd7200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087afe70>]}
[0m12:39:53.781040 [debug] [MainThread]: Flushing usage events
[0m12:39:54.339871 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:41:00.877626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061b3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078fbb10>]}


============================== 12:41:00.879922 | 11a690f5-21bb-406c-a84d-6ba5a92fa493 ==============================
[0m12:41:00.879922 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:41:00.880153 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'version_check': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'printer_width': '80', 'invocation_command': 'dbt run --target dev', 'use_experimental_parser': 'False', 'warn_error': 'None', 'write_json': 'True', 'indirect_selection': 'eager', 'empty': 'False', 'introspect': 'True', 'log_format': 'default', 'quiet': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'partial_parse': 'True', 'no_print': 'None', 'use_colors': 'True', 'static_parser': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs'}
[0m12:41:01.170272 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:41:01.170459 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:41:01.170552 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:41:01.624154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '11a690f5-21bb-406c-a84d-6ba5a92fa493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10682b360>]}
[0m12:41:01.642698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '11a690f5-21bb-406c-a84d-6ba5a92fa493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112106ad0>]}
[0m12:41:01.642968 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:41:01.694568 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:41:01.694873 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:41:01.695008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '11a690f5-21bb-406c-a84d-6ba5a92fa493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117d53550>]}
[0m12:41:02.166884 [error] [MainThread]: Encountered an error:
Compilation Error in model silver_flights (models/silver/silver_flights.sql)
  Invalid inline model config
[0m12:41:02.168910 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3201152, "process_in_blocks": "0", "process_kernel_time": 0.218334, "process_mem_max_rss": "227655680", "process_out_blocks": "0", "process_user_time": 1.385722}
[0m12:41:02.169149 [debug] [MainThread]: Command `dbt run` failed at 12:41:02.169114 after 1.32 seconds
[0m12:41:02.169304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12122f200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12122da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077afe70>]}
[0m12:41:02.169415 [debug] [MainThread]: Flushing usage events
[0m12:41:02.747270 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:42:26.552753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10388b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048a3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048a3b10>]}


============================== 12:42:26.555047 | 693b1d8f-f24a-4edc-ae37-2d0e1de1bacf ==============================
[0m12:42:26.555047 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:42:26.555281 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'printer_width': '80', 'static_parser': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'partial_parse': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'debug': 'False', 'log_cache_events': 'False', 'version_check': 'True', 'write_json': 'True', 'no_print': 'None', 'invocation_command': 'dbt run --target dev', 'use_experimental_parser': 'False', 'empty': 'False', 'indirect_selection': 'eager', 'fail_fast': 'False'}
[0m12:42:26.846324 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:42:26.846566 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:42:26.846675 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:42:27.273369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '693b1d8f-f24a-4edc-ae37-2d0e1de1bacf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bd3360>]}
[0m12:42:27.292000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '693b1d8f-f24a-4edc-ae37-2d0e1de1bacf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105aaead0>]}
[0m12:42:27.292226 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:42:27.341027 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:42:27.341481 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:42:27.341609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '693b1d8f-f24a-4edc-ae37-2d0e1de1bacf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b84f550>]}
[0m12:42:27.813315 [error] [MainThread]: Encountered an error:
Compilation Error in model silver_airports (models/silver/silver_airports.sql)
  Invalid inline model config
[0m12:42:27.815737 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2961323, "process_in_blocks": "0", "process_kernel_time": 0.209749, "process_mem_max_rss": "226426880", "process_out_blocks": "0", "process_user_time": 1.388703}
[0m12:42:27.815987 [debug] [MainThread]: Command `dbt run` failed at 12:42:27.815949 after 1.30 seconds
[0m12:42:27.816144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bed5e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bed5a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104757e70>]}
[0m12:42:27.816251 [debug] [MainThread]: Flushing usage events
[0m12:42:28.456094 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:43:10.964631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ccf620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ea7890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ea7b10>]}


============================== 12:43:10.966904 | 949d5513-ebfb-454f-8295-9ddc75f6a6eb ==============================
[0m12:43:10.966904 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:43:10.967154 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'use_experimental_parser': 'False', 'log_format': 'default', 'no_print': 'None', 'write_json': 'True', 'version_check': 'True', 'invocation_command': 'dbt run --target dev', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'partial_parse': 'True', 'target_path': 'None', 'empty': 'False', 'printer_width': '80', 'fail_fast': 'False', 'debug': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'static_parser': 'True', 'use_colors': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'cache_selected_only': 'False'}
[0m12:43:11.255251 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:43:11.255446 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:43:11.255542 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:43:11.675180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '949d5513-ebfb-454f-8295-9ddc75f6a6eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081d7360>]}
[0m12:43:11.693954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '949d5513-ebfb-454f-8295-9ddc75f6a6eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a0aead0>]}
[0m12:43:11.694182 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:43:11.742834 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:43:11.743181 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:43:11.743311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '949d5513-ebfb-454f-8295-9ddc75f6a6eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f873550>]}
[0m12:43:12.212116 [error] [MainThread]: Encountered an error:
Compilation Error in model silver_airlines (models/silver/silver_airlines.sql)
  Invalid inline model config
[0m12:43:12.214198 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2786891, "process_in_blocks": "0", "process_kernel_time": 0.20737, "process_mem_max_rss": "228278272", "process_out_blocks": "0", "process_user_time": 1.387683}
[0m12:43:12.214639 [debug] [MainThread]: Command `dbt run` failed at 12:43:12.214598 after 1.28 seconds
[0m12:43:12.214808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087aa3f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcec7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d5be70>]}
[0m12:43:12.214930 [debug] [MainThread]: Flushing usage events
[0m12:43:12.742863 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:43:54.401469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061cb620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071e3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071e3b10>]}


============================== 12:43:54.403591 | 4cce5b66-8274-48aa-8aa1-c7ba9943d16b ==============================
[0m12:43:54.403591 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:43:54.403831 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'quiet': 'False', 'write_json': 'True', 'debug': 'False', 'warn_error': 'None', 'static_parser': 'True', 'invocation_command': 'dbt run --target dev', 'log_cache_events': 'False', 'log_format': 'default', 'indirect_selection': 'eager', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'partial_parse': 'True', 'fail_fast': 'False', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'no_print': 'None', 'empty': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'printer_width': '80', 'use_colors': 'True'}
[0m12:43:54.692932 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:43:54.693179 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:43:54.693301 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:43:55.108730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4cce5b66-8274-48aa-8aa1-c7ba9943d16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106513360>]}
[0m12:43:55.127504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4cce5b66-8274-48aa-8aa1-c7ba9943d16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083eead0>]}
[0m12:43:55.127743 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:43:55.175632 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:43:55.175928 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:43:55.176049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4cce5b66-8274-48aa-8aa1-c7ba9943d16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dcd7550>]}
[0m12:43:55.825210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4cce5b66-8274-48aa-8aa1-c7ba9943d16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ae63f0>]}
[0m12:43:55.859274 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m12:43:55.860130 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m12:43:55.865489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4cce5b66-8274-48aa-8aa1-c7ba9943d16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2a92b0>]}
[0m12:43:55.865647 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m12:43:55.865746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4cce5b66-8274-48aa-8aa1-c7ba9943d16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e1eb860>]}
[0m12:43:55.866472 [info ] [MainThread]: 
[0m12:43:55.866587 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:43:55.866672 [info ] [MainThread]: 
[0m12:43:55.866848 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:43:55.866941 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:43:55.869385 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_main) - Creating connection
[0m12:43:55.869564 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_main) - Creating connection
[0m12:43:55.869662 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m12:43:55.870051 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_main) - Creating connection
[0m12:43:55.870193 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m12:43:55.870345 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_main) - Creating connection
[0m12:43:55.873643 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m12:43:55.873767 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m12:43:55.874641 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m12:43:55.874750 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m12:43:55.874863 [debug] [ThreadPool]: On list_main: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_main"} */

    show databases
  
[0m12:43:55.875549 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m12:43:55.875653 [debug] [ThreadPool]: On list_main: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_main"} */

    show databases
  
[0m12:43:55.876282 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m12:43:55.876374 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:43:55.876472 [debug] [ThreadPool]: On list_main: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_main"} */

    show databases
  
[0m12:43:55.876667 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:43:55.876768 [debug] [ThreadPool]: On list_main: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_main"} */

    show databases
  
[0m12:43:55.877262 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:43:55.877396 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:43:56.754310 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-0eef-1e26-a497-d60c494583f4) - Created
[0m12:43:56.766898 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-0ef4-1346-856b-8c287d683980) - Created
[0m12:43:56.787783 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-0ef3-1e28-9514-637d5aa37a98) - Created
[0m12:43:56.792340 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-0ef5-104f-b0e4-e49293d846fe) - Created
[0m12:43:57.645072 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_main"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-0f0c-1dce-89e4-d54dd984fdf2
[0m12:43:57.647166 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:43:57.647688 [debug] [ThreadPool]: On list_main: Close
[0m12:43:57.647965 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-0eef-1e26-a497-d60c494583f4) - Closing
[0m12:43:57.653119 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_main"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-0f15-137b-bec0-fce97a39788e
[0m12:43:57.654690 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:43:57.674062 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_main"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-0f15-15a9-927a-2c4560ea0a60
[0m12:43:57.676400 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:43:57.820046 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_main"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-0f13-1e66-b9cc-8849f329e77e
[0m12:43:57.822507 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:43:57.843844 [debug] [ThreadPool]: On list_main: Close
[0m12:43:57.844293 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-0ef3-1e28-9514-637d5aa37a98) - Closing
[0m12:43:58.043184 [debug] [ThreadPool]: On list_main: Close
[0m12:43:58.044286 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-0ef4-1346-856b-8c287d683980) - Closing
[0m12:43:58.249087 [debug] [ThreadPool]: On list_main: Close
[0m12:43:58.250093 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-0ef5-104f-b0e4-e49293d846fe) - Closing
[0m12:43:58.452046 [info ] [MainThread]: 
[0m12:43:58.452792 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 2.59 seconds (2.59s).
[0m12:43:58.453547 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:43:58.460448 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.0857153, "process_in_blocks": "0", "process_kernel_time": 0.242723, "process_mem_max_rss": "242302976", "process_out_blocks": "0", "process_user_time": 2.222968}
[0m12:43:58.461112 [debug] [MainThread]: Command `dbt run` failed at 12:43:58.461003 after 4.09 seconds
[0m12:43:58.461564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f9ae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e260e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2610d0>]}
[0m12:43:58.461918 [debug] [MainThread]: Flushing usage events
[0m12:43:59.074372 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:47:15.615290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d77620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d87890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d87b10>]}


============================== 12:47:15.617484 | 041cb480-b381-4598-8646-41cb62541564 ==============================
[0m12:47:15.617484 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:47:15.617713 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'static_parser': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'log_cache_events': 'False', 'printer_width': '80', 'invocation_command': 'dbt debug --target dev', 'use_colors': 'True', 'debug': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'introspect': 'True', 'write_json': 'True', 'warn_error': 'None', 'partial_parse': 'True', 'version_check': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager'}
[0m12:47:15.624194 [info ] [MainThread]: dbt version: 1.10.9
[0m12:47:15.624320 [info ] [MainThread]: python version: 3.13.0
[0m12:47:15.624417 [info ] [MainThread]: python path: /Users/artakerqeli/flights_dbt/.venv/bin/python3.13
[0m12:47:15.624501 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m12:47:15.917435 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:47:15.917626 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:47:15.917716 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:47:16.294777 [info ] [MainThread]: Using profiles dir at /Users/artakerqeli/.dbt
[0m12:47:16.294985 [info ] [MainThread]: Using profiles.yml file at /Users/artakerqeli/.dbt/profiles.yml
[0m12:47:16.295102 [info ] [MainThread]: Using dbt_project.yml file at /Users/artakerqeli/flights_dbt/flights_dbt/dbt_project.yml
[0m12:47:16.295194 [info ] [MainThread]: adapter type: databricks
[0m12:47:16.295272 [info ] [MainThread]: adapter version: 1.10.11
[0m12:47:16.328383 [info ] [MainThread]: Configuration:
[0m12:47:16.328591 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:47:16.328691 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:47:16.328775 [info ] [MainThread]: Required dependencies:
[0m12:47:16.328924 [debug] [MainThread]: Executing "git --help"
[0m12:47:16.346033 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:47:16.346445 [debug] [MainThread]: STDERR: "b''"
[0m12:47:16.346569 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:47:16.346672 [info ] [MainThread]: Connection:
[0m12:47:16.346801 [info ] [MainThread]:   host: https://dbc-8fd4d429-f882.cloud.databricks.com
[0m12:47:16.346889 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/2444393c45259518
[0m12:47:16.346968 [info ] [MainThread]:   catalog: raw
[0m12:47:16.347046 [info ] [MainThread]:   schema: flights_dbt
[0m12:47:16.347311 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:47:16.389928 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m12:47:16.390291 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m12:47:16.390418 [debug] [MainThread]: Using databricks connection "debug"
[0m12:47:16.390504 [debug] [MainThread]: On debug: select 1 as id
[0m12:47:16.390581 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:47:17.113065 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0897c-865c-135b-95ef-24104924b0eb) - Created
[0m12:47:17.904290 [debug] [MainThread]: SQL status: OK in 1.510 seconds
[0m12:47:17.906093 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0897c-865c-135b-95ef-24104924b0eb, command-id=01f0897c-86b5-168e-831e-67a1a40fea4a) - Closing
[0m12:47:17.906815 [debug] [MainThread]: On debug: Close
[0m12:47:17.907093 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0897c-865c-135b-95ef-24104924b0eb) - Closing
[0m12:47:18.101488 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m12:47:18.102527 [info ] [MainThread]: [32mAll checks passed![0m
[0m12:47:18.110408 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 2.5236833, "process_in_blocks": "0", "process_kernel_time": 0.213439, "process_mem_max_rss": "224493568", "process_out_blocks": "0", "process_user_time": 0.978298}
[0m12:47:18.111335 [debug] [MainThread]: Command `dbt debug` succeeded at 12:47:18.111194 after 2.52 seconds
[0m12:47:18.111917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bac5ba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba83410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b7eb8a0>]}
[0m12:47:18.112429 [debug] [MainThread]: Flushing usage events
[0m12:47:18.762122 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:47:36.172164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062a7620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10747f890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10747fb10>]}


============================== 12:47:36.174269 | 1afde4f9-d4ed-49af-b0ba-50b3bbab9214 ==============================
[0m12:47:36.174269 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:47:36.174489 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'indirect_selection': 'eager', 'static_parser': 'True', 'partial_parse': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error': 'None', 'debug': 'False', 'target_path': 'None', 'empty': 'False', 'write_json': 'True', 'use_colors': 'True', 'fail_fast': 'False', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'introspect': 'True', 'no_print': 'None', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --target dev', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt'}
[0m12:47:36.462341 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:47:36.462548 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:47:36.462648 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:47:36.885399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1afde4f9-d4ed-49af-b0ba-50b3bbab9214', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067ab360>]}
[0m12:47:36.904076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1afde4f9-d4ed-49af-b0ba-50b3bbab9214', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108686ad0>]}
[0m12:47:36.904311 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:47:36.956772 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:47:36.992324 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m12:47:36.992533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1afde4f9-d4ed-49af-b0ba-50b3bbab9214', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e207450>]}
[0m12:47:37.632953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1afde4f9-d4ed-49af-b0ba-50b3bbab9214', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e460f50>]}
[0m12:47:37.663976 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m12:47:37.664973 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m12:47:37.669442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1afde4f9-d4ed-49af-b0ba-50b3bbab9214', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2b73f0>]}
[0m12:47:37.669599 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m12:47:37.669703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1afde4f9-d4ed-49af-b0ba-50b3bbab9214', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e41b380>]}
[0m12:47:37.670421 [info ] [MainThread]: 
[0m12:47:37.670532 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:47:37.670616 [info ] [MainThread]: 
[0m12:47:37.670782 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:47:37.670871 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:47:37.673248 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_raw) - Creating connection
[0m12:47:37.673388 [debug] [ThreadPool]: Acquiring new databricks connection 'list_raw'
[0m12:47:37.673532 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_raw) - Creating connection
[0m12:47:37.676875 [debug] [ThreadPool]: Using databricks connection "list_raw"
[0m12:47:37.677056 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_raw) - Creating connection
[0m12:47:37.677194 [debug] [ThreadPool]: Acquiring new databricks connection 'list_raw'
[0m12:47:37.677372 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_raw) - Creating connection
[0m12:47:37.677502 [debug] [ThreadPool]: On list_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_raw"} */

    show databases
  
[0m12:47:37.677601 [debug] [ThreadPool]: Acquiring new databricks connection 'list_raw'
[0m12:47:37.678400 [debug] [ThreadPool]: Using databricks connection "list_raw"
[0m12:47:37.678508 [debug] [ThreadPool]: Acquiring new databricks connection 'list_raw'
[0m12:47:37.678601 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:37.679244 [debug] [ThreadPool]: Using databricks connection "list_raw"
[0m12:47:37.679335 [debug] [ThreadPool]: On list_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_raw"} */

    show databases
  
[0m12:47:37.679949 [debug] [ThreadPool]: Using databricks connection "list_raw"
[0m12:47:37.680397 [debug] [ThreadPool]: On list_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_raw"} */

    show databases
  
[0m12:47:37.680598 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:37.680698 [debug] [ThreadPool]: On list_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_raw"} */

    show databases
  
[0m12:47:37.680790 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:37.680934 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:38.451526 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-9315-197b-b6a5-770c6561b896) - Created
[0m12:47:38.454010 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-9318-1369-8449-0f65582716bf) - Created
[0m12:47:38.464238 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-931a-10d7-900d-deff367ecce1) - Created
[0m12:47:38.476409 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-931a-1f01-8836-96888f93c042) - Created
[0m12:47:39.041956 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_raw"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-9332-15a0-89da-e2d8d04f7026
[0m12:47:39.044162 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:47:39.044677 [debug] [ThreadPool]: On list_raw: Close
[0m12:47:39.044980 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-9318-1369-8449-0f65582716bf) - Closing
[0m12:47:39.096143 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_raw"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-9335-1a48-8e30-29dfc2985e2a
[0m12:47:39.099557 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_raw"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-9331-1f2a-9f27-3d2537c26012
[0m12:47:39.101186 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:47:39.101601 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:47:39.206161 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_raw"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-9337-1156-aba3-75ad9ce9f530
[0m12:47:39.208655 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:47:39.238062 [debug] [ThreadPool]: On list_raw: Close
[0m12:47:39.238915 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-931a-10d7-900d-deff367ecce1) - Closing
[0m12:47:39.459562 [debug] [ThreadPool]: On list_raw: Close
[0m12:47:39.460252 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-9315-197b-b6a5-770c6561b896) - Closing
[0m12:47:39.664647 [debug] [ThreadPool]: On list_raw: Close
[0m12:47:39.665589 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-931a-1f01-8836-96888f93c042) - Closing
[0m12:47:39.876239 [info ] [MainThread]: 
[0m12:47:39.876992 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 2.21 seconds (2.21s).
[0m12:47:39.877636 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    [NO_SUCH_CATALOG_EXCEPTION] Catalog 'raw' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:47:39.885070 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.7394454, "process_in_blocks": "0", "process_kernel_time": 0.235866, "process_mem_max_rss": "244957184", "process_out_blocks": "0", "process_user_time": 2.277807}
[0m12:47:39.885997 [debug] [MainThread]: Command `dbt run` failed at 12:47:39.885803 after 3.74 seconds
[0m12:47:39.886519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107236e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7d8470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7d8310>]}
[0m12:47:39.886929 [debug] [MainThread]: Flushing usage events
[0m12:47:40.493089 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:49:09.857489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040af620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105287890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105287b10>]}


============================== 12:49:09.859762 | 4534bb24-b666-45ba-8486-fd0c9b3b17b6 ==============================
[0m12:49:09.859762 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:49:09.859997 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'indirect_selection': 'eager', 'empty': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run --target dev', 'introspect': 'True', 'target_path': 'None', 'quiet': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'debug': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'version_check': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'printer_width': '80', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'no_print': 'None'}
[0m12:49:10.151718 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:49:10.151896 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:49:10.151991 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:49:10.564823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4534bb24-b666-45ba-8486-fd0c9b3b17b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045b3360>]}
[0m12:49:10.583319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4534bb24-b666-45ba-8486-fd0c9b3b17b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10648aad0>]}
[0m12:49:10.583553 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:49:10.632808 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:49:10.668804 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m12:49:10.668998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4534bb24-b666-45ba-8486-fd0c9b3b17b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c00b450>]}
[0m12:49:11.299940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4534bb24-b666-45ba-8486-fd0c9b3b17b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c10f3e0>]}
[0m12:49:11.331646 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m12:49:11.332685 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m12:49:11.338296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4534bb24-b666-45ba-8486-fd0c9b3b17b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0caeb0>]}
[0m12:49:11.338459 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m12:49:11.338565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4534bb24-b666-45ba-8486-fd0c9b3b17b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c253a00>]}
[0m12:49:11.339281 [info ] [MainThread]: 
[0m12:49:11.339396 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:49:11.339479 [info ] [MainThread]: 
[0m12:49:11.339648 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:49:11.339741 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:49:11.342114 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_dbt) - Creating connection
[0m12:49:11.342288 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_dbt) - Creating connection
[0m12:49:11.342381 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_dbt'
[0m12:49:11.342770 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_dbt) - Creating connection
[0m12:49:11.342908 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_dbt'
[0m12:49:11.343075 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_dbt) - Creating connection
[0m12:49:11.346329 [debug] [ThreadPool]: Using databricks connection "list_flight_dbt"
[0m12:49:11.346434 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_dbt'
[0m12:49:11.347223 [debug] [ThreadPool]: Using databricks connection "list_flight_dbt"
[0m12:49:11.347322 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_dbt'
[0m12:49:11.347425 [debug] [ThreadPool]: On list_flight_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_dbt"} */

    show databases
  
[0m12:49:11.348082 [debug] [ThreadPool]: Using databricks connection "list_flight_dbt"
[0m12:49:11.348176 [debug] [ThreadPool]: On list_flight_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_dbt"} */

    show databases
  
[0m12:49:11.348789 [debug] [ThreadPool]: Using databricks connection "list_flight_dbt"
[0m12:49:11.348888 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:11.348983 [debug] [ThreadPool]: On list_flight_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_dbt"} */

    show databases
  
[0m12:49:11.349172 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:11.349267 [debug] [ThreadPool]: On list_flight_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_dbt"} */

    show databases
  
[0m12:49:11.349488 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:11.349633 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:12.173127 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-caf4-10f7-9afb-0880bd113623) - Created
[0m12:49:12.189670 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-caf8-10f4-9f7e-54e1d9b24d48) - Created
[0m12:49:12.192306 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-caf8-115d-9099-22a0b1fe28b0) - Created
[0m12:49:12.213853 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-cafb-1016-9e50-9eed44842449) - Created
[0m12:49:12.920329 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_dbt"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-cb0f-11d6-b8b3-633aad585546
[0m12:49:12.922490 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:49:12.922981 [debug] [ThreadPool]: On list_flight_dbt: Close
[0m12:49:12.923255 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-caf4-10f7-9afb-0880bd113623) - Closing
[0m12:49:12.935198 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_dbt"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-cb12-1c80-b89e-aed420112b07
[0m12:49:12.936912 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:49:12.985287 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_dbt"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-cb17-125b-b9c5-0bcdc9701f11
[0m12:49:12.987148 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:49:13.001801 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_dbt"} */

    show databases
  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:361)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:428)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:408)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1191)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7339)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7338)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:267)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7319)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7305)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1157)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1143)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:826)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1723)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:86)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:86)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:82)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:96)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:480)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:480)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:524)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:444)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:819)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:373)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:373)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:848)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:372)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:772)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:475)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1374)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:471)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:400)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:469)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:551)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:543)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:356)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 62 more
, operation-id=01f0897c-cb1a-138e-a4e6-9d0464c8c145
[0m12:49:13.003164 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:49:13.118036 [debug] [ThreadPool]: On list_flight_dbt: Close
[0m12:49:13.118962 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-caf8-10f4-9f7e-54e1d9b24d48) - Closing
[0m12:49:13.322914 [debug] [ThreadPool]: On list_flight_dbt: Close
[0m12:49:13.323899 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-caf8-115d-9099-22a0b1fe28b0) - Closing
[0m12:49:13.521079 [debug] [ThreadPool]: On list_flight_dbt: Close
[0m12:49:13.521812 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-cafb-1016-9e50-9eed44842449) - Closing
[0m12:49:13.721463 [info ] [MainThread]: 
[0m12:49:13.722477 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 2.38 seconds (2.38s).
[0m12:49:13.723343 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    [NO_SUCH_CATALOG_EXCEPTION] Catalog 'flight_dbt' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m12:49:13.730256 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.9035406, "process_in_blocks": "0", "process_kernel_time": 0.2329, "process_mem_max_rss": "241926144", "process_out_blocks": "0", "process_user_time": 2.277704}
[0m12:49:13.730875 [debug] [MainThread]: Command `dbt run` failed at 12:49:13.730773 after 3.90 seconds
[0m12:49:13.731327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10503ee10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c5f4520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c5f4310>]}
[0m12:49:13.731710 [debug] [MainThread]: Flushing usage events
[0m12:49:14.367691 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:49:52.451738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103ecf620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050a7890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050a7b10>]}


============================== 12:49:52.454012 | 51026041-c864-4e4c-b9d7-e042d20894ff ==============================
[0m12:49:52.454012 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:49:52.454245 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'log_format': 'default', 'target_path': 'None', 'fail_fast': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'indirect_selection': 'eager', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run --target dev', 'use_colors': 'True', 'printer_width': '80', 'empty': 'False', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'version_check': 'True', 'static_parser': 'True', 'no_print': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'use_experimental_parser': 'False'}
[0m12:49:52.744559 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:49:52.744739 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:49:52.744836 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:49:53.162320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043d7360>]}
[0m12:49:53.181032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062aead0>]}
[0m12:49:53.181240 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:49:53.229260 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:49:53.265624 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m12:49:53.265812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd2f450>]}
[0m12:49:53.894559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf733e0>]}
[0m12:49:53.926707 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m12:49:53.927590 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m12:49:53.932462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdd2eb0>]}
[0m12:49:53.932614 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m12:49:53.932733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf4ba00>]}
[0m12:49:53.933441 [info ] [MainThread]: 
[0m12:49:53.933556 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:49:53.933640 [info ] [MainThread]: 
[0m12:49:53.933815 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:49:53.933905 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:49:53.936537 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:49:53.936721 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:49:53.936818 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:49:53.937259 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:49:53.937346 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:49:53.937505 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:49:53.940772 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:49:53.940889 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:49:53.941675 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:49:53.941769 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:49:53.941873 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:49:53.942525 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:49:53.942619 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:49:53.943247 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:49:53.943345 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:53.943447 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:49:53.943641 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:53.943732 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:49:53.944266 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:53.944411 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:54.772317 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e457-124b-ab12-f891ada3b93b) - Created
[0m12:49:54.774575 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e457-1c86-a205-f934ae10ac87) - Created
[0m12:49:54.822380 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e45f-1620-9b4e-9c3bd6f9e69f) - Created
[0m12:49:54.826110 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e460-150e-a5e7-afe8a81a4d9a) - Created
[0m12:49:55.224996 [debug] [ThreadPool]: SQL status: OK in 1.280 seconds
[0m12:49:55.236399 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e457-1c86-a205-f934ae10ac87, command-id=01f0897c-e473-1a8c-bfdd-0814afca98d5) - Closing
[0m12:49:55.237073 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:49:55.237355 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e457-1c86-a205-f934ae10ac87) - Closing
[0m12:49:55.248780 [debug] [ThreadPool]: SQL status: OK in 1.300 seconds
[0m12:49:55.250426 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e457-124b-ab12-f891ada3b93b, command-id=01f0897c-e473-1487-91c5-c1a3455afb44) - Closing
[0m12:49:55.341962 [debug] [ThreadPool]: SQL status: OK in 1.400 seconds
[0m12:49:55.344514 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e460-150e-a5e7-afe8a81a4d9a, command-id=01f0897c-e47c-1857-a023-b74b8a1f1228) - Closing
[0m12:49:55.439520 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:49:55.440501 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e457-124b-ab12-f891ada3b93b) - Closing
[0m12:49:55.441451 [debug] [ThreadPool]: SQL status: OK in 1.500 seconds
[0m12:49:55.444092 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e45f-1620-9b4e-9c3bd6f9e69f, command-id=01f0897c-e47b-1969-b42a-5a5f10fe642a) - Closing
[0m12:49:55.628528 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:49:55.629566 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e460-150e-a5e7-afe8a81a4d9a) - Closing
[0m12:49:55.835110 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:49:55.836152 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e45f-1620-9b4e-9c3bd6f9e69f) - Closing
[0m12:49:56.038070 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_flight_db_flights_dbt) - Creating connection
[0m12:49:56.038944 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_flight_db_flights_dbt_silver) - Creating connection
[0m12:49:56.039658 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_flight_db_flights_dbt_bronze) - Creating connection
[0m12:49:56.040155 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_flight_db_flights_dbt_gold) - Creating connection
[0m12:49:56.040427 [debug] [ThreadPool]: Acquiring new databricks connection 'create_flight_db_flights_dbt'
[0m12:49:56.040775 [debug] [ThreadPool]: Acquiring new databricks connection 'create_flight_db_flights_dbt_silver'
[0m12:49:56.041055 [debug] [ThreadPool]: Acquiring new databricks connection 'create_flight_db_flights_dbt_bronze'
[0m12:49:56.041298 [debug] [ThreadPool]: Acquiring new databricks connection 'create_flight_db_flights_dbt_gold'
[0m12:49:56.041758 [debug] [ThreadPool]: Creating schema "database: "flight_db"
schema: "flights_dbt"
"
[0m12:49:56.042119 [debug] [ThreadPool]: Creating schema "database: "flight_db"
schema: "flights_dbt_silver"
"
[0m12:49:56.042427 [debug] [ThreadPool]: Creating schema "database: "flight_db"
schema: "flights_dbt_bronze"
"
[0m12:49:56.042730 [debug] [ThreadPool]: Creating schema "database: "flight_db"
schema: "flights_dbt_gold"
"
[0m12:49:56.047603 [debug] [ThreadPool]: Using databricks connection "create_flight_db_flights_dbt"
[0m12:49:56.050698 [debug] [ThreadPool]: Using databricks connection "create_flight_db_flights_dbt_bronze"
[0m12:49:56.053255 [debug] [ThreadPool]: Using databricks connection "create_flight_db_flights_dbt_gold"
[0m12:49:56.054133 [debug] [ThreadPool]: On create_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "create_flight_db_flights_dbt"} */
create schema if not exists `flight_db`.`flights_dbt`
  
[0m12:49:56.054931 [debug] [ThreadPool]: On create_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "create_flight_db_flights_dbt_bronze"} */
create schema if not exists `flight_db`.`flights_dbt_bronze`
  
[0m12:49:56.057791 [debug] [ThreadPool]: Using databricks connection "create_flight_db_flights_dbt_silver"
[0m12:49:56.057920 [debug] [ThreadPool]: On create_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "create_flight_db_flights_dbt_gold"} */
create schema if not exists `flight_db`.`flights_dbt_gold`
  
[0m12:49:56.058043 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:56.058162 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:56.058274 [debug] [ThreadPool]: On create_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "create_flight_db_flights_dbt_silver"} */
create schema if not exists `flight_db`.`flights_dbt_silver`
  
[0m12:49:56.058369 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:56.058605 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:56.767885 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e589-1239-b58a-17b63407a995) - Created
[0m12:49:56.814944 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e590-1aa7-bddd-a4ab8758d66d) - Created
[0m12:49:56.826301 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e592-1926-9733-7c6d12bf1170) - Created
[0m12:49:56.831302 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e592-1c6b-8a0b-18b733285bfc) - Created
[0m12:49:57.346136 [debug] [ThreadPool]: SQL status: OK in 1.290 seconds
[0m12:49:57.347765 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e589-1239-b58a-17b63407a995, command-id=01f0897c-e5a3-1c8b-9f08-1e7570753fd6) - Closing
[0m12:49:57.348484 [debug] [ThreadPool]: On create_flight_db_flights_dbt_gold: Close
[0m12:49:57.348937 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e589-1239-b58a-17b63407a995) - Closing
[0m12:49:57.351586 [debug] [ThreadPool]: SQL status: OK in 1.290 seconds
[0m12:49:57.352716 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e592-1926-9733-7c6d12bf1170, command-id=01f0897c-e5ad-1683-9f5d-6696b9b155f6) - Closing
[0m12:49:57.397506 [debug] [ThreadPool]: SQL status: OK in 1.340 seconds
[0m12:49:57.398964 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e590-1aa7-bddd-a4ab8758d66d, command-id=01f0897c-e5ac-1785-a807-2659cba22003) - Closing
[0m12:49:57.444823 [debug] [ThreadPool]: SQL status: OK in 1.390 seconds
[0m12:49:57.446556 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e592-1c6b-8a0b-18b733285bfc, command-id=01f0897c-e5ae-125c-905c-ddd42a161d71) - Closing
[0m12:49:57.534812 [debug] [ThreadPool]: On create_flight_db_flights_dbt: Close
[0m12:49:57.535519 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e592-1926-9733-7c6d12bf1170) - Closing
[0m12:49:57.744782 [debug] [ThreadPool]: On create_flight_db_flights_dbt_silver: Close
[0m12:49:57.745492 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e590-1aa7-bddd-a4ab8758d66d) - Closing
[0m12:49:57.948604 [debug] [ThreadPool]: On create_flight_db_flights_dbt_bronze: Close
[0m12:49:57.949214 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e592-1c6b-8a0b-18b733285bfc) - Closing
[0m12:49:58.159774 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m12:49:58.160497 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m12:49:58.160772 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m12:49:58.161263 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m12:49:58.161794 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m12:49:58.162221 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m12:49:58.168968 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m12:49:58.169247 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m12:49:58.170462 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m12:49:58.171390 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m12:49:58.173392 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m12:49:58.175151 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m12:49:58.179055 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m12:49:58.179244 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:58.179456 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m12:49:58.179669 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m12:49:58.179870 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m12:49:58.180181 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:58.180323 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:58.180505 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:58.926815 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e6d2-1c81-baee-826fa3f483c8) - Created
[0m12:49:58.947250 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e6d5-1a55-840e-a24b751840e4) - Created
[0m12:49:58.948761 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e6d5-1edb-8abd-bd0badc9142d) - Created
[0m12:49:58.964625 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e6d7-1c6f-a092-4ff51d1bf161) - Created
[0m12:49:59.945945 [debug] [ThreadPool]: SQL status: OK in 1.770 seconds
[0m12:49:59.946510 [debug] [ThreadPool]: SQL status: OK in 1.770 seconds
[0m12:49:59.946804 [debug] [ThreadPool]: SQL status: OK in 1.770 seconds
[0m12:49:59.950782 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e6d5-1edb-8abd-bd0badc9142d, command-id=01f0897c-e6f0-1a68-ae58-5069728dc718) - Closing
[0m12:49:59.951301 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m12:49:59.951639 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e6d5-1edb-8abd-bd0badc9142d) - Closing
[0m12:49:59.953042 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e6d7-1c6f-a092-4ff51d1bf161, command-id=01f0897c-e6f4-1ba1-9bd3-1a853e9821fd) - Closing
[0m12:49:59.954783 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e6d2-1c81-baee-826fa3f483c8, command-id=01f0897c-e6ee-1372-9bbe-458404482d4d) - Closing
[0m12:49:59.959932 [debug] [ThreadPool]: SQL status: OK in 1.780 seconds
[0m12:49:59.961283 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897c-e6d5-1a55-840e-a24b751840e4, command-id=01f0897c-e6f0-1907-b747-1bdd6a8afdbf) - Closing
[0m12:50:00.151972 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m12:50:00.152902 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e6d7-1c6f-a092-4ff51d1bf161) - Closing
[0m12:50:00.366589 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m12:50:00.367614 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e6d2-1c81-baee-826fa3f483c8) - Closing
[0m12:50:00.565437 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m12:50:00.566913 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897c-e6d5-1a55-840e-a24b751840e4) - Closing
[0m12:50:00.763019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c006e10>]}
[0m12:50:00.767734 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m12:50:00.768243 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m12:50:00.768615 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m12:50:00.768971 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m12:50:00.769633 [info ] [Thread-2 (]: 2 of 9 START sql view model flights_dbt_bronze.bronze_airports ................. [RUN]
[0m12:50:00.770147 [info ] [Thread-1 (]: 1 of 9 START sql table model flights_dbt_bronze.bronze_airlines ................ [RUN]
[0m12:50:00.770648 [info ] [Thread-3 (]: 3 of 9 START sql table model flights_dbt_bronze.bronze_flights ................. [RUN]
[0m12:50:00.771134 [info ] [Thread-4 (]: 4 of 9 START sql table model flights_dbt.my_first_dbt_model .................... [RUN]
[0m12:50:00.771801 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m12:50:00.772273 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m12:50:00.772659 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m12:50:00.773010 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m12:50:00.773294 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m12:50:00.773542 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m12:50:00.773774 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m12:50:00.774009 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m12:50:00.774275 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m12:50:00.774524 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m12:50:00.774748 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m12:50:00.774981 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m12:50:00.782919 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m12:50:00.786695 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m12:50:00.788961 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m12:50:00.790982 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m12:50:00.792133 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m12:50:00.792428 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m12:50:00.792651 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m12:50:00.792844 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m12:50:00.802628 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m12:50:00.817958 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m12:50:00.819902 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m12:50:00.820714 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m12:50:00.821558 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:50:00.821751 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:50:00.821983 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:50:00.822360 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c315650>]}
[0m12:50:00.822179 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:50:00.822528 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c59b6b0>]}
[0m12:50:00.822653 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c59b750>]}
[0m12:50:00.828677 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
[0m12:50:00.828820 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba15850>]}
[0m12:50:00.846546 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m12:50:00.847203 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m12:50:00.848086 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m12:50:00.853241 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m12:50:00.853769 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m12:50:00.853892 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m12:50:00.854047 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select flight_id,
       airline_id,
       origin_airport,
       destination_airport,
       departure_time,
       arrival_time,
       delay
from `main`.`raw`.`flights`
  
[0m12:50:00.854152 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m12:50:00.854235 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m12:50:00.854367 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`flights_dbt`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m12:50:00.854490 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:50:00.854607 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select upper(code) as airline_code, name as airline_name
from `main`.`raw`.`airlines`
  
[0m12:50:00.854729 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select upper(iata) as iata, airport_name, city, state
from `main`.`raw`.`airports`
  )

[0m12:50:00.854832 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:50:00.855016 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:50:00.855192 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:50:01.560875 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897c-e865-1115-affa-dad6b96dcdb5) - Created
[0m12:50:01.570912 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897c-e866-1c43-9ddb-df586e7a01e8) - Created
[0m12:50:01.596156 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897c-e86a-1355-a5da-230e1d188582) - Created
[0m12:50:01.603459 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897c-e86a-1cf0-91cd-2993f1c34a90) - Created
[0m12:50:02.342843 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select upper(code) as airline_code, name as airline_name
from `main`.`raw`.`airlines`
  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897c-e880-1bd0-b139-53eeaa946770
[0m12:50:02.343853 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m12:50:02.344271 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897c-e866-1c43-9ddb-df586e7a01e8) - Closing
[0m12:50:02.361473 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select upper(iata) as iata, airport_name, city, state
from `main`.`raw`.`airports`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897c-e887-1217-b035-c42050ff763d
[0m12:50:02.411350 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select flight_id,
       airline_id,
       origin_airport,
       destination_airport,
       departure_time,
       arrival_time,
       delay
from `main`.`raw`.`flights`
  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897c-e885-15c2-b363-795d0d9af906
[0m12:50:02.536117 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m12:50:02.536920 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897c-e86a-1cf0-91cd-2993f1c34a90) - Closing
[0m12:50:02.551492 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m12:50:02.742197 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m12:50:02.742875 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897c-e86a-1355-a5da-230e1d188582) - Closing
[0m12:50:02.748022 [debug] [Thread-2 (]: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m12:50:02.949563 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c1405f0>]}
[0m12:50:02.950372 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb0b590>]}
[0m12:50:02.951342 [debug] [Thread-3 (]: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m12:50:02.952263 [error] [Thread-1 (]: 1 of 9 ERROR creating sql table model flights_dbt_bronze.bronze_airlines ....... [[31mERROR[0m in 2.17s]
[0m12:50:02.952878 [error] [Thread-2 (]: 2 of 9 ERROR creating sql view model flights_dbt_bronze.bronze_airports ........ [[31mERROR[0m in 2.17s]
[0m12:50:02.953720 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c524dd0>]}
[0m12:50:02.954322 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m12:50:02.954905 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m12:50:02.955641 [error] [Thread-3 (]: 3 of 9 ERROR creating sql table model flights_dbt_bronze.bronze_flights ........ [[31mERROR[0m in 2.18s]
[0m12:50:02.956565 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m12:50:02.957311 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m12:50:02.958610 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airports' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql.
[0m12:50:02.959176 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_airlines
[0m12:50:02.959744 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_flights' to be skipped because of status 'error'.  Reason: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql.
[0m12:50:02.960247 [info ] [Thread-1 (]: 5 of 9 SKIP relation flights_dbt_silver.silver_airlines ........................ [[33mSKIP[0m]
[0m12:50:02.960659 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airports
[0m12:50:02.961081 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_flights
[0m12:50:02.961353 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_airlines
[0m12:50:02.961644 [info ] [Thread-2 (]: 6 of 9 SKIP relation flights_dbt_silver.silver_airports ........................ [[33mSKIP[0m]
[0m12:50:02.961936 [info ] [Thread-3 (]: 7 of 9 SKIP relation flights_dbt_silver.silver_flights ......................... [[33mSKIP[0m]
[0m12:50:02.962300 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airports
[0m12:50:02.962603 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_flights
[0m12:50:02.963130 [debug] [Thread-1 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m12:50:02.963376 [info ] [Thread-1 (]: 8 of 9 SKIP relation flights_dbt_gold.gold_flight_metrics ...................... [[33mSKIP[0m]
[0m12:50:02.963642 [debug] [Thread-1 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m12:50:04.864987 [debug] [Thread-4 (]: SQL status: OK in 4.010 seconds
[0m12:50:04.866757 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0897c-e865-1115-affa-dad6b96dcdb5, command-id=01f0897c-e87e-1a71-911c-9490df668399) - Closing
[0m12:50:04.882770 [debug] [Thread-4 (]: Applying tags to relation None
[0m12:50:04.897505 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m12:50:04.897892 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897c-e865-1115-affa-dad6b96dcdb5) - Closing
[0m12:50:05.095243 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c524d70>]}
[0m12:50:05.097029 [info ] [Thread-4 (]: 4 of 9 OK created sql table model flights_dbt.my_first_dbt_model ............... [[32mOK[0m in 4.32s]
[0m12:50:05.097835 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m12:50:05.099046 [debug] [Thread-2 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m12:50:05.099765 [info ] [Thread-2 (]: 9 of 9 START sql view model flights_dbt.my_second_dbt_model .................... [RUN]
[0m12:50:05.100589 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m12:50:05.101038 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m12:50:05.101399 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m12:50:05.113373 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m12:50:05.114409 [debug] [Thread-2 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m12:50:05.116251 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m12:50:05.117132 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt`.`my_second_dbt_model`
[0m12:50:05.117740 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m12:50:05.118274 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m12:50:05.118539 [debug] [Thread-2 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`flights_dbt`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`flights_dbt`.`my_first_dbt_model`
where id = 1
  )

[0m12:50:05.118775 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:50:05.752598 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897c-ead6-1038-b1fd-c4d01d29c524) - Created
[0m12:50:06.390414 [debug] [Thread-2 (]: SQL status: OK in 1.270 seconds
[0m12:50:06.392205 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0897c-ead6-1038-b1fd-c4d01d29c524, command-id=01f0897c-eaf0-1d97-8eb6-d6204b6ef5a4) - Closing
[0m12:50:06.393165 [debug] [Thread-2 (]: Applying tags to relation None
[0m12:50:06.395060 [debug] [Thread-2 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m12:50:06.395448 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897c-ead6-1038-b1fd-c4d01d29c524) - Closing
[0m12:50:06.593781 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '51026041-c864-4e4c-b9d7-e042d20894ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdb1bb0>]}
[0m12:50:06.595187 [info ] [Thread-2 (]: 9 of 9 OK created sql view model flights_dbt.my_second_dbt_model ............... [[32mOK[0m in 1.49s]
[0m12:50:06.595886 [debug] [Thread-2 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m12:50:06.597792 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:50:06.598173 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:50:06.598706 [info ] [MainThread]: 
[0m12:50:06.599024 [info ] [MainThread]: Finished running 7 table models, 2 view models in 0 hours 0 minutes and 12.66 seconds (12.66s).
[0m12:50:06.600363 [debug] [MainThread]: Command end result
[0m12:50:06.631824 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m12:50:06.633461 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m12:50:06.637670 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m12:50:06.637918 [info ] [MainThread]: 
[0m12:50:06.638165 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m12:50:06.638314 [info ] [MainThread]: 
[0m12:50:06.638489 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m12:50:06.638656 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m12:50:06.638782 [info ] [MainThread]: 
[0m12:50:06.638922 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m12:50:06.639032 [info ] [MainThread]: 
[0m12:50:06.639172 [error] [MainThread]: [31mFailure in model bronze_airports (models/bronze/bronze_airports.sql)[0m
[0m12:50:06.639318 [error] [MainThread]:   Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m12:50:06.639438 [info ] [MainThread]: 
[0m12:50:06.639573 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airports.sql
[0m12:50:06.639681 [info ] [MainThread]: 
[0m12:50:06.639809 [error] [MainThread]: [31mFailure in model bronze_flights (models/bronze/bronze_flights.sql)[0m
[0m12:50:06.640093 [error] [MainThread]:   Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m12:50:06.640245 [info ] [MainThread]: 
[0m12:50:06.640391 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_flights.sql
[0m12:50:06.640514 [info ] [MainThread]: 
[0m12:50:06.640649 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=3 SKIP=4 NO-OP=0 TOTAL=9
[0m12:50:06.643353 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 14.12844, "process_in_blocks": "0", "process_kernel_time": 0.293916, "process_mem_max_rss": "250478592", "process_out_blocks": "0", "process_user_time": 4.294933}
[0m12:50:06.643578 [debug] [MainThread]: Command `dbt run` failed at 12:50:06.643532 after 14.13 seconds
[0m12:50:06.643747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a16870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c143b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb0b590>]}
[0m12:50:06.643895 [debug] [MainThread]: Flushing usage events
[0m12:50:07.381557 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:51:16.376037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10379b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104973890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104973b10>]}


============================== 12:51:16.378280 | c08d7750-80e0-4661-88e6-5672f307547a ==============================
[0m12:51:16.378280 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:51:16.378507 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'empty': 'False', 'invocation_command': 'dbt run --target dev', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'partial_parse': 'True', 'printer_width': '80', 'write_json': 'True', 'version_check': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'fail_fast': 'False', 'log_cache_events': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'static_parser': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'debug': 'False', 'use_colors': 'True'}
[0m12:51:16.668381 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:51:16.668571 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:51:16.668668 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:51:17.083226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c08d7750-80e0-4661-88e6-5672f307547a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c9b360>]}
[0m12:51:17.101905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c08d7750-80e0-4661-88e6-5672f307547a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b7aad0>]}
[0m12:51:17.102119 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:51:17.150532 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:51:17.207963 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m12:51:17.208230 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/sources.yml
[0m12:51:17.208370 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m12:51:17.301965 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_flights_airlines".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_flights", "airlines").
  
  To fix this, change the name of one of these resources:
  - source.flights_dbt.raw_flights.airlines (models/bronze/sources.yml)
  - source.flights_dbt.raw_flights.airlines (models/sources.yml)
[0m12:51:17.304143 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.95858496, "process_in_blocks": "0", "process_kernel_time": 0.211885, "process_mem_max_rss": "229982208", "process_out_blocks": "0", "process_user_time": 1.062292}
[0m12:51:17.304343 [debug] [MainThread]: Command `dbt run` failed at 12:51:17.304305 after 0.96 seconds
[0m12:51:17.304514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb30950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb30a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba09c70>]}
[0m12:51:17.304637 [debug] [MainThread]: Flushing usage events
[0m12:51:17.960438 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:52:03.700781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c3b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095fbb10>]}


============================== 12:52:03.702942 | d2022271-f460-4887-bb24-7edbd803d582 ==============================
[0m12:52:03.702942 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:52:03.703164 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'write_json': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'static_parser': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'no_print': 'None', 'introspect': 'True', 'printer_width': '80', 'debug': 'False', 'log_format': 'default', 'fail_fast': 'False', 'target_path': 'None', 'invocation_command': 'dbt run --target dev', 'log_cache_events': 'False'}
[0m12:52:03.989577 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:52:03.989790 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:52:03.989896 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:52:04.400371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd2022271-f460-4887-bb24-7edbd803d582', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10812b360>]}
[0m12:52:04.419266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd2022271-f460-4887-bb24-7edbd803d582', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b506ad0>]}
[0m12:52:04.419504 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:52:04.474456 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:52:04.513487 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading flights_dbt: bronze/sources.yml - Runtime Error
    Contents of file 'models/bronze/sources.yml' are not valid. Dictionary expected.
[0m12:52:04.515420 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8433272, "process_in_blocks": "0", "process_kernel_time": 0.204752, "process_mem_max_rss": "223707136", "process_out_blocks": "0", "process_user_time": 0.972683}
[0m12:52:04.515663 [debug] [MainThread]: Command `dbt run` failed at 12:52:04.515622 after 0.84 seconds
[0m12:52:04.515839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b50f450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b50f050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b865f40>]}
[0m12:52:04.515959 [debug] [MainThread]: Flushing usage events
[0m12:52:05.083915 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:52:45.848739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103827620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049ff890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049ffb10>]}


============================== 12:52:45.850922 | af3c7316-b2a2-41d5-94ec-287471bc6c4a ==============================
[0m12:52:45.850922 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:52:45.851140 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'log_cache_events': 'False', 'debug': 'False', 'use_colors': 'True', 'target_path': 'None', 'printer_width': '80', 'introspect': 'True', 'empty': 'False', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'write_json': 'True', 'invocation_command': 'dbt run --target dev', 'warn_error': 'None', 'cache_selected_only': 'False', 'no_print': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'static_parser': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'quiet': 'False'}
[0m12:52:46.133753 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:52:46.133944 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:52:46.134038 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:52:46.562211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d2b360>]}
[0m12:52:46.581305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c06ad0>]}
[0m12:52:46.581553 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:52:46.629547 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:52:46.685468 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:52:46.685731 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m12:52:46.776192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be0e950>]}
[0m12:52:46.808956 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m12:52:46.810528 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m12:52:46.815969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b7e26c0>]}
[0m12:52:46.816154 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m12:52:46.816272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c185b70>]}
[0m12:52:46.817027 [info ] [MainThread]: 
[0m12:52:46.817147 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:52:46.817233 [info ] [MainThread]: 
[0m12:52:46.817427 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:52:46.817522 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:52:46.819985 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:52:46.820146 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:52:46.820314 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:52:46.823906 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:52:46.824131 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:52:46.824238 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:52:46.824394 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:52:46.824521 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:52:46.824613 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:52:46.825271 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:52:46.825374 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:52:46.825477 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:52:46.826244 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:52:46.826337 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:52:46.826951 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:52:46.827211 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:52:46.827300 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:52:46.827397 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:52:46.827482 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:52:46.827615 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:52:47.652146 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4b55-1cf4-8561-e65fdcf4e82c) - Created
[0m12:52:47.666000 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4b57-1a6b-ba17-5c31a2431047) - Created
[0m12:52:47.691760 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4b5b-12ca-8faa-41450dd19aa6) - Created
[0m12:52:47.710624 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4b5e-11cd-9271-1c0b27cfae6e) - Created
[0m12:52:48.089298 [debug] [ThreadPool]: SQL status: OK in 1.260 seconds
[0m12:52:48.098536 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-4b55-1cf4-8561-e65fdcf4e82c, command-id=01f0897d-4b70-14dc-96fb-4ea81d381aed) - Closing
[0m12:52:48.099278 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:52:48.099657 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4b55-1cf4-8561-e65fdcf4e82c) - Closing
[0m12:52:48.186617 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m12:52:48.189473 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-4b57-1a6b-ba17-5c31a2431047, command-id=01f0897d-4b73-1895-9824-7186505410da) - Closing
[0m12:52:48.255907 [debug] [ThreadPool]: SQL status: OK in 1.430 seconds
[0m12:52:48.258383 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-4b5e-11cd-9271-1c0b27cfae6e, command-id=01f0897d-4b7a-1cd7-9cf3-50b71fd3b0fb) - Closing
[0m12:52:48.274623 [debug] [ThreadPool]: SQL status: OK in 1.450 seconds
[0m12:52:48.277043 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-4b5b-12ca-8faa-41450dd19aa6, command-id=01f0897d-4b7a-1c9f-80d1-070b0e7a162b) - Closing
[0m12:52:48.289272 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:52:48.289844 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4b57-1a6b-ba17-5c31a2431047) - Closing
[0m12:52:48.484930 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:52:48.485903 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4b5e-11cd-9271-1c0b27cfae6e) - Closing
[0m12:52:48.688469 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:52:48.689483 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4b5b-12ca-8faa-41450dd19aa6) - Closing
[0m12:52:48.895905 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m12:52:48.896465 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m12:52:48.896840 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m12:52:48.903322 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m12:52:48.904839 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m12:52:48.905083 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m12:52:48.905511 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m12:52:48.905732 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m12:52:48.906013 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m12:52:48.907981 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m12:52:48.908183 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m12:52:48.909364 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m12:52:48.909545 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:52:48.909756 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m12:52:48.913786 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m12:52:48.915333 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:52:48.916182 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:52:48.919853 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m12:52:48.920203 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m12:52:48.920495 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:52:49.598559 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4c7e-1afd-aab1-35d1a5302913) - Created
[0m12:52:49.618256 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4c81-1724-add5-0110cf8c5650) - Created
[0m12:52:49.637681 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4c84-1d29-9d39-d20a61232002) - Created
[0m12:52:49.647242 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4c86-1b05-a5cb-5971d32c5205) - Created
[0m12:52:50.172216 [debug] [ThreadPool]: SQL status: OK in 1.260 seconds
[0m12:52:50.175683 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-4c7e-1afd-aab1-35d1a5302913, command-id=01f0897d-4c99-1330-9097-06da311f7b64) - Closing
[0m12:52:50.178584 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m12:52:50.179028 [debug] [ThreadPool]: SQL status: OK in 1.260 seconds
[0m12:52:50.179324 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4c7e-1afd-aab1-35d1a5302913) - Closing
[0m12:52:50.181560 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-4c84-1d29-9d39-d20a61232002, command-id=01f0897d-4c9f-1201-a4d6-6fd269da2087) - Closing
[0m12:52:50.306394 [debug] [ThreadPool]: SQL status: OK in 1.390 seconds
[0m12:52:50.308694 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-4c86-1b05-a5cb-5971d32c5205, command-id=01f0897d-4ca1-1f28-8fc5-9f9d58f60bae) - Closing
[0m12:52:50.371534 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m12:52:50.372424 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4c84-1d29-9d39-d20a61232002) - Closing
[0m12:52:50.571371 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m12:52:50.572354 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4c86-1b05-a5cb-5971d32c5205) - Closing
[0m12:52:50.710862 [debug] [ThreadPool]: SQL status: OK in 1.800 seconds
[0m12:52:50.713945 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-4c81-1724-add5-0110cf8c5650, command-id=01f0897d-4c9d-1d7a-aac0-2befc9a5ea3f) - Closing
[0m12:52:50.769751 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m12:52:50.770579 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-4c81-1724-add5-0110cf8c5650) - Closing
[0m12:52:50.972164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c177ad0>]}
[0m12:52:50.977878 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m12:52:50.978533 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m12:52:50.978950 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m12:52:50.979325 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m12:52:50.979995 [info ] [Thread-4 (]: 4 of 9 START sql table model flights_dbt.my_first_dbt_model .................... [RUN]
[0m12:52:50.980538 [info ] [Thread-2 (]: 2 of 9 START sql view model flights_dbt_bronze.bronze_airports ................. [RUN]
[0m12:52:50.981046 [info ] [Thread-3 (]: 3 of 9 START sql table model flights_dbt_bronze.bronze_flights ................. [RUN]
[0m12:52:50.981571 [info ] [Thread-1 (]: 1 of 9 START sql table model flights_dbt_bronze.bronze_airlines ................ [RUN]
[0m12:52:50.982286 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m12:52:50.982792 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m12:52:50.983277 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m12:52:50.983677 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m12:52:50.983959 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m12:52:50.984209 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m12:52:50.984449 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m12:52:50.984680 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m12:52:50.984928 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m12:52:50.985151 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m12:52:50.985354 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m12:52:50.985551 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m12:52:50.995427 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m12:52:50.998316 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m12:52:51.001128 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m12:52:51.006526 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m12:52:51.007398 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m12:52:51.007602 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m12:52:51.007773 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m12:52:51.007917 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m12:52:51.024162 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m12:52:51.024389 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m12:52:51.025296 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m12:52:51.026143 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m12:52:51.026359 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:52:51.027446 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:52:51.027663 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:52:51.027869 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:52:51.028051 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf8a5d0>]}
[0m12:52:51.028177 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf4b960>]}
[0m12:52:51.028294 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf4ba10>]}
[0m12:52:51.028404 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ec48410>]}
[0m12:52:51.046245 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m12:52:51.049922 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
[0m12:52:51.050712 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m12:52:51.051242 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m12:52:51.051557 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m12:52:51.051841 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m12:52:51.052050 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select flight_id,
       airline_id,
       origin_airport,
       destination_airport,
       departure_time,
       arrival_time,
       delay
from `main`.`raw`.`flights`
  
[0m12:52:51.052163 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m12:52:51.052257 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m12:52:51.052358 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:52:51.052439 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m12:52:51.052559 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `main`.`raw`.`airlines`
  
[0m12:52:51.052693 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`flights_dbt`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m12:52:51.052906 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select upper(iata) as iata, airport_name, city, state
from `main`.`raw`.`airports`
  )

[0m12:52:51.053054 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:52:51.053187 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:52:51.053282 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:52:51.782654 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897d-4dcd-10e3-b966-23cb55210b48) - Created
[0m12:52:51.796814 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897d-4dcd-1d11-8c95-1c60c8eab3ca) - Created
[0m12:52:51.806671 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897d-4dd0-19bb-b190-987a4a134cc7) - Created
[0m12:52:51.816454 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897d-4dd1-1927-b4b1-1fd1d4e4c7d4) - Created
[0m12:52:52.324987 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select upper(iata) as iata, airport_name, city, state
from `main`.`raw`.`airports`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897d-4deb-19d5-bfe3-c960bba32664
[0m12:52:52.326100 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m12:52:52.326405 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897d-4dd0-19bb-b190-987a4a134cc7) - Closing
[0m12:52:52.344912 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `main`.`raw`.`airlines`
  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897d-4de6-1a25-a68b-6c92b130abd7
[0m12:52:52.421145 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select flight_id,
       airline_id,
       origin_airport,
       destination_airport,
       departure_time,
       arrival_time,
       delay
from `main`.`raw`.`flights`
  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897d-4ded-13fc-8e46-ed5082025afa
[0m12:52:52.516410 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m12:52:52.517116 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897d-4dcd-10e3-b966-23cb55210b48) - Closing
[0m12:52:52.528797 [debug] [Thread-2 (]: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m12:52:52.704127 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m12:52:52.705068 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897d-4dd1-1927-b4b1-1fd1d4e4c7d4) - Closing
[0m12:52:52.710779 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m12:52:52.912721 [debug] [Thread-3 (]: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m12:52:52.913440 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ec34c20>]}
[0m12:52:52.913916 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ecf0fa0>]}
[0m12:52:52.914208 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ecc3170>]}
[0m12:52:52.915105 [error] [Thread-1 (]: 1 of 9 ERROR creating sql table model flights_dbt_bronze.bronze_airlines ....... [[31mERROR[0m in 1.92s]
[0m12:52:52.915666 [error] [Thread-3 (]: 3 of 9 ERROR creating sql table model flights_dbt_bronze.bronze_flights ........ [[31mERROR[0m in 1.93s]
[0m12:52:52.916608 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m12:52:52.917307 [error] [Thread-2 (]: 2 of 9 ERROR creating sql view model flights_dbt_bronze.bronze_airports ........ [[31mERROR[0m in 1.92s]
[0m12:52:52.917893 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m12:52:52.918488 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m12:52:52.919000 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m12:52:52.920261 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_flights' to be skipped because of status 'error'.  Reason: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql.
[0m12:52:52.920789 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_airlines
[0m12:52:52.921228 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airports' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql.
[0m12:52:52.921869 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_flights
[0m12:52:52.922387 [info ] [Thread-1 (]: 5 of 9 SKIP relation flights_dbt_silver.silver_airlines ........................ [[33mSKIP[0m]
[0m12:52:52.923043 [info ] [Thread-3 (]: 6 of 9 SKIP relation flights_dbt_silver.silver_flights ......................... [[33mSKIP[0m]
[0m12:52:52.923489 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airports
[0m12:52:52.923874 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_airlines
[0m12:52:52.924219 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_flights
[0m12:52:52.924540 [info ] [Thread-2 (]: 7 of 9 SKIP relation flights_dbt_silver.silver_airports ........................ [[33mSKIP[0m]
[0m12:52:52.925113 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airports
[0m12:52:52.925605 [debug] [Thread-1 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m12:52:52.925882 [info ] [Thread-1 (]: 8 of 9 SKIP relation flights_dbt_gold.gold_flight_metrics ...................... [[33mSKIP[0m]
[0m12:52:52.926158 [debug] [Thread-1 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m12:52:53.874552 [debug] [Thread-4 (]: SQL status: OK in 2.820 seconds
[0m12:52:53.875701 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0897d-4dcd-1d11-8c95-1c60c8eab3ca, command-id=01f0897d-4de8-1ce0-9f99-24fd4164e2f2) - Closing
[0m12:52:53.886755 [debug] [Thread-4 (]: Applying tags to relation None
[0m12:52:53.896539 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m12:52:53.896737 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897d-4dcd-1d11-8c95-1c60c8eab3ca) - Closing
[0m12:52:54.071863 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eceb650>]}
[0m12:52:54.073401 [info ] [Thread-4 (]: 4 of 9 OK created sql table model flights_dbt.my_first_dbt_model ............... [[32mOK[0m in 3.09s]
[0m12:52:54.074179 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m12:52:54.075284 [debug] [Thread-3 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m12:52:54.075837 [info ] [Thread-3 (]: 9 of 9 START sql view model flights_dbt.my_second_dbt_model .................... [RUN]
[0m12:52:54.076479 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m12:52:54.076823 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m12:52:54.077154 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m12:52:54.085350 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m12:52:54.086354 [debug] [Thread-3 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m12:52:54.088300 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m12:52:54.089415 [debug] [Thread-3 (]: Creating view `flight_db`.`flights_dbt`.`my_second_dbt_model`
[0m12:52:54.090209 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m12:52:54.090844 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m12:52:54.091179 [debug] [Thread-3 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`flights_dbt`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`flights_dbt`.`my_first_dbt_model`
where id = 1
  )

[0m12:52:54.091469 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:52:54.719982 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897d-4f8c-1957-9ca8-807293df8e00) - Created
[0m12:52:55.338176 [debug] [Thread-3 (]: SQL status: OK in 1.250 seconds
[0m12:52:55.339850 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0897d-4f8c-1957-9ca8-807293df8e00, command-id=01f0897d-4fa6-16d7-a366-ad294f6e8523) - Closing
[0m12:52:55.341451 [debug] [Thread-3 (]: Applying tags to relation None
[0m12:52:55.343878 [debug] [Thread-3 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m12:52:55.344256 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897d-4f8c-1957-9ca8-807293df8e00) - Closing
[0m12:52:55.541810 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af3c7316-b2a2-41d5-94ec-287471bc6c4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0b8830>]}
[0m12:52:55.543621 [info ] [Thread-3 (]: 9 of 9 OK created sql view model flights_dbt.my_second_dbt_model ............... [[32mOK[0m in 1.47s]
[0m12:52:55.544421 [debug] [Thread-3 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m12:52:55.546238 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:52:55.546576 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:52:55.547092 [info ] [MainThread]: 
[0m12:52:55.547418 [info ] [MainThread]: Finished running 7 table models, 2 view models in 0 hours 0 minutes and 8.73 seconds (8.73s).
[0m12:52:55.548796 [debug] [MainThread]: Command end result
[0m12:52:55.577616 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m12:52:55.579012 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m12:52:55.583123 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m12:52:55.583299 [info ] [MainThread]: 
[0m12:52:55.583508 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m12:52:55.583654 [info ] [MainThread]: 
[0m12:52:55.583838 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m12:52:55.584021 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m12:52:55.584154 [info ] [MainThread]: 
[0m12:52:55.584300 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m12:52:55.584416 [info ] [MainThread]: 
[0m12:52:55.584561 [error] [MainThread]: [31mFailure in model bronze_flights (models/bronze/bronze_flights.sql)[0m
[0m12:52:55.584719 [error] [MainThread]:   Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 24 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m12:52:55.584839 [info ] [MainThread]: 
[0m12:52:55.584978 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_flights.sql
[0m12:52:55.585086 [info ] [MainThread]: 
[0m12:52:55.585228 [error] [MainThread]: [31mFailure in model bronze_airports (models/bronze/bronze_airports.sql)[0m
[0m12:52:55.585382 [error] [MainThread]:   Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`raw`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m12:52:55.585502 [info ] [MainThread]: 
[0m12:52:55.585624 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airports.sql
[0m12:52:55.585727 [info ] [MainThread]: 
[0m12:52:55.585853 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=3 SKIP=4 NO-OP=0 TOTAL=9
[0m12:52:55.588486 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.766407, "process_in_blocks": "0", "process_kernel_time": 0.265673, "process_mem_max_rss": "247545856", "process_out_blocks": "0", "process_user_time": 3.098131}
[0m12:52:55.588691 [debug] [MainThread]: Command `dbt run` failed at 12:52:55.588650 after 9.77 seconds
[0m12:52:55.588863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c57e30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed27a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed27890>]}
[0m12:52:55.589013 [debug] [MainThread]: Flushing usage events
[0m12:53:00.467121 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:54:13.070170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e6f620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e87890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e87b10>]}


============================== 12:54:13.072320 | 69207518-d05d-4dec-97c8-6541409a3ce1 ==============================
[0m12:54:13.072320 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:54:13.072544 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'debug': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'static_parser': 'True', 'use_colors': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'no_print': 'None', 'empty': 'False', 'indirect_selection': 'eager', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'printer_width': '80', 'cache_selected_only': 'False', 'quiet': 'False', 'fail_fast': 'False', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'target_path': 'None', 'write_json': 'True', 'invocation_command': 'dbt run --target dev'}
[0m12:54:13.357088 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:54:13.357265 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:54:13.357360 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:54:13.771276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061b7360>]}
[0m12:54:13.790238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10808aad0>]}
[0m12:54:13.790462 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:54:13.837502 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:54:13.892701 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:54:13.893172 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/sources.yml
[0m12:54:14.023596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd63550>]}
[0m12:54:14.056830 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m12:54:14.057801 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m12:54:14.062477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082f1f40>]}
[0m12:54:14.062619 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m12:54:14.062718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db682f0>]}
[0m12:54:14.063480 [info ] [MainThread]: 
[0m12:54:14.063603 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:54:14.063690 [info ] [MainThread]: 
[0m12:54:14.063875 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:54:14.063967 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:54:14.066332 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:54:14.066511 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:54:14.066610 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:54:14.066816 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:54:14.066952 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:54:14.067103 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m12:54:14.070527 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:54:14.070638 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:54:14.071326 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:54:14.071432 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m12:54:14.071537 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:54:14.072308 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:54:14.072399 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:54:14.072992 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m12:54:14.073084 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:54:14.073172 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:54:14.073245 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:54:14.073321 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m12:54:14.073838 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:54:14.073980 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:54:14.896623 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-7f56-1339-89cb-263a2194a178) - Created
[0m12:54:14.918490 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-7f58-1ecd-9932-c7c9c3971010) - Created
[0m12:54:14.924716 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-7f5a-1f26-a0ec-1d6e252667fe) - Created
[0m12:54:14.929832 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-7f5b-1f2d-9133-7055238e6958) - Created
[0m12:54:15.587344 [debug] [ThreadPool]: SQL status: OK in 1.510 seconds
[0m12:54:15.587711 [debug] [ThreadPool]: SQL status: OK in 1.510 seconds
[0m12:54:15.587947 [debug] [ThreadPool]: SQL status: OK in 1.510 seconds
[0m12:54:15.588132 [debug] [ThreadPool]: SQL status: OK in 1.510 seconds
[0m12:54:15.596004 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-7f5a-1f26-a0ec-1d6e252667fe, command-id=01f0897d-7f76-1a92-a969-7244dbe7077c) - Closing
[0m12:54:15.596829 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-7f5b-1f2d-9133-7055238e6958, command-id=01f0897d-7f77-1305-a28e-3ac325ac0832) - Closing
[0m12:54:15.597498 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-7f56-1339-89cb-263a2194a178, command-id=01f0897d-7f74-1a32-92ae-6e92077dd76e) - Closing
[0m12:54:15.597860 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:54:15.598512 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-7f58-1ecd-9932-c7c9c3971010, command-id=01f0897d-7f78-17c0-a8f5-b7f0ab6f45a9) - Closing
[0m12:54:15.598851 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-7f5a-1f26-a0ec-1d6e252667fe) - Closing
[0m12:54:15.794011 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:54:15.794947 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-7f5b-1f2d-9133-7055238e6958) - Closing
[0m12:54:16.008324 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:54:16.009326 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-7f56-1339-89cb-263a2194a178) - Closing
[0m12:54:16.211166 [debug] [ThreadPool]: On list_flight_db: Close
[0m12:54:16.212204 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-7f58-1ecd-9932-c7c9c3971010) - Closing
[0m12:54:16.422951 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m12:54:16.423808 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m12:54:16.424106 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m12:54:16.424540 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m12:54:16.425023 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m12:54:16.425286 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m12:54:16.434346 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m12:54:16.434741 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m12:54:16.434975 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m12:54:16.437626 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m12:54:16.438133 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m12:54:16.440168 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m12:54:16.442144 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m12:54:16.442430 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m12:54:16.442662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:54:16.442897 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m12:54:16.443157 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m12:54:16.443369 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:54:16.443727 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:54:16.443940 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:54:17.192590 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-80b4-15cb-a5a3-306d4b929ed8) - Created
[0m12:54:17.231522 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-80bb-1187-b1cb-4c9edd750dc5) - Created
[0m12:54:17.234431 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-80ba-1ad0-83bf-c5af586666a5) - Created
[0m12:54:17.235863 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-80bc-11c0-9fb1-f3dbbf50608d) - Created
[0m12:54:17.778445 [debug] [ThreadPool]: SQL status: OK in 1.330 seconds
[0m12:54:17.781522 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-80b4-15cb-a5a3-306d4b929ed8, command-id=01f0897d-80ce-1f74-8bbe-b2a78d442680) - Closing
[0m12:54:17.782141 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m12:54:17.782418 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-80b4-15cb-a5a3-306d4b929ed8) - Closing
[0m12:54:17.835193 [debug] [ThreadPool]: SQL status: OK in 1.390 seconds
[0m12:54:17.837954 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-80bc-11c0-9fb1-f3dbbf50608d, command-id=01f0897d-80d7-1a7c-b2ad-2f81255d53e4) - Closing
[0m12:54:17.845959 [debug] [ThreadPool]: SQL status: OK in 1.400 seconds
[0m12:54:17.848017 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-80ba-1ad0-83bf-c5af586666a5, command-id=01f0897d-80d6-1f92-890e-8512ddad3279) - Closing
[0m12:54:17.916363 [debug] [ThreadPool]: SQL status: OK in 1.470 seconds
[0m12:54:17.919049 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897d-80bb-1187-b1cb-4c9edd750dc5, command-id=01f0897d-80d7-140c-b5a6-f7da19349dff) - Closing
[0m12:54:17.978894 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m12:54:17.979849 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-80bc-11c0-9fb1-f3dbbf50608d) - Closing
[0m12:54:18.180208 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m12:54:18.181230 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-80ba-1ad0-83bf-c5af586666a5) - Closing
[0m12:54:18.382217 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m12:54:18.383238 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897d-80bb-1187-b1cb-4c9edd750dc5) - Closing
[0m12:54:18.592609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e17d300>]}
[0m12:54:18.597527 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m12:54:18.597980 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m12:54:18.598296 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m12:54:18.598620 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m12:54:18.599217 [info ] [Thread-2 (]: 2 of 9 START sql view model flights_dbt_bronze.bronze_airports ................. [RUN]
[0m12:54:18.599674 [info ] [Thread-1 (]: 1 of 9 START sql table model flights_dbt_bronze.bronze_airlines ................ [RUN]
[0m12:54:18.600104 [info ] [Thread-4 (]: 4 of 9 START sql table model flights_dbt.my_first_dbt_model .................... [RUN]
[0m12:54:18.600502 [info ] [Thread-3 (]: 3 of 9 START sql table model flights_dbt_bronze.bronze_flights ................. [RUN]
[0m12:54:18.601107 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m12:54:18.601508 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m12:54:18.601882 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m12:54:18.602230 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m12:54:18.602510 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m12:54:18.602760 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m12:54:18.602998 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m12:54:18.603228 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m12:54:18.603491 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m12:54:18.603743 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m12:54:18.603977 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m12:54:18.604183 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m12:54:18.612451 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m12:54:18.616230 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m12:54:18.619077 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m12:54:18.625703 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m12:54:18.626234 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m12:54:18.626467 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m12:54:18.626682 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m12:54:18.626858 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m12:54:18.637161 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m12:54:18.638179 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m12:54:18.639094 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m12:54:18.646037 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m12:54:18.646272 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:54:18.646491 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:54:18.646764 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:54:18.647640 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:54:18.647837 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dddaed0>]}
[0m12:54:18.647966 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e350520>]}
[0m12:54:18.648084 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3505d0>]}
[0m12:54:18.648198 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3e8370>]}
[0m12:54:18.668398 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m12:54:18.668655 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m12:54:18.669226 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m12:54:18.673099 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
[0m12:54:18.673509 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m12:54:18.673739 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m12:54:18.673851 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m12:54:18.673954 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m12:54:18.674034 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m12:54:18.674165 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`flights_dbt`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m12:54:18.674308 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select upper(iata) as iata, airport_name, city, state
from `flight_db`.`raw`.`airports`
  )

[0m12:54:18.674426 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`airlines`
  
[0m12:54:18.674549 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select flight_id,
       airline_id,
       origin_airport,
       destination_airport,
       departure_time,
       arrival_time,
       delay
from `flight_db`.`raw`.`flights`
  
[0m12:54:18.674660 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:54:18.674752 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:54:18.674844 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:54:18.674936 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:54:19.431970 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897d-8207-1e15-9a71-9aed6b027583) - Created
[0m12:54:19.433389 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897d-8208-1794-9554-2820a91f54c3) - Created
[0m12:54:19.444883 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897d-820a-130e-8d30-c20e6bec81b4) - Created
[0m12:54:19.449748 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897d-820d-156a-8b84-0891a91914aa) - Created
[0m12:54:20.060526 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`airlines`
  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897d-8228-1597-a69d-aabc9ac9c0a2
[0m12:54:20.061848 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m12:54:20.062234 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897d-820d-156a-8b84-0891a91914aa) - Closing
[0m12:54:20.194207 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select upper(iata) as iata, airport_name, city, state
from `flight_db`.`raw`.`airports`
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`city`, `state`, `iata_code`, `country`, `airport_id`]. SQLSTATE: 42703; line 8 pos 17
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`city`, `state`, `iata_code`, `country`, `airport_id`]. SQLSTATE: 42703; line 8 pos 17
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`city`, `state`, `iata_code`, `country`, `airport_id`]. SQLSTATE: 42703; line 8 pos 17
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897d-8226-1dc7-8a91-141dd8318641
[0m12:54:20.270444 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m12:54:20.273830 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select flight_id,
       airline_id,
       origin_airport,
       destination_airport,
       departure_time,
       arrival_time,
       delay
from `flight_db`.`raw`.`flights`
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airline`, `flight_id`, `arr_time`, `origin`, `arr_delay`]. SQLSTATE: 42703; line 18 pos 7
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airline`, `flight_id`, `arr_time`, `origin`, `arr_delay`]. SQLSTATE: 42703; line 18 pos 7
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airline`, `flight_id`, `arr_time`, `origin`, `arr_delay`]. SQLSTATE: 42703; line 18 pos 7
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897d-8224-1d6c-a5e4-e1a2aca0cd31
[0m12:54:20.275650 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897d-820a-130e-8d30-c20e6bec81b4) - Closing
[0m12:54:20.292963 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m12:54:20.466253 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m12:54:20.466925 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897d-8208-1794-9554-2820a91f54c3) - Closing
[0m12:54:20.472475 [debug] [Thread-2 (]: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`city`, `state`, `iata_code`, `country`, `airport_id`]. SQLSTATE: 42703; line 8 pos 17
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m12:54:20.670501 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e149ef0>]}
[0m12:54:20.671163 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e149f60>]}
[0m12:54:20.672584 [debug] [Thread-3 (]: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airline`, `flight_id`, `arr_time`, `origin`, `arr_delay`]. SQLSTATE: 42703; line 18 pos 7
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m12:54:20.673454 [error] [Thread-1 (]: 1 of 9 ERROR creating sql table model flights_dbt_bronze.bronze_airlines ....... [[31mERROR[0m in 2.06s]
[0m12:54:20.674540 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e433110>]}
[0m12:54:20.674068 [error] [Thread-2 (]: 2 of 9 ERROR creating sql view model flights_dbt_bronze.bronze_airports ........ [[31mERROR[0m in 2.06s]
[0m12:54:20.675096 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m12:54:20.676135 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m12:54:20.675639 [error] [Thread-3 (]: 3 of 9 ERROR creating sql table model flights_dbt_bronze.bronze_flights ........ [[31mERROR[0m in 2.07s]
[0m12:54:20.676805 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m12:54:20.677884 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m12:54:20.679645 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airports' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`city`, `state`, `iata_code`, `country`, `airport_id`]. SQLSTATE: 42703; line 8 pos 17
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql.
[0m12:54:20.680271 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_airlines
[0m12:54:20.681000 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_flights' to be skipped because of status 'error'.  Reason: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airline`, `flight_id`, `arr_time`, `origin`, `arr_delay`]. SQLSTATE: 42703; line 18 pos 7
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql.
[0m12:54:20.682011 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airports
[0m12:54:20.681588 [info ] [Thread-1 (]: 5 of 9 SKIP relation flights_dbt_silver.silver_airlines ........................ [[33mSKIP[0m]
[0m12:54:20.682684 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_flights
[0m12:54:20.683029 [info ] [Thread-2 (]: 6 of 9 SKIP relation flights_dbt_silver.silver_airports ........................ [[33mSKIP[0m]
[0m12:54:20.683416 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_airlines
[0m12:54:20.683722 [info ] [Thread-3 (]: 7 of 9 SKIP relation flights_dbt_silver.silver_flights ......................... [[33mSKIP[0m]
[0m12:54:20.684095 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airports
[0m12:54:20.684502 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_flights
[0m12:54:20.684969 [debug] [Thread-1 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m12:54:20.685280 [info ] [Thread-1 (]: 8 of 9 SKIP relation flights_dbt_gold.gold_flight_metrics ...................... [[33mSKIP[0m]
[0m12:54:20.685571 [debug] [Thread-1 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m12:54:21.292261 [debug] [Thread-4 (]: SQL status: OK in 2.620 seconds
[0m12:54:21.294050 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0897d-8207-1e15-9a71-9aed6b027583, command-id=01f0897d-8224-1a2f-b56a-11251eaea2c4) - Closing
[0m12:54:21.311236 [debug] [Thread-4 (]: Applying tags to relation None
[0m12:54:21.324992 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m12:54:21.325280 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897d-8207-1e15-9a71-9aed6b027583) - Closing
[0m12:54:21.520080 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd85a30>]}
[0m12:54:21.521832 [info ] [Thread-4 (]: 4 of 9 OK created sql table model flights_dbt.my_first_dbt_model ............... [[32mOK[0m in 2.92s]
[0m12:54:21.522673 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m12:54:21.523590 [debug] [Thread-2 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m12:54:21.524168 [info ] [Thread-2 (]: 9 of 9 START sql view model flights_dbt.my_second_dbt_model .................... [RUN]
[0m12:54:21.524885 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m12:54:21.525323 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m12:54:21.525643 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m12:54:21.529486 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m12:54:21.530307 [debug] [Thread-2 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m12:54:21.532927 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m12:54:21.533789 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt`.`my_second_dbt_model`
[0m12:54:21.534415 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m12:54:21.534887 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m12:54:21.535148 [debug] [Thread-2 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`flights_dbt`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`flights_dbt`.`my_first_dbt_model`
where id = 1
  )

[0m12:54:21.535382 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:54:22.186978 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897d-83ad-1da4-97d5-1c43bda44513) - Created
[0m12:54:22.756699 [debug] [Thread-2 (]: SQL status: OK in 1.220 seconds
[0m12:54:22.758054 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0897d-83ad-1da4-97d5-1c43bda44513, command-id=01f0897d-83c9-169c-942e-d2aaec962ce8) - Closing
[0m12:54:22.758929 [debug] [Thread-2 (]: Applying tags to relation None
[0m12:54:22.761069 [debug] [Thread-2 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m12:54:22.761509 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897d-83ad-1da4-97d5-1c43bda44513) - Closing
[0m12:54:22.970323 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69207518-d05d-4dec-97c8-6541409a3ce1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e12d130>]}
[0m12:54:22.971631 [info ] [Thread-2 (]: 9 of 9 OK created sql view model flights_dbt.my_second_dbt_model ............... [[32mOK[0m in 1.45s]
[0m12:54:22.972546 [debug] [Thread-2 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m12:54:22.975369 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:54:22.976230 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:54:22.977327 [info ] [MainThread]: 
[0m12:54:22.978478 [info ] [MainThread]: Finished running 7 table models, 2 view models in 0 hours 0 minutes and 8.91 seconds (8.91s).
[0m12:54:22.980206 [debug] [MainThread]: Command end result
[0m12:54:23.013152 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m12:54:23.014750 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m12:54:23.018096 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m12:54:23.018251 [info ] [MainThread]: 
[0m12:54:23.018415 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m12:54:23.018547 [info ] [MainThread]: 
[0m12:54:23.018717 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m12:54:23.018876 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m12:54:23.019005 [info ] [MainThread]: 
[0m12:54:23.019132 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m12:54:23.019237 [info ] [MainThread]: 
[0m12:54:23.019369 [error] [MainThread]: [31mFailure in model bronze_airports (models/bronze/bronze_airports.sql)[0m
[0m12:54:23.019506 [error] [MainThread]:   Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`city`, `state`, `iata_code`, `country`, `airport_id`]. SQLSTATE: 42703; line 8 pos 17
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m12:54:23.019615 [info ] [MainThread]: 
[0m12:54:23.019747 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airports.sql
[0m12:54:23.019845 [info ] [MainThread]: 
[0m12:54:23.019981 [error] [MainThread]: [31mFailure in model bronze_flights (models/bronze/bronze_flights.sql)[0m
[0m12:54:23.020244 [error] [MainThread]:   Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airline`, `flight_id`, `arr_time`, `origin`, `arr_delay`]. SQLSTATE: 42703; line 18 pos 7
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m12:54:23.020410 [info ] [MainThread]: 
[0m12:54:23.020560 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_flights.sql
[0m12:54:23.020682 [info ] [MainThread]: 
[0m12:54:23.020820 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=3 SKIP=4 NO-OP=0 TOTAL=9
[0m12:54:23.024097 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.983365, "process_in_blocks": "0", "process_kernel_time": 0.27933, "process_mem_max_rss": "255098880", "process_out_blocks": "0", "process_user_time": 3.302282}
[0m12:54:23.024312 [debug] [MainThread]: Command `dbt run` failed at 12:54:23.024271 after 9.98 seconds
[0m12:54:23.024491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060deab0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2db1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2dbd70>]}
[0m12:54:23.024643 [debug] [MainThread]: Flushing usage events
[0m12:54:24.296611 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:57:40.863075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10457f620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105757890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105757b10>]}


============================== 12:57:40.865392 | ee5953ef-2ef6-4509-a818-7d37f74191d8 ==============================
[0m12:57:40.865392 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:57:40.865618 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'debug': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'indirect_selection': 'eager', 'fail_fast': 'False', 'version_check': 'True', 'introspect': 'True', 'log_format': 'default', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'use_experimental_parser': 'False', 'target_path': 'None', 'invocation_command': 'dbt run --target dev', 'partial_parse': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'write_json': 'True', 'printer_width': '80', 'empty': 'False', 'warn_error': 'None', 'cache_selected_only': 'False', 'no_print': 'None'}
[0m12:57:41.158944 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:57:41.159130 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:57:41.159228 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:57:41.589951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ee5953ef-2ef6-4509-a818-7d37f74191d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a7f360>]}
[0m12:57:41.608704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ee5953ef-2ef6-4509-a818-7d37f74191d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106962ad0>]}
[0m12:57:41.608927 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:57:41.657699 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:57:41.713570 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 2 files changed.
[0m12:57:41.713800 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/sources.yml
[0m12:57:41.713904 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/bronze_weather.sql
[0m12:57:41.714011 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m12:57:41.714117 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m12:57:41.806600 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_flights_airports".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_flights", "airports").
  
  To fix this, change the name of one of these resources:
  - source.flights_dbt.raw_flights.airports (models/bronze/sources.yml)
  - source.flights_dbt.raw_flights.airports (models/sources.yml)
[0m12:57:41.809183 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9759319, "process_in_blocks": "0", "process_kernel_time": 0.210692, "process_mem_max_rss": "229916672", "process_out_blocks": "0", "process_user_time": 1.070259}
[0m12:57:41.809371 [debug] [MainThread]: Command `dbt run` failed at 12:57:41.809334 after 0.98 seconds
[0m12:57:41.809538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c91c650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c91c750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7ea210>]}
[0m12:57:41.809657 [debug] [MainThread]: Flushing usage events
[0m12:57:42.482903 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:58:40.271878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109d3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129fbb10>]}


============================== 12:58:40.274107 | 8de88cfb-ebf1-4796-9d33-ab3bcdc2426a ==============================
[0m12:58:40.274107 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:58:40.274328 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --target dev', 'target_path': 'None', 'no_print': 'None', 'debug': 'False', 'version_check': 'True', 'quiet': 'False', 'warn_error': 'None', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'empty': 'False', 'printer_width': '80', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'introspect': 'True', 'log_format': 'default', 'profiles_dir': '/Users/artakerqeli/.dbt', 'fail_fast': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'use_colors': 'True'}
[0m12:58:40.565130 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:58:40.565321 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:58:40.565422 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:58:41.018313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8de88cfb-ebf1-4796-9d33-ab3bcdc2426a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111927360>]}
[0m12:58:41.036924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8de88cfb-ebf1-4796-9d33-ab3bcdc2426a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114906ad0>]}
[0m12:58:41.037177 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:58:41.085815 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:58:41.142146 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 2 files changed.
[0m12:58:41.142394 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/sources.yml
[0m12:58:41.142504 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/bronze_weather.sql
[0m12:58:41.142615 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m12:58:41.142721 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m12:58:41.233904 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_flights_airports".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_flights", "airports").
  
  To fix this, change the name of one of these resources:
  - source.flights_dbt.raw_flights.airports (models/bronze/sources.yml)
  - source.flights_dbt.raw_flights.airports (models/sources.yml)
[0m12:58:41.237646 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9945043, "process_in_blocks": "0", "process_kernel_time": 0.20542, "process_mem_max_rss": "230408192", "process_out_blocks": "0", "process_user_time": 1.064708}
[0m12:58:41.237851 [debug] [MainThread]: Command `dbt run` failed at 12:58:41.237811 after 0.99 seconds
[0m12:58:41.238045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12484c650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12484c750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12471a210>]}
[0m12:58:41.238164 [debug] [MainThread]: Flushing usage events
[0m12:58:41.994733 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:59:10.919315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c27620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dff890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dffb10>]}


============================== 12:59:10.921488 | 1b5a438d-21f8-4a84-b016-b70b2a710bf1 ==============================
[0m12:59:10.921488 [info ] [MainThread]: Running with dbt=1.10.9
[0m12:59:10.921718 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'quiet': 'False', 'no_print': 'None', 'empty': 'False', 'invocation_command': 'dbt run --target dev', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'debug': 'False', 'log_cache_events': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'cache_selected_only': 'False', 'version_check': 'True', 'partial_parse': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'static_parser': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None', 'printer_width': '80', 'use_colors': 'True'}
[0m12:59:11.205730 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:59:11.205916 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:59:11.206012 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:59:11.626260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1b5a438d-21f8-4a84-b016-b70b2a710bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10412b360>]}
[0m12:59:11.645043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1b5a438d-21f8-4a84-b016-b70b2a710bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106006ad0>]}
[0m12:59:11.645285 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m12:59:11.695763 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m12:59:11.751945 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 2 files changed.
[0m12:59:11.752169 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/bronze_weather.sql
[0m12:59:11.752290 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/sources.yml
[0m12:59:11.752403 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m12:59:11.752520 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m12:59:11.845295 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_flights_airports".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_flights", "airports").
  
  To fix this, change the name of one of these resources:
  - source.flights_dbt.raw_flights.airports (models/bronze/sources.yml)
  - source.flights_dbt.raw_flights.airports (models/sources.yml)
[0m12:59:11.853302 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.96662056, "process_in_blocks": "0", "process_kernel_time": 0.213387, "process_mem_max_rss": "229294080", "process_out_blocks": "0", "process_user_time": 1.065083}
[0m12:59:11.853557 [debug] [MainThread]: Command `dbt run` failed at 12:59:11.853510 after 0.97 seconds
[0m12:59:11.853753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bfc4650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bfc4750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be92210>]}
[0m12:59:11.853892 [debug] [MainThread]: Flushing usage events
[0m12:59:12.513704 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:01:11.848380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057bb620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069fbb10>]}


============================== 13:01:11.850624 | 41c2fe5d-18c0-4d9b-8b87-dba4badbd24f ==============================
[0m13:01:11.850624 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:01:11.850860 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'introspect': 'True', 'empty': 'False', 'invocation_command': 'dbt run --target dev', 'quiet': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'partial_parse': 'True', 'use_colors': 'True', 'fail_fast': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'log_format': 'default', 'static_parser': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'printer_width': '80', 'version_check': 'True', 'debug': 'False', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'warn_error': 'None'}
[0m13:01:12.142980 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:01:12.143160 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:01:12.143255 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:01:12.568742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '41c2fe5d-18c0-4d9b-8b87-dba4badbd24f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b03360>]}
[0m13:01:12.587337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '41c2fe5d-18c0-4d9b-8b87-dba4badbd24f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a06ad0>]}
[0m13:01:12.587566 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:01:12.637504 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m13:01:12.693875 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 2 files changed.
[0m13:01:12.694100 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/bronze_weather.sql
[0m13:01:12.694227 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:01:12.694334 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m13:01:12.761192 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.flights_dbt.bronze_weather' (models/bronze/bronze_weather.sql) depends on a source named 'raw_flights.weather' which was not found
[0m13:01:12.763594 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.94453114, "process_in_blocks": "0", "process_kernel_time": 0.207459, "process_mem_max_rss": "228573184", "process_out_blocks": "0", "process_user_time": 1.039504}
[0m13:01:12.763849 [debug] [MainThread]: Command `dbt run` failed at 13:01:12.763811 after 0.94 seconds
[0m13:01:12.764007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11750c750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11750c050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117815040>]}
[0m13:01:12.764124 [debug] [MainThread]: Flushing usage events
[0m13:01:13.516392 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:01:50.874841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083eb620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095c3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095c3b10>]}


============================== 13:01:50.877034 | da9fed85-950c-47df-a832-6809ebdd7ace ==============================
[0m13:01:50.877034 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:01:50.877279 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'empty': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'no_print': 'None', 'invocation_command': 'dbt run --target dev', 'target_path': 'None', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'static_parser': 'True', 'version_check': 'True', 'debug': 'False', 'introspect': 'True', 'log_format': 'default', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/artakerqeli/.dbt', 'quiet': 'False', 'partial_parse': 'True', 'printer_width': '80', 'fail_fast': 'False', 'warn_error': 'None'}
[0m13:01:51.164634 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:01:51.164830 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:01:51.164924 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:01:51.593144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'da9fed85-950c-47df-a832-6809ebdd7ace', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088ef360>]}
[0m13:01:51.612476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'da9fed85-950c-47df-a832-6809ebdd7ace', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b606ad0>]}
[0m13:01:51.612710 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:01:51.664227 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m13:01:51.718637 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 2 files changed.
[0m13:01:51.718849 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/sources.yml
[0m13:01:51.718952 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/bronze_weather.sql
[0m13:01:51.719064 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:01:51.719168 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m13:01:51.810472 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_flights_airports".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_flights", "airports").
  
  To fix this, change the name of one of these resources:
  - source.flights_dbt.raw_flights.airports (models/bronze/sources.yml)
  - source.flights_dbt.raw_flights.airports (models/sources.yml)
[0m13:01:51.812599 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9644524, "process_in_blocks": "0", "process_kernel_time": 0.206491, "process_mem_max_rss": "230686720", "process_out_blocks": "0", "process_user_time": 1.070071}
[0m13:01:51.812791 [debug] [MainThread]: Command `dbt run` failed at 13:01:51.812749 after 0.96 seconds
[0m13:01:51.812960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b94c650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b94c750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b71a210>]}
[0m13:01:51.813079 [debug] [MainThread]: Flushing usage events
[0m13:01:52.476482 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:03:38.172200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081c3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091db890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091dbb10>]}


============================== 13:03:38.174433 | 46c226fc-990b-4c8f-a05f-ae8b059520c5 ==============================
[0m13:03:38.174433 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:03:38.174675 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'invocation_command': 'dbt run --target dev', 'profiles_dir': '/Users/artakerqeli/.dbt', 'no_print': 'None', 'indirect_selection': 'eager', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'write_json': 'True', 'log_cache_events': 'False', 'target_path': 'None', 'debug': 'False', 'use_colors': 'True', 'quiet': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'log_format': 'default', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error': 'None', 'empty': 'False', 'cache_selected_only': 'False'}
[0m13:03:38.464324 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:03:38.464520 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:03:38.464616 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:03:38.883767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '46c226fc-990b-4c8f-a05f-ae8b059520c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10850b360>]}
[0m13:03:38.902826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '46c226fc-990b-4c8f-a05f-ae8b059520c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3e6ad0>]}
[0m13:03:38.903052 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:03:38.951069 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m13:03:39.006012 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 2 files changed.
[0m13:03:39.006270 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/bronze_weather.sql
[0m13:03:39.006400 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/sources.yml
[0m13:03:39.006518 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:03:39.006627 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m13:03:39.077087 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.flights_dbt.bronze_weather' (models/bronze/bronze_weather.sql) depends on a source named 'raw_flights.weather' which was not found
[0m13:03:39.079119 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.93618536, "process_in_blocks": "0", "process_kernel_time": 0.207544, "process_mem_max_rss": "227852288", "process_out_blocks": "0", "process_user_time": 1.053624}
[0m13:03:39.079384 [debug] [MainThread]: Command `dbt run` failed at 13:03:39.079344 after 0.94 seconds
[0m13:03:39.079563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100de350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11051c150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1103e9d60>]}
[0m13:03:39.079696 [debug] [MainThread]: Flushing usage events
[0m13:03:39.764425 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:04:20.717247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103ed7620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105bfb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105bfbb10>]}


============================== 13:04:20.719647 | ceb9d24d-9930-4cbc-af8f-5e34d7aef179 ==============================
[0m13:04:20.719647 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:04:20.719901 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'write_json': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'introspect': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'warn_error': 'None', 'quiet': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'partial_parse': 'True', 'version_check': 'True', 'invocation_command': 'dbt run --target dev', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'target_path': 'None', 'log_format': 'default', 'debug': 'False', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt'}
[0m13:04:21.008672 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:04:21.008867 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:04:21.008962 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:04:21.429405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ceb9d24d-9930-4cbc-af8f-5e34d7aef179', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043df360>]}
[0m13:04:21.448324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ceb9d24d-9930-4cbc-af8f-5e34d7aef179', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b06ad0>]}
[0m13:04:21.448544 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:04:21.500483 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m13:04:21.539746 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading flights_dbt: sources.yml - Runtime Error
    Syntax error near line 8
    ------------------------------
    5  |     schema: raw
    6  |     tables:
    7  |        - name: airports
    8  |       - name: flights
    9  |       - name: weather
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 3, column 5
    did not find expected key
      in "<unicode string>", line 8, column 7
[0m13:04:21.541735 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.85318923, "process_in_blocks": "0", "process_kernel_time": 0.201378, "process_mem_max_rss": "223674368", "process_out_blocks": "0", "process_user_time": 0.978682}
[0m13:04:21.541960 [debug] [MainThread]: Command `dbt run` failed at 13:04:21.541922 after 0.85 seconds
[0m13:04:21.542127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11650f450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11650f050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1164f2210>]}
[0m13:04:21.542248 [debug] [MainThread]: Flushing usage events
[0m13:04:22.214540 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:04:53.454349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10444b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105463890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105463b10>]}


============================== 13:04:53.456585 | 7ce8a8b3-ef89-4e33-a335-da604bbe36b4 ==============================
[0m13:04:53.456585 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:04:53.456846 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'partial_parse': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'log_format': 'default', 'static_parser': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'introspect': 'True', 'debug': 'False', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'target_path': 'None', 'empty': 'False', 'log_cache_events': 'False', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'indirect_selection': 'eager', 'write_json': 'True', 'invocation_command': 'dbt run --target dev'}
[0m13:04:53.743585 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:04:53.743783 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:04:53.743880 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:04:54.170399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7ce8a8b3-ef89-4e33-a335-da604bbe36b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10478f360>]}
[0m13:04:54.189213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7ce8a8b3-ef89-4e33-a335-da604bbe36b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10666ead0>]}
[0m13:04:54.189440 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:04:54.240395 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m13:04:54.297321 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 3 files changed.
[0m13:04:54.297548 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/bronze_weather.sql
[0m13:04:54.297667 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/sources.yml
[0m13:04:54.297909 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/sources.yml
[0m13:04:54.298009 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m13:04:54.298101 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:04:54.402601 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.flights_dbt.bronze_airlines' (models/bronze/bronze_airlines.sql) depends on a source named 'raw_flights.airlines' which was not found
[0m13:04:54.405155 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9788156, "process_in_blocks": "0", "process_kernel_time": 0.223962, "process_mem_max_rss": "231882752", "process_out_blocks": "0", "process_user_time": 1.092499}
[0m13:04:54.405340 [debug] [MainThread]: Command `dbt run` failed at 13:04:54.405303 after 0.98 seconds
[0m13:04:54.405507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c33f750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c33de50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c67da90>]}
[0m13:04:54.405625 [debug] [MainThread]: Flushing usage events
[0m13:04:55.078004 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:06:06.192473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107927620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108aff890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108affb10>]}


============================== 13:06:06.194696 | e8c7f3cd-462e-4772-932a-8b1e79b01a08 ==============================
[0m13:06:06.194696 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:06:06.194940 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'fail_fast': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'invocation_command': 'dbt run --target dev', 'profiles_dir': '/Users/artakerqeli/.dbt', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'version_check': 'True', 'use_colors': 'True', 'debug': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'cache_selected_only': 'False', 'introspect': 'True', 'write_json': 'True', 'partial_parse': 'True', 'no_print': 'None', 'log_format': 'default', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'warn_error': 'None'}
[0m13:06:06.485759 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:06:06.485944 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:06:06.486041 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:06:06.920500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e2f360>]}
[0m13:06:06.939371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d06ad0>]}
[0m13:06:06.939589 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:06:06.987153 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m13:06:07.044454 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 4 files changed.
[0m13:06:07.044697 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/sources.yml
[0m13:06:07.044805 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/bronze_weather.sql
[0m13:06:07.045077 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/sources.yml
[0m13:06:07.045175 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m13:06:07.045258 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m13:06:07.045338 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:06:07.180702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a0e050>]}
[0m13:06:07.214613 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:06:07.215896 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:06:07.221135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199f2120>]}
[0m13:06:07.221290 [info ] [MainThread]: Found 10 models, 4 data tests, 3 sources, 686 macros
[0m13:06:07.221392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119815d30>]}
[0m13:06:07.222138 [info ] [MainThread]: 
[0m13:06:07.222254 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:06:07.222336 [info ] [MainThread]: 
[0m13:06:07.222508 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:06:07.222604 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:06:07.224994 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:06:07.225171 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:06:07.225267 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:06:07.225458 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:06:07.225565 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:06:07.225737 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:06:07.229263 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:06:07.229380 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:06:07.230044 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:06:07.230130 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:06:07.230232 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:06:07.230957 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:06:07.231045 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:06:07.231682 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:06:07.231779 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:06:07.231880 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:06:07.231970 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:06:07.232058 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:06:07.232621 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:06:07.232757 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:06:08.394669 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-287e-1a97-9e9f-1d140ee2bb25) - Created
[0m13:06:08.397491 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-2883-1093-8327-9c41f7c9deac) - Created
[0m13:06:08.398841 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-2882-1aa2-930f-1935faa6bacc) - Created
[0m13:06:08.399899 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-288b-1687-bb6d-040555508419) - Created
[0m13:06:19.980661 [debug] [ThreadPool]: SQL status: OK in 12.750 seconds
[0m13:06:19.981177 [debug] [ThreadPool]: SQL status: OK in 12.750 seconds
[0m13:06:19.981434 [debug] [ThreadPool]: SQL status: OK in 12.750 seconds
[0m13:06:19.981660 [debug] [ThreadPool]: SQL status: OK in 12.750 seconds
[0m13:06:19.993374 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-2882-1aa2-930f-1935faa6bacc, command-id=01f0897f-28b9-1ac7-8f7f-84e9bd2eada2) - Closing
[0m13:06:19.994500 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-2883-1093-8327-9c41f7c9deac, command-id=01f0897f-28b9-14aa-8381-a19ebfc11bf0) - Closing
[0m13:06:19.995389 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-287e-1a97-9e9f-1d140ee2bb25, command-id=01f0897f-28b9-1116-a6c1-f8500624ede2) - Closing
[0m13:06:19.996516 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-288b-1687-bb6d-040555508419, command-id=01f0897f-28bb-1517-b7f0-c07a43c65105) - Closing
[0m13:06:20.373655 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:06:20.375682 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-287e-1a97-9e9f-1d140ee2bb25) - Closing
[0m13:06:20.565860 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:06:20.566912 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-2883-1093-8327-9c41f7c9deac) - Closing
[0m13:06:20.760487 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:06:20.761458 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-2882-1aa2-930f-1935faa6bacc) - Closing
[0m13:06:21.176304 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:06:21.177341 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-288b-1687-bb6d-040555508419) - Closing
[0m13:06:21.407200 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m13:06:21.407878 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m13:06:21.408376 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m13:06:21.417472 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m13:06:21.417907 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m13:06:21.418282 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m13:06:21.418483 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m13:06:21.418836 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m13:06:21.419079 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m13:06:21.419258 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m13:06:21.422158 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m13:06:21.422547 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:06:21.424297 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m13:06:21.425996 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m13:06:21.426284 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m13:06:21.426789 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m13:06:21.427241 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m13:06:21.427492 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:06:21.427681 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:06:21.427849 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:06:22.263687 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-30e1-1778-adbd-a727dfd03fd8) - Created
[0m13:06:22.266665 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-30e0-18ad-852b-3620bc8e9245) - Created
[0m13:06:22.268080 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-30e1-1bf7-af13-a7b9489d45ba) - Created
[0m13:06:22.284034 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-30e0-1960-a3b1-805928a6cbf2) - Created
[0m13:06:25.460816 [debug] [ThreadPool]: SQL status: OK in 4.030 seconds
[0m13:06:25.461406 [debug] [ThreadPool]: SQL status: OK in 4.040 seconds
[0m13:06:25.464464 [debug] [ThreadPool]: SQL status: OK in 4.040 seconds
[0m13:06:25.467788 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-30e1-1bf7-af13-a7b9489d45ba, command-id=01f0897f-30fe-1552-9dcf-957a0e0c030b) - Closing
[0m13:06:25.468986 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-30e1-1778-adbd-a727dfd03fd8, command-id=01f0897f-30fd-1388-8af4-0383365c6e83) - Closing
[0m13:06:25.469435 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m13:06:25.470558 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-30e0-18ad-852b-3620bc8e9245, command-id=01f0897f-30fd-16d9-874f-e29708a19160) - Closing
[0m13:06:25.470955 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-30e1-1bf7-af13-a7b9489d45ba) - Closing
[0m13:06:25.662737 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m13:06:25.663718 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-30e1-1778-adbd-a727dfd03fd8) - Closing
[0m13:06:25.860995 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m13:06:25.862037 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-30e0-18ad-852b-3620bc8e9245) - Closing
[0m13:06:26.922868 [debug] [ThreadPool]: SQL status: OK in 5.500 seconds
[0m13:06:26.926127 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-30e0-1960-a3b1-805928a6cbf2, command-id=01f0897f-310c-19d2-9560-623c61e0868f) - Closing
[0m13:06:26.926999 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m13:06:26.927338 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-30e0-1960-a3b1-805928a6cbf2) - Closing
[0m13:06:27.111180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a0293d0>]}
[0m13:06:27.116551 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m13:06:27.117043 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m13:06:27.117384 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m13:06:27.117673 [debug] [Thread-4 (]: Began running node model.flights_dbt.bronze_weather
[0m13:06:27.118251 [info ] [Thread-3 (]: 3 of 10 START sql table model flights_dbt_bronze.bronze_flights ................ [RUN]
[0m13:06:27.118812 [info ] [Thread-1 (]: 1 of 10 START sql table model flights_dbt_bronze.bronze_airlines ............... [RUN]
[0m13:06:27.119330 [info ] [Thread-2 (]: 2 of 10 START sql view model flights_dbt_bronze.bronze_airports ................ [RUN]
[0m13:06:27.119843 [info ] [Thread-4 (]: 4 of 10 START sql table model flights_dbt_bronze.bronze_weather ................ [RUN]
[0m13:06:27.120573 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m13:06:27.121106 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m13:06:27.121619 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m13:06:27.122047 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_weather) - Creating connection
[0m13:06:27.122393 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m13:06:27.122706 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m13:06:27.122964 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m13:06:27.123210 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_weather'
[0m13:06:27.123478 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m13:06:27.123726 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m13:06:27.123955 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m13:06:27.124181 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.bronze_weather
[0m13:06:27.132055 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m13:06:27.135019 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m13:06:27.136627 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.bronze_weather"
[0m13:06:27.140062 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m13:06:27.140315 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m13:06:27.147653 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m13:06:27.147893 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m13:06:27.148127 [debug] [Thread-4 (]: Began executing node model.flights_dbt.bronze_weather
[0m13:06:27.149242 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:06:27.158729 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m13:06:27.158947 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m13:06:27.160081 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m13:06:27.160273 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a87710>]}
[0m13:06:27.160493 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:06:27.161494 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m13:06:27.161700 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:06:27.168249 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f962b0>]}
[0m13:06:27.168431 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
[0m13:06:27.174844 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aa79e50>]}
[0m13:06:27.185787 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m13:06:27.186428 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m13:06:27.186693 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m13:06:27.187269 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.bronze_weather"
[0m13:06:27.187711 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m13:06:27.187828 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m13:06:27.187996 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    flight_id,
    airline,
    origin,
    dest,
    arr_time,
    dep_time,
    arr_delay,
    dep_delay
from `flight_db`.`raw`.`flights`
  
[0m13:06:27.188108 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.bronze_weather"
[0m13:06:27.188244 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`airports`
  
[0m13:06:27.188368 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m13:06:27.188486 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:06:27.188613 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_weather"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_weather`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`weather`
  
[0m13:06:27.188739 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:06:27.188876 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select 
    upper(iata_code) as iata,  -- use iata_code if iata doesn’t exist
    airport_name,
    city,
    state
from `flight_db`.`raw`.`airports`
  )

[0m13:06:27.189063 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:06:27.189250 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:06:27.895458 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-343d-1b1d-bcec-85188a275421) - Created
[0m13:06:27.951708 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-3445-1195-9703-8219d5fb1a69) - Created
[0m13:06:27.953931 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-3445-1bec-a517-d1de3a2e9616) - Created
[0m13:06:27.955274 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897f-3445-12c5-b3b3-447ce4714c3c) - Created
[0m13:06:29.379511 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select 
    upper(iata_code) as iata,  -- use iata_code if iata doesn’t exist
    airport_name,
    city,
    state
from `flight_db`.`raw`.`airports`
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 10 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 10 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 10 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897f-3461-181f-a8ff-f57159fa54e7
[0m13:06:29.385929 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m13:06:29.387120 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-3445-1bec-a517-d1de3a2e9616) - Closing
[0m13:06:29.393948 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    flight_id,
    airline,
    origin,
    dest,
    arr_time,
    dep_time,
    arr_delay,
    dep_delay
from `flight_db`.`raw`.`flights`
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897f-3461-118d-8aa1-102db8803dbb
[0m13:06:29.600930 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m13:06:29.601736 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-3445-1195-9703-8219d5fb1a69) - Closing
[0m13:06:29.613085 [debug] [Thread-2 (]: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 10 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m13:06:29.810439 [debug] [Thread-3 (]: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m13:06:29.810991 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107deded0>]}
[0m13:06:29.811396 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a0bb6d0>]}
[0m13:06:29.812153 [error] [Thread-2 (]: 2 of 10 ERROR creating sql view model flights_dbt_bronze.bronze_airports ....... [[31mERROR[0m in 2.68s]
[0m13:06:29.813270 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m13:06:29.812718 [error] [Thread-3 (]: 3 of 10 ERROR creating sql table model flights_dbt_bronze.bronze_flights ....... [[31mERROR[0m in 2.69s]
[0m13:06:29.813672 [debug] [Thread-2 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m13:06:29.814016 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airports' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 10 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql.
[0m13:06:29.814366 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m13:06:29.814686 [info ] [Thread-2 (]: 5 of 10 START sql table model flights_dbt.my_first_dbt_model ................... [RUN]
[0m13:06:29.815813 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_flights' to be skipped because of status 'error'.  Reason: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql.
[0m13:06:29.816081 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airports
[0m13:06:29.816460 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m13:06:29.816814 [info ] [Thread-3 (]: 6 of 10 SKIP relation flights_dbt_silver.silver_airports ....................... [[33mSKIP[0m]
[0m13:06:29.817080 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m13:06:29.817320 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airports
[0m13:06:29.817543 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m13:06:29.820126 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m13:06:29.820701 [debug] [Thread-2 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m13:06:29.822053 [debug] [Thread-2 (]: MATERIALIZING TABLE
[0m13:06:29.823796 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m13:06:29.824197 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m13:06:29.824451 [debug] [Thread-2 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`flights_dbt`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:06:29.824675 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:06:30.470863 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-35c5-1afe-bcf0-d2f2174fc042) - Created
[0m13:06:36.980949 [debug] [Thread-4 (]: SQL status: OK in 9.790 seconds
[0m13:06:36.983188 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0897f-3445-12c5-b3b3-447ce4714c3c, command-id=01f0897f-3461-1ab4-9771-b9ba7cfad369) - Closing
[0m13:06:37.013311 [debug] [Thread-1 (]: SQL status: OK in 9.820 seconds
[0m13:06:37.014843 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0897f-343d-1b1d-bcec-85188a275421, command-id=01f0897f-3458-10e6-8704-9d4d7c54d784) - Closing
[0m13:06:37.260002 [debug] [Thread-1 (]: Applying tags to relation None
[0m13:06:37.261452 [debug] [Thread-4 (]: Applying tags to relation None
[0m13:06:37.275787 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m13:06:37.276666 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-343d-1b1d-bcec-85188a275421) - Closing
[0m13:06:37.511573 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: Close
[0m13:06:37.513082 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897f-3445-12c5-b3b3-447ce4714c3c) - Closing
[0m13:06:37.721208 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a143a10>]}
[0m13:06:37.722249 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a143c50>]}
[0m13:06:37.723750 [info ] [Thread-1 (]: 1 of 10 OK created sql table model flights_dbt_bronze.bronze_airlines .......... [[32mOK[0m in 10.60s]
[0m13:06:37.724837 [info ] [Thread-4 (]: 4 of 10 OK created sql table model flights_dbt_bronze.bronze_weather ........... [[32mOK[0m in 10.60s]
[0m13:06:37.725504 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m13:06:37.726037 [debug] [Thread-4 (]: Finished running node model.flights_dbt.bronze_weather
[0m13:06:37.726837 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airlines
[0m13:06:37.727190 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_flights
[0m13:06:37.727593 [info ] [Thread-3 (]: 7 of 10 START sql table model flights_dbt_silver.silver_airlines ............... [RUN]
[0m13:06:37.727990 [info ] [Thread-1 (]: 8 of 10 SKIP relation flights_dbt_silver.silver_flights ........................ [[33mSKIP[0m]
[0m13:06:37.728667 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airlines) - Creating connection
[0m13:06:37.729169 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_flights
[0m13:06:37.729566 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airlines'
[0m13:06:37.730035 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_airlines
[0m13:06:37.735124 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_airlines"
[0m13:06:37.736581 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_airlines
[0m13:06:37.740014 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m13:06:37.741867 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.silver_airlines"
[0m13:06:37.742832 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.silver_airlines"
[0m13:06:37.743209 [debug] [Thread-3 (]: On model.flights_dbt.silver_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_silver`.`silver_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select distinct airline_code, airline_name
from `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
[0m13:06:37.743518 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:06:38.431142 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-3a84-1217-b5cd-e1e99a8a4a2a) - Created
[0m13:06:38.882806 [debug] [Thread-2 (]: SQL status: OK in 9.060 seconds
[0m13:06:38.884867 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0897f-35c5-1afe-bcf0-d2f2174fc042, command-id=01f0897f-35e1-191c-9fa1-59aa11fb1956) - Closing
[0m13:06:38.971176 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_silver`.`silver_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select distinct airline_code, airline_name
from `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_code` cannot be resolved. Did you mean one of the following? [`iata_code`, `airport_id`, `city`, `country`, `state`]. SQLSTATE: 42703; line 17 pos 22
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_code` cannot be resolved. Did you mean one of the following? [`iata_code`, `airport_id`, `city`, `country`, `state`]. SQLSTATE: 42703; line 17 pos 22
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_code` cannot be resolved. Did you mean one of the following? [`iata_code`, `airport_id`, `city`, `country`, `state`]. SQLSTATE: 42703; line 17 pos 22
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897f-3a9f-1566-9f8f-a4c49d550eaa
[0m13:06:38.973142 [debug] [Thread-3 (]: On model.flights_dbt.silver_airlines: Close
[0m13:06:38.973725 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-3a84-1217-b5cd-e1e99a8a4a2a) - Closing
[0m13:06:39.129831 [debug] [Thread-2 (]: Applying tags to relation None
[0m13:06:39.181989 [debug] [Thread-2 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m13:06:39.183124 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-35c5-1afe-bcf0-d2f2174fc042) - Closing
[0m13:06:39.189188 [debug] [Thread-3 (]: Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_code` cannot be resolved. Did you mean one of the following? [`iata_code`, `airport_id`, `city`, `country`, `state`]. SQLSTATE: 42703; line 17 pos 22
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql
[0m13:06:39.433310 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119eff0b0>]}
[0m13:06:39.434323 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a143d70>]}
[0m13:06:39.435579 [error] [Thread-3 (]: 7 of 10 ERROR creating sql table model flights_dbt_silver.silver_airlines ...... [[31mERROR[0m in 1.70s]
[0m13:06:39.436952 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airlines
[0m13:06:39.436383 [info ] [Thread-2 (]: 5 of 10 OK created sql table model flights_dbt.my_first_dbt_model .............. [[32mOK[0m in 9.62s]
[0m13:06:39.437507 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_airlines' to be skipped because of status 'error'.  Reason: Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_code` cannot be resolved. Did you mean one of the following? [`iata_code`, `airport_id`, `city`, `country`, `state`]. SQLSTATE: 42703; line 17 pos 22
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql.
[0m13:06:39.438016 [debug] [Thread-2 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m13:06:39.438699 [debug] [Thread-4 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m13:06:39.439396 [debug] [Thread-1 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m13:06:39.439093 [info ] [Thread-4 (]: 9 of 10 SKIP relation flights_dbt_gold.gold_flight_metrics ..................... [[33mSKIP[0m]
[0m13:06:39.439875 [info ] [Thread-1 (]: 10 of 10 START sql view model flights_dbt.my_second_dbt_model .................. [RUN]
[0m13:06:39.440290 [debug] [Thread-4 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m13:06:39.440935 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m13:06:39.441374 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m13:06:39.441686 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m13:06:39.446703 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m13:06:39.447727 [debug] [Thread-1 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m13:06:39.450251 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m13:06:39.451563 [debug] [Thread-1 (]: Creating view `flight_db`.`flights_dbt`.`my_second_dbt_model`
[0m13:06:39.452337 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m13:06:39.452870 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m13:06:39.453188 [debug] [Thread-1 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`flights_dbt`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`flights_dbt`.`my_first_dbt_model`
where id = 1
  )

[0m13:06:39.453462 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:06:40.128717 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-3b87-157a-bc61-297ac170b3a0) - Created
[0m13:06:40.912563 [debug] [Thread-1 (]: SQL status: OK in 1.460 seconds
[0m13:06:40.914254 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0897f-3b87-157a-bc61-297ac170b3a0, command-id=01f0897f-3ba2-1e8a-b514-3af2c251cdb4) - Closing
[0m13:06:40.915237 [debug] [Thread-1 (]: Applying tags to relation None
[0m13:06:40.917905 [debug] [Thread-1 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m13:06:40.918284 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-3b87-157a-bc61-297ac170b3a0) - Closing
[0m13:06:41.120898 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8c7f3cd-462e-4772-932a-8b1e79b01a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a143830>]}
[0m13:06:41.122719 [info ] [Thread-1 (]: 10 of 10 OK created sql view model flights_dbt.my_second_dbt_model ............. [[32mOK[0m in 1.68s]
[0m13:06:41.123832 [debug] [Thread-1 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m13:06:41.126396 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:06:41.126774 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:06:41.127333 [info ] [MainThread]: 
[0m13:06:41.127651 [info ] [MainThread]: Finished running 8 table models, 2 view models in 0 hours 0 minutes and 33.90 seconds (33.90s).
[0m13:06:41.129267 [debug] [MainThread]: Command end result
[0m13:06:41.161217 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:06:41.162999 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:06:41.166770 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m13:06:41.166949 [info ] [MainThread]: 
[0m13:06:41.167142 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m13:06:41.167279 [info ] [MainThread]: 
[0m13:06:41.167467 [error] [MainThread]: [31mFailure in model bronze_airports (models/bronze/bronze_airports.sql)[0m
[0m13:06:41.167639 [error] [MainThread]:   Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 10 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m13:06:41.167761 [info ] [MainThread]: 
[0m13:06:41.167910 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airports.sql
[0m13:06:41.168046 [info ] [MainThread]: 
[0m13:06:41.168329 [error] [MainThread]: [31mFailure in model bronze_flights (models/bronze/bronze_flights.sql)[0m
[0m13:06:41.168559 [error] [MainThread]:   Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m13:06:41.168693 [info ] [MainThread]: 
[0m13:06:41.168833 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_flights.sql
[0m13:06:41.168943 [info ] [MainThread]: 
[0m13:06:41.169083 [error] [MainThread]: [31mFailure in model silver_airlines (models/silver/silver_airlines.sql)[0m
[0m13:06:41.169224 [error] [MainThread]:   Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_code` cannot be resolved. Did you mean one of the following? [`iata_code`, `airport_id`, `city`, `country`, `state`]. SQLSTATE: 42703; line 17 pos 22
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql
[0m13:06:41.169338 [info ] [MainThread]: 
[0m13:06:41.169470 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_airlines.sql
[0m13:06:41.169580 [info ] [MainThread]: 
[0m13:06:41.169839 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=3 SKIP=3 NO-OP=0 TOTAL=10
[0m13:06:41.172901 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 35.00982, "process_in_blocks": "0", "process_kernel_time": 0.300912, "process_mem_max_rss": "250642432", "process_out_blocks": "0", "process_user_time": 3.550642}
[0m13:06:41.173128 [debug] [MainThread]: Command `dbt run` failed at 13:06:41.173085 after 35.01 seconds
[0m13:06:41.173325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10846d1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e2d8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a133e90>]}
[0m13:06:41.173483 [debug] [MainThread]: Flushing usage events
[0m13:06:41.908638 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:08:49.904542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10622b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107243890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107243b10>]}


============================== 13:08:49.906856 | 7fb97264-328d-4a7f-8053-3793c312b9ce ==============================
[0m13:08:49.906856 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:08:49.907101 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'no_print': 'None', 'quiet': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'target_path': 'None', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'static_parser': 'True', 'empty': 'False', 'debug': 'False', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt run --target dev', 'log_cache_events': 'False', 'partial_parse': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False'}
[0m13:08:50.205037 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:08:50.205244 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:08:50.205341 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:08:50.639607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10656f360>]}
[0m13:08:50.658377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10844aad0>]}
[0m13:08:50.658595 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:08:50.707143 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m13:08:50.767117 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m13:08:50.767361 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m13:08:50.767491 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/silver/silver_airlines.sql
[0m13:08:50.859077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e11ea50>]}
[0m13:08:50.892001 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:08:50.892900 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:08:50.897816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e102300>]}
[0m13:08:50.897965 [info ] [MainThread]: Found 10 models, 4 data tests, 3 sources, 686 macros
[0m13:08:50.898065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e22dfd0>]}
[0m13:08:50.898804 [info ] [MainThread]: 
[0m13:08:50.898924 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:08:50.899008 [info ] [MainThread]: 
[0m13:08:50.899181 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:08:50.899274 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:08:50.901663 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:08:50.901794 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:08:50.901937 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:08:50.905250 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:08:50.905427 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:08:50.905572 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:08:50.905719 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:08:50.905837 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:08:50.905937 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:08:50.906717 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:08:50.906827 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:08:50.906918 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:50.907613 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:08:50.907725 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:08:50.908446 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:08:50.908747 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:08:50.908847 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:50.908958 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:08:50.909055 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:50.909202 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:51.817228 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8a05-109d-a7fe-76f6aab8fc46) - Created
[0m13:08:51.844698 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8a07-1606-94dc-4e21084d7366) - Created
[0m13:08:51.848920 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8a09-118f-83ca-099c810ea060) - Created
[0m13:08:51.852870 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8a09-15f7-9524-9f444ac8c910) - Created
[0m13:08:52.356624 [debug] [ThreadPool]: SQL status: OK in 1.450 seconds
[0m13:08:52.357166 [debug] [ThreadPool]: SQL status: OK in 1.450 seconds
[0m13:08:52.357421 [debug] [ThreadPool]: SQL status: OK in 1.450 seconds
[0m13:08:52.357638 [debug] [ThreadPool]: SQL status: OK in 1.450 seconds
[0m13:08:52.366636 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-8a05-109d-a7fe-76f6aab8fc46, command-id=01f0897f-8a21-11a4-af91-b1bd7c6d4d2d) - Closing
[0m13:08:52.367701 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-8a09-118f-83ca-099c810ea060, command-id=01f0897f-8a2a-162a-a1eb-52352ca9fd53) - Closing
[0m13:08:52.368543 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-8a07-1606-94dc-4e21084d7366, command-id=01f0897f-8a2a-1460-bf19-d46713eedbdc) - Closing
[0m13:08:52.368930 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:08:52.369623 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-8a09-15f7-9524-9f444ac8c910, command-id=01f0897f-8a2a-1666-ae31-0b19811d5fd2) - Closing
[0m13:08:52.370030 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8a05-109d-a7fe-76f6aab8fc46) - Closing
[0m13:08:52.565371 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:08:52.566182 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8a09-118f-83ca-099c810ea060) - Closing
[0m13:08:52.772008 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:08:52.773030 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8a07-1606-94dc-4e21084d7366) - Closing
[0m13:08:52.974764 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:08:52.975507 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8a09-15f7-9524-9f444ac8c910) - Closing
[0m13:08:53.188763 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m13:08:53.189402 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m13:08:53.189929 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m13:08:53.190449 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m13:08:53.196272 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m13:08:53.199810 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m13:08:53.200091 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m13:08:53.200337 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m13:08:53.200528 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m13:08:53.200777 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m13:08:53.204830 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m13:08:53.207315 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m13:08:53.210987 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m13:08:53.211237 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:53.211475 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m13:08:53.211705 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m13:08:53.211939 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m13:08:53.212287 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:53.212454 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:53.212667 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:54.047034 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8b59-1888-8e40-0aa9036df6ec) - Created
[0m13:08:54.085002 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8b5c-1fbf-a073-ac5b2a4f48bb) - Created
[0m13:08:54.145399 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8b66-1ab1-a7ee-89131ead941d) - Created
[0m13:08:54.147717 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8b66-1f70-b0de-10bb2835c5b6) - Created
[0m13:08:54.980599 [debug] [ThreadPool]: SQL status: OK in 1.770 seconds
[0m13:08:54.984672 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-8b5c-1fbf-a073-ac5b2a4f48bb, command-id=01f0897f-8b81-1c34-bbe7-5da5be1a0751) - Closing
[0m13:08:54.985305 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m13:08:54.985568 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8b5c-1fbf-a073-ac5b2a4f48bb) - Closing
[0m13:08:55.001954 [debug] [ThreadPool]: SQL status: OK in 1.790 seconds
[0m13:08:55.004181 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-8b66-1f70-b0de-10bb2835c5b6, command-id=01f0897f-8b86-1e74-bc94-322a951b9772) - Closing
[0m13:08:55.012537 [debug] [ThreadPool]: SQL status: OK in 1.800 seconds
[0m13:08:55.014710 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-8b59-1888-8e40-0aa9036df6ec, command-id=01f0897f-8b7e-170a-89ae-c1c179db10b3) - Closing
[0m13:08:55.043260 [debug] [ThreadPool]: SQL status: OK in 1.830 seconds
[0m13:08:55.045573 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-8b66-1ab1-a7ee-89131ead941d, command-id=01f0897f-8b85-1b4f-8fa3-753f1043a32d) - Closing
[0m13:08:55.190200 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m13:08:55.191149 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8b66-1f70-b0de-10bb2835c5b6) - Closing
[0m13:08:55.405148 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m13:08:55.406145 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8b59-1888-8e40-0aa9036df6ec) - Closing
[0m13:08:55.613388 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m13:08:55.615126 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-8b66-1ab1-a7ee-89131ead941d) - Closing
[0m13:08:55.820750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e52fee0>]}
[0m13:08:55.826011 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m13:08:55.826473 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m13:08:55.826804 [debug] [Thread-4 (]: Began running node model.flights_dbt.bronze_weather
[0m13:08:55.827119 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m13:08:55.827744 [info ] [Thread-1 (]: 1 of 10 START sql table model flights_dbt_bronze.bronze_airlines ............... [RUN]
[0m13:08:55.828200 [info ] [Thread-3 (]: 3 of 10 START sql table model flights_dbt_bronze.bronze_flights ................ [RUN]
[0m13:08:55.828640 [info ] [Thread-4 (]: 4 of 10 START sql table model flights_dbt_bronze.bronze_weather ................ [RUN]
[0m13:08:55.829057 [info ] [Thread-2 (]: 2 of 10 START sql view model flights_dbt_bronze.bronze_airports ................ [RUN]
[0m13:08:55.829732 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m13:08:55.830217 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m13:08:55.830671 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_weather) - Creating connection
[0m13:08:55.831065 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m13:08:55.831359 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m13:08:55.831605 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m13:08:55.831834 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_weather'
[0m13:08:55.832061 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m13:08:55.832331 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m13:08:55.832568 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m13:08:55.832797 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.bronze_weather
[0m13:08:55.833023 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m13:08:55.840949 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m13:08:55.844182 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.bronze_weather"
[0m13:08:55.846699 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m13:08:55.854183 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m13:08:55.854827 [debug] [Thread-4 (]: Began executing node model.flights_dbt.bronze_weather
[0m13:08:55.865096 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m13:08:55.865540 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m13:08:55.865372 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:08:55.866540 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m13:08:55.866687 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m13:08:55.866816 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m13:08:55.866981 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e58ec90>]}
[0m13:08:55.867188 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:08:55.874589 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m13:08:55.875510 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m13:08:55.881983 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e58bd80>]}
[0m13:08:55.893944 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.bronze_weather"
[0m13:08:55.898289 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
[0m13:08:55.898876 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m13:08:55.899464 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m13:08:55.899728 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m13:08:55.899961 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.bronze_weather"
[0m13:08:55.900139 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_weather"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_weather`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`weather`
  
[0m13:08:55.900293 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:08:55.900390 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m13:08:55.900474 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m13:08:55.900627 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m13:08:55.900777 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    flight_id,
    airline,
    origin,
    dest,
    arr_time,
    dep_time,
    arr_delay,
    dep_delay
from `flight_db`.`raw`.`flights`
  
[0m13:08:55.900948 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`airports`
  
[0m13:08:55.901070 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select flight_id, airline, origin, arr_time, arr_delay
from `flight_db`.`raw`.`flights`
  )

[0m13:08:55.901170 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:08:55.901262 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:08:55.901354 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:08:56.621675 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-8ce2-19db-a63b-5e4bf32acee6) - Created
[0m13:08:56.638443 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-8ce3-1e69-8fb7-dd64b6201301) - Created
[0m13:08:56.656679 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897f-8ce5-1ead-885b-eff3f3f4a558) - Created
[0m13:08:56.659505 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-8ce8-17ff-8e20-d705674c86f6) - Created
[0m13:08:57.565821 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    flight_id,
    airline,
    origin,
    dest,
    arr_time,
    dep_time,
    arr_delay,
    dep_delay
from `flight_db`.`raw`.`flights`
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897f-8d04-1635-949c-679a28b26363
[0m13:08:57.567862 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m13:08:57.568531 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-8ce8-17ff-8e20-d705674c86f6) - Closing
[0m13:08:57.745674 [debug] [Thread-2 (]: SQL status: OK in 1.840 seconds
[0m13:08:57.746997 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0897f-8ce3-1e69-8fb7-dd64b6201301, command-id=01f0897f-8d02-1338-82af-f2d71f15cc90) - Closing
[0m13:08:57.759016 [debug] [Thread-2 (]: Applying tags to relation None
[0m13:08:57.764826 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m13:08:57.765210 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-8ce3-1e69-8fb7-dd64b6201301) - Closing
[0m13:08:57.774591 [debug] [Thread-3 (]: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m13:08:57.958496 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e739d10>]}
[0m13:08:57.959062 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e801f40>]}
[0m13:08:57.959985 [info ] [Thread-2 (]: 2 of 10 OK created sql view model flights_dbt_bronze.bronze_airports ........... [[32mOK[0m in 2.12s]
[0m13:08:57.961392 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m13:08:57.960827 [error] [Thread-3 (]: 3 of 10 ERROR creating sql table model flights_dbt_bronze.bronze_flights ....... [[31mERROR[0m in 2.12s]
[0m13:08:57.961972 [debug] [Thread-2 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m13:08:57.962626 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m13:08:57.963197 [info ] [Thread-2 (]: 5 of 10 START sql table model flights_dbt.my_first_dbt_model ................... [RUN]
[0m13:08:57.963638 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airports
[0m13:08:57.964126 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_flights' to be skipped because of status 'error'.  Reason: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql.
[0m13:08:57.964836 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m13:08:57.965281 [info ] [Thread-3 (]: 6 of 10 START sql table model flights_dbt_silver.silver_airports ............... [RUN]
[0m13:08:57.966504 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m13:08:57.966955 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m13:08:57.967245 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m13:08:57.967510 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m13:08:57.971400 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m13:08:57.971817 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_airports
[0m13:08:57.975524 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m13:08:57.975917 [debug] [Thread-2 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m13:08:57.977529 [debug] [Thread-2 (]: MATERIALIZING TABLE
[0m13:08:57.978966 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m13:08:57.979292 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_airports
[0m13:08:57.980963 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m13:08:57.982150 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m13:08:57.982434 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m13:08:57.982781 [debug] [Thread-2 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`flights_dbt`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:08:57.983028 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:08:57.983203 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m13:08:57.983574 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
    
        create or replace table `flight_db`.`flights_dbt_silver`.`silver_airports`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select distinct iata, airport_name, city, state
from `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
[0m13:08:57.983866 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:08:58.678114 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-8e1c-1944-8e45-9fc3a62f1cc7) - Created
[0m13:08:58.688627 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-8e1d-14b6-a423-6d7047f61577) - Created
[0m13:08:59.509503 [debug] [Thread-1 (]: SQL status: OK in 3.610 seconds
[0m13:08:59.511556 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0897f-8ce2-19db-a63b-5e4bf32acee6, command-id=01f0897f-8cfd-1c85-8778-62e40466230a) - Closing
[0m13:08:59.515937 [debug] [Thread-1 (]: Applying tags to relation None
[0m13:08:59.531243 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m13:08:59.531581 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-8ce2-19db-a63b-5e4bf32acee6) - Closing
[0m13:08:59.706078 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
    
        create or replace table `flight_db`.`flights_dbt_silver`.`silver_airports`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select distinct iata, airport_name, city, state
from `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`origin`, `airline`, `arr_time`, `arr_delay`, `flight_id`]. SQLSTATE: 42703; line 17 pos 22
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`origin`, `airline`, `arr_time`, `arr_delay`, `flight_id`]. SQLSTATE: 42703; line 17 pos 22
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`origin`, `airline`, `arr_time`, `arr_delay`, `flight_id`]. SQLSTATE: 42703; line 17 pos 22
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897f-8e3a-1199-aeb8-1128f488deec
[0m13:08:59.726554 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: Close
[0m13:08:59.727413 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-8e1d-14b6-a423-6d7047f61577) - Closing
[0m13:08:59.815995 [debug] [Thread-4 (]: SQL status: OK in 3.920 seconds
[0m13:08:59.817535 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0897f-8ce5-1ead-885b-eff3f3f4a558, command-id=01f0897f-8d02-1c27-8272-fc83b7db00ef) - Closing
[0m13:08:59.818712 [debug] [Thread-4 (]: Applying tags to relation None
[0m13:09:00.045305 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ea4f620>]}
[0m13:09:00.046091 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: Close
[0m13:09:00.047487 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897f-8ce5-1ead-885b-eff3f3f4a558) - Closing
[0m13:09:00.047057 [info ] [Thread-1 (]: 1 of 10 OK created sql table model flights_dbt_bronze.bronze_airlines .......... [[32mOK[0m in 4.22s]
[0m13:09:00.048738 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m13:09:00.049991 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_airlines
[0m13:09:00.052373 [info ] [Thread-1 (]: 7 of 10 START sql table model flights_dbt_silver.silver_airlines ............... [RUN]
[0m13:09:00.055632 [debug] [Thread-3 (]: Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`origin`, `airline`, `arr_time`, `arr_delay`, `flight_id`]. SQLSTATE: 42703; line 17 pos 22
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql
[0m13:09:00.243007 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airlines) - Creating connection
[0m13:09:00.244195 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3db410>]}
[0m13:09:00.244798 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ea05d30>]}
[0m13:09:00.245208 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airlines'
[0m13:09:00.246202 [error] [Thread-3 (]: 6 of 10 ERROR creating sql table model flights_dbt_silver.silver_airports ...... [[31mERROR[0m in 2.28s]
[0m13:09:00.247217 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_airlines
[0m13:09:00.246788 [info ] [Thread-4 (]: 4 of 10 OK created sql table model flights_dbt_bronze.bronze_weather ........... [[32mOK[0m in 4.41s]
[0m13:09:00.247794 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airports
[0m13:09:00.253137 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_airlines"
[0m13:09:00.253733 [debug] [Thread-4 (]: Finished running node model.flights_dbt.bronze_weather
[0m13:09:00.254117 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_flights
[0m13:09:00.254527 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_airports' to be skipped because of status 'error'.  Reason: Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`origin`, `airline`, `arr_time`, `arr_delay`, `flight_id`]. SQLSTATE: 42703; line 17 pos 22
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql.
[0m13:09:00.255002 [info ] [Thread-3 (]: 8 of 10 SKIP relation flights_dbt_silver.silver_flights ........................ [[33mSKIP[0m]
[0m13:09:00.255596 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_flights
[0m13:09:00.256225 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_airlines
[0m13:09:00.258518 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m13:09:00.260231 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.silver_airlines"
[0m13:09:00.260931 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.silver_airlines"
[0m13:09:00.261314 [debug] [Thread-1 (]: On model.flights_dbt.silver_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_silver`.`silver_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select distinct iata_code,_name airline_name
from `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
[0m13:09:00.261621 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:09:00.966273 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-8f79-1e6e-b9ed-914c93f3cf58) - Created
[0m13:09:01.790791 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_silver`.`silver_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select distinct iata_code,_name airline_name
from `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `_name` cannot be resolved. Did you mean one of the following? [`state`, `city`, `country`, `iata_code`, `airport_id`]. SQLSTATE: 42703; line 17 pos 32
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `_name` cannot be resolved. Did you mean one of the following? [`state`, `city`, `country`, `iata_code`, `airport_id`]. SQLSTATE: 42703; line 17 pos 32
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `_name` cannot be resolved. Did you mean one of the following? [`state`, `city`, `country`, `iata_code`, `airport_id`]. SQLSTATE: 42703; line 17 pos 32
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897f-8f93-1a1e-89b7-2bacb3ce0c01
[0m13:09:01.792445 [debug] [Thread-1 (]: On model.flights_dbt.silver_airlines: Close
[0m13:09:01.792872 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-8f79-1e6e-b9ed-914c93f3cf58) - Closing
[0m13:09:01.825229 [debug] [Thread-2 (]: SQL status: OK in 3.840 seconds
[0m13:09:01.826744 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0897f-8e1c-1944-8e45-9fc3a62f1cc7, command-id=01f0897f-8e37-15ac-8937-d8f62fe0c196) - Closing
[0m13:09:01.827868 [debug] [Thread-2 (]: Applying tags to relation None
[0m13:09:02.020016 [debug] [Thread-2 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m13:09:02.020925 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-8e1c-1944-8e45-9fc3a62f1cc7) - Closing
[0m13:09:02.026611 [debug] [Thread-1 (]: Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `_name` cannot be resolved. Did you mean one of the following? [`state`, `city`, `country`, `iata_code`, `airport_id`]. SQLSTATE: 42703; line 17 pos 32
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql
[0m13:09:02.209337 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e53b890>]}
[0m13:09:02.210165 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e20ae10>]}
[0m13:09:02.211334 [error] [Thread-1 (]: 7 of 10 ERROR creating sql table model flights_dbt_silver.silver_airlines ...... [[31mERROR[0m in 2.15s]
[0m13:09:02.211936 [info ] [Thread-2 (]: 5 of 10 OK created sql table model flights_dbt.my_first_dbt_model .............. [[32mOK[0m in 4.25s]
[0m13:09:02.212674 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_airlines
[0m13:09:02.213202 [debug] [Thread-2 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m13:09:02.213690 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_airlines' to be skipped because of status 'error'.  Reason: Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `_name` cannot be resolved. Did you mean one of the following? [`state`, `city`, `country`, `iata_code`, `airport_id`]. SQLSTATE: 42703; line 17 pos 32
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql.
[0m13:09:02.214701 [debug] [Thread-3 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m13:09:02.215092 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m13:09:02.215423 [info ] [Thread-3 (]: 10 of 10 SKIP relation flights_dbt_gold.gold_flight_metrics .................... [[33mSKIP[0m]
[0m13:09:02.215961 [info ] [Thread-4 (]: 9 of 10 START sql view model flights_dbt.my_second_dbt_model ................... [RUN]
[0m13:09:02.216338 [debug] [Thread-3 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m13:09:02.216931 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m13:09:02.217357 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m13:09:02.217675 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m13:09:02.221889 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m13:09:02.222806 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m13:09:02.224826 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m13:09:02.242785 [debug] [Thread-4 (]: Creating view `flight_db`.`flights_dbt`.`my_second_dbt_model`
[0m13:09:02.243546 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m13:09:02.244134 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m13:09:02.244355 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`flights_dbt`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`flights_dbt`.`my_first_dbt_model`
where id = 1
  )

[0m13:09:02.244550 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:09:02.925033 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897f-90a4-1016-9f5c-201301ec91c1) - Created
[0m13:09:03.617284 [debug] [Thread-4 (]: SQL status: OK in 1.370 seconds
[0m13:09:03.618918 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0897f-90a4-1016-9f5c-201301ec91c1, command-id=01f0897f-90be-1bcc-a3c8-0bb7dea0da1e) - Closing
[0m13:09:03.619732 [debug] [Thread-4 (]: Applying tags to relation None
[0m13:09:03.620745 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m13:09:03.621036 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897f-90a4-1016-9f5c-201301ec91c1) - Closing
[0m13:09:03.818477 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb97264-328d-4a7f-8053-3793c312b9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7c4950>]}
[0m13:09:03.820067 [info ] [Thread-4 (]: 9 of 10 OK created sql view model flights_dbt.my_second_dbt_model .............. [[32mOK[0m in 1.60s]
[0m13:09:03.820789 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m13:09:03.822690 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:09:03.823072 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:09:03.823650 [info ] [MainThread]: 
[0m13:09:03.823984 [info ] [MainThread]: Finished running 8 table models, 2 view models in 0 hours 0 minutes and 12.92 seconds (12.92s).
[0m13:09:03.825666 [debug] [MainThread]: Command end result
[0m13:09:03.856572 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:09:03.858000 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:09:03.861965 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m13:09:03.862129 [info ] [MainThread]: 
[0m13:09:03.862328 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m13:09:03.862475 [info ] [MainThread]: 
[0m13:09:03.862669 [error] [MainThread]: [31mFailure in model bronze_flights (models/bronze/bronze_flights.sql)[0m
[0m13:09:03.862847 [error] [MainThread]:   Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m13:09:03.862972 [info ] [MainThread]: 
[0m13:09:03.863113 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_flights.sql
[0m13:09:03.863224 [info ] [MainThread]: 
[0m13:09:03.863362 [error] [MainThread]: [31mFailure in model silver_airports (models/silver/silver_airports.sql)[0m
[0m13:09:03.863507 [error] [MainThread]:   Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `iata` cannot be resolved. Did you mean one of the following? [`origin`, `airline`, `arr_time`, `arr_delay`, `flight_id`]. SQLSTATE: 42703; line 17 pos 22
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql
[0m13:09:03.863619 [info ] [MainThread]: 
[0m13:09:03.863745 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_airports.sql
[0m13:09:03.863851 [info ] [MainThread]: 
[0m13:09:03.863984 [error] [MainThread]: [31mFailure in model silver_airlines (models/silver/silver_airlines.sql)[0m
[0m13:09:03.864123 [error] [MainThread]:   Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `_name` cannot be resolved. Did you mean one of the following? [`state`, `city`, `country`, `iata_code`, `airport_id`]. SQLSTATE: 42703; line 17 pos 32
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql
[0m13:09:03.864228 [info ] [MainThread]: 
[0m13:09:03.864358 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_airlines.sql
[0m13:09:03.864463 [info ] [MainThread]: 
[0m13:09:03.864593 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=3 SKIP=2 NO-OP=0 TOTAL=10
[0m13:09:03.867489 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 13.992624, "process_in_blocks": "0", "process_kernel_time": 0.287273, "process_mem_max_rss": "257802240", "process_out_blocks": "0", "process_user_time": 3.514992}
[0m13:09:03.867755 [debug] [MainThread]: Command `dbt run` failed at 13:09:03.867707 after 13.99 seconds
[0m13:09:03.867957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051b5610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107084710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ea07e30>]}
[0m13:09:03.868118 [debug] [MainThread]: Flushing usage events
[0m13:09:04.620154 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:11:44.980706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cc3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e9b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e9bb10>]}


============================== 13:11:44.982922 | 7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8 ==============================
[0m13:11:44.982922 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:11:44.983140 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'printer_width': '80', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'fail_fast': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --target dev', 'debug': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'quiet': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'use_experimental_parser': 'False', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'write_json': 'True', 'warn_error': 'None', 'no_print': 'None', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'version_check': 'True'}
[0m13:11:45.278366 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:11:45.278568 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:11:45.278662 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:11:45.710202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061c7360>]}
[0m13:11:45.728784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080a6ad0>]}
[0m13:11:45.729015 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:11:45.777513 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m13:11:45.833923 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m13:11:45.834160 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/silver/silver_airports.sql
[0m13:11:45.834286 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/silver/silver_airlines.sql
[0m13:11:45.924199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc26a50>]}
[0m13:11:45.956625 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:11:45.958317 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:11:45.963213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc06300>]}
[0m13:11:45.963363 [info ] [MainThread]: Found 10 models, 4 data tests, 3 sources, 686 macros
[0m13:11:45.963468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd320b0>]}
[0m13:11:45.964209 [info ] [MainThread]: 
[0m13:11:45.964327 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:11:45.964409 [info ] [MainThread]: 
[0m13:11:45.964586 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:11:45.964680 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:11:45.967112 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:11:45.967249 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:11:45.967397 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:11:45.970737 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:11:45.970915 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:11:45.971061 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:11:45.971219 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:11:45.971344 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:11:45.971436 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:11:45.972235 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:11:45.972340 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:11:45.972429 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:11:45.973056 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:11:45.973152 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:11:45.973751 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:11:45.974295 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:11:45.974375 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:11:45.974476 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:11:45.974563 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:11:45.974700 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:11:46.804657 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f251-19ac-a92c-daf8270126f6) - Created
[0m13:11:46.807555 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f251-1a19-aa16-6f3985582da1) - Created
[0m13:11:46.840478 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f258-136d-a24a-0d9db87e4db3) - Created
[0m13:11:46.842851 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f258-107e-b645-94e48331e6f2) - Created
[0m13:11:47.603273 [debug] [ThreadPool]: SQL status: OK in 1.630 seconds
[0m13:11:47.613408 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-f258-107e-b645-94e48331e6f2, command-id=01f0897f-f274-1215-a062-7c0d0195e7f4) - Closing
[0m13:11:47.614052 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:11:47.614298 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f258-107e-b645-94e48331e6f2) - Closing
[0m13:11:47.721123 [debug] [ThreadPool]: SQL status: OK in 1.750 seconds
[0m13:11:47.721459 [debug] [ThreadPool]: SQL status: OK in 1.750 seconds
[0m13:11:47.722094 [debug] [ThreadPool]: SQL status: OK in 1.750 seconds
[0m13:11:47.723561 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-f251-1a19-aa16-6f3985582da1, command-id=01f0897f-f26c-1fd8-bf0f-3ea3ba417148) - Closing
[0m13:11:47.724524 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-f258-136d-a24a-0d9db87e4db3, command-id=01f0897f-f279-11d1-8d00-e98e4f910116) - Closing
[0m13:11:47.725797 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-f251-19ac-a92c-daf8270126f6, command-id=01f0897f-f274-1229-b2a9-58b5b5c1b63f) - Closing
[0m13:11:47.811211 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:11:47.812071 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f251-1a19-aa16-6f3985582da1) - Closing
[0m13:11:48.405587 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:11:48.406675 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f258-136d-a24a-0d9db87e4db3) - Closing
[0m13:11:48.616374 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:11:48.617334 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f251-19ac-a92c-daf8270126f6) - Closing
[0m13:11:48.824480 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m13:11:48.825209 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m13:11:48.825467 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m13:11:48.825888 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m13:11:48.826289 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m13:11:48.826503 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m13:11:48.835028 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m13:11:48.835328 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m13:11:48.835534 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m13:11:48.837592 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m13:11:48.840150 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m13:11:48.843446 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m13:11:48.844916 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m13:11:48.845108 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:11:48.845326 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m13:11:48.845555 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m13:11:48.845773 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m13:11:48.846090 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:11:48.846229 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:11:48.846444 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:11:49.574134 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f3f9-1848-a87b-cd400012e6eb) - Created
[0m13:11:49.606774 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f3fc-1fb6-9fc8-d94b53946543) - Created
[0m13:11:49.631332 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f401-1f5c-acb6-8254edd90bbd) - Created
[0m13:11:49.640523 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f402-1614-8dd1-ba3a9a3b7bf7) - Created
[0m13:11:50.215570 [debug] [ThreadPool]: SQL status: OK in 1.370 seconds
[0m13:11:50.218764 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-f3fc-1fb6-9fc8-d94b53946543, command-id=01f0897f-f417-1d1b-9036-1981d837dcc4) - Closing
[0m13:11:50.219419 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m13:11:50.219722 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f3fc-1fb6-9fc8-d94b53946543) - Closing
[0m13:11:50.244266 [debug] [ThreadPool]: SQL status: OK in 1.400 seconds
[0m13:11:50.246715 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-f3f9-1848-a87b-cd400012e6eb, command-id=01f0897f-f412-1dfa-8ad4-08c85c2d027c) - Closing
[0m13:11:50.314179 [debug] [ThreadPool]: SQL status: OK in 1.470 seconds
[0m13:11:50.316961 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-f402-1614-8dd1-ba3a9a3b7bf7, command-id=01f0897f-f41e-166b-92d7-74f7868d31c4) - Closing
[0m13:11:50.353696 [debug] [ThreadPool]: SQL status: OK in 1.510 seconds
[0m13:11:50.355629 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0897f-f401-1f5c-acb6-8254edd90bbd, command-id=01f0897f-f41c-1c4f-983c-0607922197c9) - Closing
[0m13:11:50.411768 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m13:11:50.412537 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f3f9-1848-a87b-cd400012e6eb) - Closing
[0m13:11:50.589330 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m13:11:50.590284 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f402-1614-8dd1-ba3a9a3b7bf7) - Closing
[0m13:11:50.781030 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m13:11:50.783085 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0897f-f401-1f5c-acb6-8254edd90bbd) - Closing
[0m13:11:50.977016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e12d160>]}
[0m13:11:50.982340 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m13:11:50.982769 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m13:11:50.983058 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m13:11:50.983326 [debug] [Thread-4 (]: Began running node model.flights_dbt.bronze_weather
[0m13:11:50.983882 [info ] [Thread-3 (]: 3 of 10 START sql table model flights_dbt_bronze.bronze_flights ................ [RUN]
[0m13:11:50.984309 [info ] [Thread-2 (]: 2 of 10 START sql view model flights_dbt_bronze.bronze_airports ................ [RUN]
[0m13:11:50.984755 [info ] [Thread-1 (]: 1 of 10 START sql table model flights_dbt_bronze.bronze_airlines ............... [RUN]
[0m13:11:50.985163 [info ] [Thread-4 (]: 4 of 10 START sql table model flights_dbt_bronze.bronze_weather ................ [RUN]
[0m13:11:50.985782 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m13:11:50.986161 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m13:11:50.986499 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m13:11:50.986798 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_weather) - Creating connection
[0m13:11:50.987058 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m13:11:50.987275 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m13:11:50.987480 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m13:11:50.987687 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_weather'
[0m13:11:50.987910 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m13:11:50.988127 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m13:11:50.988488 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m13:11:50.988792 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.bronze_weather
[0m13:11:50.996771 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m13:11:50.999607 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m13:11:51.002178 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.bronze_weather"
[0m13:11:51.009213 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m13:11:51.009727 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m13:11:51.009944 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m13:11:51.019969 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m13:11:51.020134 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m13:11:51.021063 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m13:11:51.021209 [debug] [Thread-4 (]: Began executing node model.flights_dbt.bronze_weather
[0m13:11:51.021456 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:11:51.028426 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m13:11:51.028615 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:11:51.029457 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m13:11:51.029614 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e096ed0>]}
[0m13:11:51.030446 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:11:51.030600 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e08bc20>]}
[0m13:11:51.030750 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:11:51.036879 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e620f50>]}
[0m13:11:51.049551 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m13:11:51.049753 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e05be30>]}
[0m13:11:51.050333 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m13:11:51.054202 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
[0m13:11:51.054817 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.bronze_weather"
[0m13:11:51.055171 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m13:11:51.055432 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m13:11:51.055572 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    flight_id,
    airline,
    origin,
    dest,
    arr_time,
    dep_time,
    arr_delay,
    dep_delay
from `flight_db`.`raw`.`flights`
  
[0m13:11:51.055676 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.bronze_weather"
[0m13:11:51.055764 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m13:11:51.055861 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:11:51.055989 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_weather"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_weather`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`weather`
  
[0m13:11:51.056140 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`airports`
  
[0m13:11:51.056240 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m13:11:51.056406 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:11:51.056499 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:11:51.056642 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select flight_id, airline, origin, arr_time, arr_delay
from `flight_db`.`raw`.`flights`
  )

[0m13:11:51.056854 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:11:51.817075 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-f54c-17b7-9bc5-c344ba4c3b45) - Created
[0m13:11:51.820228 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-f54e-1ef6-8316-783eeb15cbaa) - Created
[0m13:11:51.831390 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-f550-1db6-be7e-f68f6722bff5) - Created
[0m13:11:51.845305 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897f-f551-145d-8859-6168f86b2f59) - Created
[0m13:11:52.290768 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    flight_id,
    airline,
    origin,
    dest,
    arr_time,
    dep_time,
    arr_delay,
    dep_delay
from `flight_db`.`raw`.`flights`
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897f-f576-1609-985f-53b2e0cde256
[0m13:11:52.292159 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m13:11:52.292639 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-f550-1db6-be7e-f68f6722bff5) - Closing
[0m13:11:52.513312 [debug] [Thread-3 (]: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m13:11:52.516128 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e6d4440>]}
[0m13:11:52.517039 [error] [Thread-3 (]: 3 of 10 ERROR creating sql table model flights_dbt_bronze.bronze_flights ....... [[31mERROR[0m in 1.53s]
[0m13:11:52.517579 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m13:11:52.517927 [debug] [Thread-3 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m13:11:52.518325 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_flights' to be skipped because of status 'error'.  Reason: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql.
[0m13:11:52.518754 [info ] [Thread-3 (]: 5 of 10 START sql table model flights_dbt.my_first_dbt_model ................... [RUN]
[0m13:11:52.520160 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m13:11:52.520553 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m13:11:52.520853 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m13:11:52.523544 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m13:11:52.524271 [debug] [Thread-3 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m13:11:52.525856 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m13:11:52.527707 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m13:11:52.528083 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m13:11:52.528337 [debug] [Thread-3 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`flights_dbt`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:11:52.528552 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:11:52.674668 [debug] [Thread-2 (]: SQL status: OK in 1.620 seconds
[0m13:11:52.677191 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0897f-f54e-1ef6-8316-783eeb15cbaa, command-id=01f0897f-f574-1c34-a0db-6ca6e559f6f5) - Closing
[0m13:11:52.693840 [debug] [Thread-2 (]: Applying tags to relation None
[0m13:11:52.696768 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m13:11:52.697062 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-f54e-1ef6-8316-783eeb15cbaa) - Closing
[0m13:11:52.903977 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da38bf0>]}
[0m13:11:52.905582 [info ] [Thread-2 (]: 2 of 10 OK created sql view model flights_dbt_bronze.bronze_airports ........... [[32mOK[0m in 1.92s]
[0m13:11:52.906431 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m13:11:52.907227 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airports
[0m13:11:52.907829 [info ] [Thread-2 (]: 6 of 10 START sql view model flights_dbt_silver.silver_airports ................ [RUN]
[0m13:11:52.908496 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m13:11:52.908908 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m13:11:52.909258 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.silver_airports
[0m13:11:52.913550 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m13:11:52.914677 [debug] [Thread-2 (]: Began executing node model.flights_dbt.silver_airports
[0m13:11:52.917607 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m13:11:52.918665 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_silver`.`silver_airports`
[0m13:11:52.919385 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m13:11:52.920112 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m13:11:52.920424 [debug] [Thread-2 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  )

[0m13:11:52.920685 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:11:53.253128 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-f627-1ed2-8688-1be8fbc75523) - Created
[0m13:11:53.673586 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-f669-1a17-91ae-ce3556c2fd30) - Created
[0m13:11:54.102346 [debug] [Thread-4 (]: SQL status: OK in 3.050 seconds
[0m13:11:54.104313 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0897f-f551-145d-8859-6168f86b2f59, command-id=01f0897f-f577-11d1-a8f4-ecbe6a369ec6) - Closing
[0m13:11:54.109073 [debug] [Thread-4 (]: Applying tags to relation None
[0m13:11:54.125883 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: Close
[0m13:11:54.126221 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897f-f551-145d-8859-6168f86b2f59) - Closing
[0m13:11:54.143412 [debug] [Thread-1 (]: SQL status: OK in 3.090 seconds
[0m13:11:54.144253 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0897f-f54c-17b7-9bc5-c344ba4c3b45, command-id=01f0897f-f574-1bb0-9ccc-db2ff7355722) - Closing
[0m13:11:54.144948 [debug] [Thread-1 (]: Applying tags to relation None
[0m13:11:54.239492 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897f-f687-13d0-a078-4bebf56109ff
[0m13:11:54.328330 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m13:11:54.329496 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-f54c-17b7-9bc5-c344ba4c3b45) - Closing
[0m13:11:54.528588 [debug] [Thread-2 (]: On model.flights_dbt.silver_airports: Close
[0m13:11:54.529693 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0897f-f669-1a17-91ae-ce3556c2fd30) - Closing
[0m13:11:54.741211 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da38110>]}
[0m13:11:54.741965 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da38d70>]}
[0m13:11:54.742903 [info ] [Thread-4 (]: 4 of 10 OK created sql table model flights_dbt_bronze.bronze_weather ........... [[32mOK[0m in 3.75s]
[0m13:11:54.744122 [debug] [Thread-4 (]: Finished running node model.flights_dbt.bronze_weather
[0m13:11:54.743582 [info ] [Thread-1 (]: 1 of 10 OK created sql table model flights_dbt_bronze.bronze_airlines .......... [[32mOK[0m in 3.76s]
[0m13:11:54.744864 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m13:11:54.745953 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_flights
[0m13:11:54.746282 [debug] [Thread-4 (]: Began running node model.flights_dbt.silver_airlines
[0m13:11:54.746682 [info ] [Thread-1 (]: 8 of 10 SKIP relation flights_dbt_silver.silver_flights ........................ [[33mSKIP[0m]
[0m13:11:54.747290 [info ] [Thread-4 (]: 7 of 10 START sql view model flights_dbt_silver.silver_airlines ................ [RUN]
[0m13:11:54.750851 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_flights
[0m13:11:54.751784 [debug] [Thread-2 (]: Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql
[0m13:11:54.752373 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airlines) - Creating connection
[0m13:11:54.752863 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da38110>]}
[0m13:11:54.753146 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airlines'
[0m13:11:54.753862 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.silver_airlines
[0m13:11:54.753637 [error] [Thread-2 (]: 6 of 10 ERROR creating sql view model flights_dbt_silver.silver_airports ....... [[31mERROR[0m in 1.84s]
[0m13:11:54.757733 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.silver_airlines"
[0m13:11:54.758271 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airports
[0m13:11:54.758796 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_airports' to be skipped because of status 'error'.  Reason: Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql.
[0m13:11:54.759364 [debug] [Thread-4 (]: Began executing node model.flights_dbt.silver_airlines
[0m13:11:54.761334 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m13:11:54.762109 [debug] [Thread-4 (]: Creating view `flight_db`.`flights_dbt_silver`.`silver_airlines`
[0m13:11:54.762702 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.silver_airlines"
[0m13:11:54.763149 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.silver_airlines"
[0m13:11:54.763417 [debug] [Thread-4 (]: On model.flights_dbt.silver_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airlines`
  
  as (
    select
    airline_id,
    iata_code,
    name
from `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  )

[0m13:11:54.763650 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:11:55.467904 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897f-f77b-1924-8e11-d837141f8363) - Created
[0m13:11:55.619499 [debug] [Thread-3 (]: SQL status: OK in 3.090 seconds
[0m13:11:55.621245 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0897f-f627-1ed2-8688-1be8fbc75523, command-id=01f0897f-f646-11af-8b67-eedbae990226) - Closing
[0m13:11:55.622353 [debug] [Thread-3 (]: Applying tags to relation None
[0m13:11:55.623858 [debug] [Thread-3 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m13:11:55.624196 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0897f-f627-1ed2-8688-1be8fbc75523) - Closing
[0m13:11:55.824611 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airlines`
  
  as (
    select
    airline_id,
    iata_code,
    name
from `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airport_id`, `city`, `country`, `state`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airport_id`, `city`, `country`, `state`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airport_id`, `city`, `country`, `state`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f0897f-f796-19cc-a369-22e8684310d8
[0m13:11:55.830003 [debug] [Thread-4 (]: On model.flights_dbt.silver_airlines: Close
[0m13:11:55.830552 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0897f-f77b-1924-8e11-d837141f8363) - Closing
[0m13:11:56.024831 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e6bd5b0>]}
[0m13:11:56.026187 [info ] [Thread-3 (]: 5 of 10 OK created sql table model flights_dbt.my_first_dbt_model .............. [[32mOK[0m in 3.50s]
[0m13:11:56.026883 [debug] [Thread-3 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m13:11:56.027929 [debug] [Thread-1 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m13:11:56.028448 [info ] [Thread-1 (]: 9 of 10 START sql view model flights_dbt.my_second_dbt_model ................... [RUN]
[0m13:11:56.030568 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m13:11:56.032866 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m13:11:56.033847 [debug] [Thread-4 (]: Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airport_id`, `city`, `country`, `state`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql
[0m13:11:56.034234 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m13:11:56.034667 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e6bd670>]}
[0m13:11:56.039199 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m13:11:56.039810 [error] [Thread-4 (]: 7 of 10 ERROR creating sql view model flights_dbt_silver.silver_airlines ....... [[31mERROR[0m in 1.28s]
[0m13:11:56.040377 [debug] [Thread-4 (]: Finished running node model.flights_dbt.silver_airlines
[0m13:11:56.040958 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_airlines' to be skipped because of status 'error'.  Reason: Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airport_id`, `city`, `country`, `state`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql.
[0m13:11:56.041429 [debug] [Thread-1 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m13:11:56.043512 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m13:11:56.043869 [debug] [Thread-2 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m13:11:56.044796 [debug] [Thread-1 (]: Creating view `flight_db`.`flights_dbt`.`my_second_dbt_model`
[0m13:11:56.045100 [info ] [Thread-2 (]: 10 of 10 SKIP relation flights_dbt_gold.gold_flight_metrics .................... [[33mSKIP[0m]
[0m13:11:56.045782 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m13:11:56.046072 [debug] [Thread-2 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m13:11:56.046780 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m13:11:56.047073 [debug] [Thread-1 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`flights_dbt`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`flights_dbt`.`my_first_dbt_model`
where id = 1
  )

[0m13:11:56.047317 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:11:56.711660 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-f839-1ded-b7bf-ee64727d635d) - Created
[0m13:11:57.300944 [debug] [Thread-1 (]: SQL status: OK in 1.250 seconds
[0m13:11:57.302665 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0897f-f839-1ded-b7bf-ee64727d635d, command-id=01f0897f-f857-1eca-b7e5-fc50a1f0bf16) - Closing
[0m13:11:57.303612 [debug] [Thread-1 (]: Applying tags to relation None
[0m13:11:57.304646 [debug] [Thread-1 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m13:11:57.304972 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0897f-f839-1ded-b7bf-ee64727d635d) - Closing
[0m13:11:57.494253 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7fb7a00b-8a6e-4242-ac07-bfb85ebd0ed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da38110>]}
[0m13:11:57.496066 [info ] [Thread-1 (]: 9 of 10 OK created sql view model flights_dbt.my_second_dbt_model .............. [[32mOK[0m in 1.46s]
[0m13:11:57.496823 [debug] [Thread-1 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m13:11:57.498520 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:11:57.498853 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:11:57.499343 [info ] [MainThread]: 
[0m13:11:57.499637 [info ] [MainThread]: Finished running 6 table models, 4 view models in 0 hours 0 minutes and 11.53 seconds (11.53s).
[0m13:11:57.500947 [debug] [MainThread]: Command end result
[0m13:11:57.535492 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:11:57.536529 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:11:57.539780 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m13:11:57.539912 [info ] [MainThread]: 
[0m13:11:57.540047 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m13:11:57.540156 [info ] [MainThread]: 
[0m13:11:57.540289 [error] [MainThread]: [31mFailure in model bronze_flights (models/bronze/bronze_flights.sql)[0m
[0m13:11:57.540424 [error] [MainThread]:   Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 21 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m13:11:57.540521 [info ] [MainThread]: 
[0m13:11:57.540633 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_flights.sql
[0m13:11:57.540727 [info ] [MainThread]: 
[0m13:11:57.540841 [error] [MainThread]: [31mFailure in model silver_airports (models/silver/silver_airports.sql)[0m
[0m13:11:57.540963 [error] [MainThread]:   Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql
[0m13:11:57.541055 [info ] [MainThread]: 
[0m13:11:57.541165 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_airports.sql
[0m13:11:57.541250 [info ] [MainThread]: 
[0m13:11:57.541361 [error] [MainThread]: [31mFailure in model silver_airlines (models/silver/silver_airlines.sql)[0m
[0m13:11:57.541481 [error] [MainThread]:   Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`airport_id`, `city`, `country`, `state`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql
[0m13:11:57.541570 [info ] [MainThread]: 
[0m13:11:57.541677 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_airlines.sql
[0m13:11:57.541768 [info ] [MainThread]: 
[0m13:11:57.541873 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=3 SKIP=2 NO-OP=0 TOTAL=10
[0m13:11:57.544701 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 12.593555, "process_in_blocks": "0", "process_kernel_time": 0.287231, "process_mem_max_rss": "256131072", "process_out_blocks": "0", "process_user_time": 3.466359}
[0m13:11:57.544897 [debug] [MainThread]: Command `dbt run` failed at 13:11:57.544860 after 12.59 seconds
[0m13:11:57.545053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d82570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e6605f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc90350>]}
[0m13:11:57.545183 [debug] [MainThread]: Flushing usage events
[0m13:11:58.297599 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:15:31.048267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069e3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100fbb10>]}


============================== 13:15:31.050445 | d202d83a-3203-43a3-9120-4021ca4303d5 ==============================
[0m13:15:31.050445 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:15:31.050684 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'empty': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --target dev', 'target_path': 'None', 'static_parser': 'True', 'debug': 'False', 'warn_error': 'None', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'version_check': 'True', 'partial_parse': 'True', 'printer_width': '80', 'introspect': 'True', 'log_cache_events': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'use_colors': 'True', 'quiet': 'False', 'use_experimental_parser': 'False'}
[0m13:15:31.343029 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:15:31.343285 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:15:31.343410 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:15:31.789120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10712b360>]}
[0m13:15:31.807718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112006ad0>]}
[0m13:15:31.807953 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:15:31.856232 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m13:15:31.912699 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m13:15:31.912952 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/silver/silver_airlines.sql
[0m13:15:31.913083 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:15:32.004340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12120fd50>]}
[0m13:15:32.036567 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:15:32.037460 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:15:32.042436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120e0a6c0>]}
[0m13:15:32.042594 [info ] [MainThread]: Found 10 models, 4 data tests, 3 sources, 686 macros
[0m13:15:32.042694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12177dfd0>]}
[0m13:15:32.043449 [info ] [MainThread]: 
[0m13:15:32.043565 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:15:32.043648 [info ] [MainThread]: 
[0m13:15:32.043822 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:15:32.043912 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:15:32.046381 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:15:32.046543 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:15:32.046730 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:15:32.050184 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:15:32.050379 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:15:32.050554 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:15:32.050643 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:15:32.050757 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:15:32.050841 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:15:32.050914 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:15:32.051634 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:15:32.051720 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:15:32.052287 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:15:32.052851 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:15:32.052949 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:15:32.053479 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:15:32.053582 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:15:32.053664 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:15:32.053733 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:15:32.053809 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:15:33.094132 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-792e-1077-a662-4b3f37b3fcb5) - Created
[0m13:15:33.095649 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-792e-13cc-adb0-4e01195bf669) - Created
[0m13:15:33.102902 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7933-10e3-b723-e0a0165caf6d) - Created
[0m13:15:33.105189 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7933-1a66-a331-32cddacdfec6) - Created
[0m13:15:33.747538 [debug] [ThreadPool]: SQL status: OK in 1.690 seconds
[0m13:15:33.758486 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-792e-1077-a662-4b3f37b3fcb5, command-id=01f08980-794e-10f4-8874-073a5b4e4988) - Closing
[0m13:15:33.759114 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:15:33.759402 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-792e-1077-a662-4b3f37b3fcb5) - Closing
[0m13:15:33.843978 [debug] [ThreadPool]: SQL status: OK in 1.790 seconds
[0m13:15:33.844394 [debug] [ThreadPool]: SQL status: OK in 1.790 seconds
[0m13:15:33.844641 [debug] [ThreadPool]: SQL status: OK in 1.790 seconds
[0m13:15:33.847103 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-7933-10e3-b723-e0a0165caf6d, command-id=01f08980-7950-1350-b6f0-de6da21fe845) - Closing
[0m13:15:33.848415 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-7933-1a66-a331-32cddacdfec6, command-id=01f08980-7955-1068-a3a2-1a2cb4a24844) - Closing
[0m13:15:33.849589 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-792e-13cc-adb0-4e01195bf669, command-id=01f08980-794f-123f-97ba-a84ce11bed57) - Closing
[0m13:15:34.000606 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:15:34.001416 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7933-10e3-b723-e0a0165caf6d) - Closing
[0m13:15:34.217448 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:15:34.218270 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7933-1a66-a331-32cddacdfec6) - Closing
[0m13:15:34.426274 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:15:34.427314 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-792e-13cc-adb0-4e01195bf669) - Closing
[0m13:15:34.620335 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m13:15:34.621083 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m13:15:34.621395 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m13:15:34.621833 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m13:15:34.622273 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m13:15:34.622656 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m13:15:34.629771 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m13:15:34.630001 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m13:15:34.630200 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m13:15:34.632090 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m13:15:34.634439 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m13:15:34.637491 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m13:15:34.638953 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m13:15:34.639172 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:15:34.639389 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m13:15:34.639609 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m13:15:34.639811 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m13:15:34.640115 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:15:34.640260 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:15:34.640461 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:15:35.377153 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7a8f-1fa1-9901-ec1c9d15ff63) - Created
[0m13:15:35.387488 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7a90-10ac-bc3d-506773a9a73a) - Created
[0m13:15:35.397128 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7a93-16e9-8821-3269ac1c8e64) - Created
[0m13:15:35.418283 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7a94-18ea-8a61-536f3a656304) - Created
[0m13:15:36.008904 [debug] [ThreadPool]: SQL status: OK in 1.370 seconds
[0m13:15:36.009725 [debug] [ThreadPool]: SQL status: OK in 1.370 seconds
[0m13:15:36.012302 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-7a8f-1fa1-9901-ec1c9d15ff63, command-id=01f08980-7aaa-1245-a662-683a79c61a8a) - Closing
[0m13:15:36.012702 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m13:15:36.013658 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-7a90-10ac-bc3d-506773a9a73a, command-id=01f08980-7aab-1a2f-840f-80472f1f2c29) - Closing
[0m13:15:36.013870 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7a8f-1fa1-9901-ec1c9d15ff63) - Closing
[0m13:15:36.102060 [debug] [ThreadPool]: SQL status: OK in 1.460 seconds
[0m13:15:36.106645 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-7a93-16e9-8821-3269ac1c8e64, command-id=01f08980-7aae-17b9-99b2-7f67bf6c1de2) - Closing
[0m13:15:36.107243 [debug] [ThreadPool]: SQL status: OK in 1.470 seconds
[0m13:15:36.108902 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-7a94-18ea-8a61-536f3a656304, command-id=01f08980-7ab0-1e40-a44f-194b6fb6e006) - Closing
[0m13:15:36.202155 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m13:15:36.203124 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7a90-10ac-bc3d-506773a9a73a) - Closing
[0m13:15:36.399197 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m13:15:36.400523 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7a93-16e9-8821-3269ac1c8e64) - Closing
[0m13:15:36.602441 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m13:15:36.603440 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-7a94-18ea-8a61-536f3a656304) - Closing
[0m13:15:36.814720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121814ae0>]}
[0m13:15:36.820037 [debug] [Thread-4 (]: Began running node model.flights_dbt.bronze_weather
[0m13:15:36.820496 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m13:15:36.820824 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m13:15:36.821117 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m13:15:36.821689 [info ] [Thread-4 (]: 4 of 10 START sql table model flights_dbt_bronze.bronze_weather ................ [RUN]
[0m13:15:36.822187 [info ] [Thread-2 (]: 2 of 10 START sql view model flights_dbt_bronze.bronze_airports ................ [RUN]
[0m13:15:36.822689 [info ] [Thread-3 (]: 3 of 10 START sql view model flights_dbt_bronze.bronze_flights ................. [RUN]
[0m13:15:36.823162 [info ] [Thread-1 (]: 1 of 10 START sql table model flights_dbt_bronze.bronze_airlines ............... [RUN]
[0m13:15:36.823827 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_weather) - Creating connection
[0m13:15:36.824326 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m13:15:36.824754 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m13:15:36.825143 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m13:15:36.825449 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_weather'
[0m13:15:36.825709 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m13:15:36.825954 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m13:15:36.826184 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m13:15:36.826462 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.bronze_weather
[0m13:15:36.826709 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m13:15:36.826942 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m13:15:36.827164 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m13:15:36.835175 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.bronze_weather"
[0m13:15:36.838167 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m13:15:36.841188 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m13:15:36.848488 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m13:15:36.848953 [debug] [Thread-4 (]: Began executing node model.flights_dbt.bronze_weather
[0m13:15:36.859506 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m13:15:36.859969 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m13:15:36.859797 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:15:36.867283 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m13:15:36.867497 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:15:36.867649 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m13:15:36.867824 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121782990>]}
[0m13:15:36.868856 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m13:15:36.869767 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:15:36.870598 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m13:15:36.870863 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:15:36.877291 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1212efe30>]}
[0m13:15:36.889067 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.bronze_weather"
[0m13:15:36.889721 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m13:15:36.889855 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12320b750>]}
[0m13:15:36.893613 [debug] [Thread-3 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
[0m13:15:36.894036 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
[0m13:15:36.894321 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m13:15:36.894605 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m13:15:36.894769 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m13:15:36.894898 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.bronze_weather"
[0m13:15:36.895019 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`airports`
  
[0m13:15:36.895200 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_weather"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_weather`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`weather`
  
[0m13:15:36.895356 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:15:36.895466 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:15:36.895542 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m13:15:36.895619 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m13:15:36.895863 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select flight_id, airline, origin, arr_time, arr_delay
from `flight_db`.`raw`.`flights`
  )

[0m13:15:36.895993 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
  
  as (
    select
    flight_id,
    airline,
    origin,
    dep_time,
    arr_time,
    arr_delay
from `flight_db`.`raw`.`flights`
  )

[0m13:15:36.896175 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:15:36.896291 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:15:37.601910 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-7be1-17ae-9b2e-c307a2e84667) - Created
[0m13:15:37.618350 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08980-7be5-11c5-afc7-4d2f3b0b1f15) - Created
[0m13:15:37.645728 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08980-7be9-17b8-a634-232f66955661) - Created
[0m13:15:37.648927 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f08980-7be9-1dec-9679-ef3622dee29f) - Created
[0m13:15:38.261499 [debug] [Thread-3 (]: SQL status: OK in 1.360 seconds
[0m13:15:38.263423 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08980-7be1-17ae-9b2e-c307a2e84667, command-id=01f08980-7bfd-15bf-ab36-1a37d295bf1c) - Closing
[0m13:15:38.277327 [debug] [Thread-3 (]: Applying tags to relation None
[0m13:15:38.280963 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m13:15:38.281287 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-7be1-17ae-9b2e-c307a2e84667) - Closing
[0m13:15:38.344250 [debug] [Thread-2 (]: SQL status: OK in 1.450 seconds
[0m13:15:38.345569 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08980-7be5-11c5-afc7-4d2f3b0b1f15, command-id=01f08980-7bff-19bc-bf67-23373ccff374) - Closing
[0m13:15:38.346368 [debug] [Thread-2 (]: Applying tags to relation None
[0m13:15:38.468406 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m13:15:38.469482 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08980-7be5-11c5-afc7-4d2f3b0b1f15) - Closing
[0m13:15:38.668853 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fe9ed0>]}
[0m13:15:38.669405 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123263750>]}
[0m13:15:38.670337 [info ] [Thread-2 (]: 2 of 10 OK created sql view model flights_dbt_bronze.bronze_airports ........... [[32mOK[0m in 1.84s]
[0m13:15:38.670871 [info ] [Thread-3 (]: 3 of 10 OK created sql view model flights_dbt_bronze.bronze_flights ............ [[32mOK[0m in 1.84s]
[0m13:15:38.671532 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m13:15:38.671979 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m13:15:38.672375 [debug] [Thread-2 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m13:15:38.673128 [info ] [Thread-2 (]: 5 of 10 START sql table model flights_dbt.my_first_dbt_model ................... [RUN]
[0m13:15:38.673567 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airports
[0m13:15:38.674210 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m13:15:38.674692 [info ] [Thread-3 (]: 6 of 10 START sql view model flights_dbt_silver.silver_airports ................ [RUN]
[0m13:15:38.675092 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m13:15:38.675530 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m13:15:38.675829 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m13:15:38.676086 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m13:15:38.680000 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m13:15:38.680366 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_airports
[0m13:15:38.683344 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m13:15:38.684035 [debug] [Thread-2 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m13:15:38.684339 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_airports
[0m13:15:38.686841 [debug] [Thread-2 (]: MATERIALIZING TABLE
[0m13:15:38.688265 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:15:38.689813 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m13:15:38.690470 [debug] [Thread-3 (]: Creating view `flight_db`.`flights_dbt_silver`.`silver_airports`
[0m13:15:38.691014 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m13:15:38.691295 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m13:15:38.691560 [debug] [Thread-2 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`flights_dbt`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:15:38.691798 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:15:38.692097 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m13:15:38.692320 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  )

[0m13:15:38.692574 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:15:39.620676 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-7d16-17bb-a130-4a8e47a0ba37) - Created
[0m13:15:39.647845 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08980-7d1b-13a9-825f-055f1f356d14) - Created
[0m13:15:39.668885 [debug] [Thread-4 (]: SQL status: OK in 2.770 seconds
[0m13:15:39.670127 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f08980-7be9-1dec-9679-ef3622dee29f, command-id=01f08980-7c05-115b-8529-07edbcbbbb85) - Closing
[0m13:15:39.674101 [debug] [Thread-4 (]: Applying tags to relation None
[0m13:15:39.687285 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: Close
[0m13:15:39.688079 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f08980-7be9-1dec-9679-ef3622dee29f) - Closing
[0m13:15:39.716514 [debug] [Thread-1 (]: SQL status: OK in 2.820 seconds
[0m13:15:39.717349 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f08980-7be9-17b8-a634-232f66955661, command-id=01f08980-7c05-135b-b17e-1385e9f5f247) - Closing
[0m13:15:39.718020 [debug] [Thread-1 (]: Applying tags to relation None
[0m13:15:39.905750 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m13:15:39.906819 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08980-7be9-17b8-a634-232f66955661) - Closing
[0m13:15:40.002031 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08980-7d31-1c00-b749-815681452763
[0m13:15:40.122784 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1212fe4b0>]}
[0m13:15:40.123680 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: Close
[0m13:15:40.124896 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-7d16-17bb-a130-4a8e47a0ba37) - Closing
[0m13:15:40.124535 [info ] [Thread-4 (]: 4 of 10 OK created sql table model flights_dbt_bronze.bronze_weather ........... [[32mOK[0m in 3.30s]
[0m13:15:40.125978 [debug] [Thread-4 (]: Finished running node model.flights_dbt.bronze_weather
[0m13:15:40.313070 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f8ee10>]}
[0m13:15:40.314266 [info ] [Thread-1 (]: 1 of 10 OK created sql table model flights_dbt_bronze.bronze_airlines .......... [[32mOK[0m in 3.49s]
[0m13:15:40.314890 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m13:15:40.315529 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_flights
[0m13:15:40.315923 [debug] [Thread-4 (]: Began running node model.flights_dbt.silver_airlines
[0m13:15:40.316321 [info ] [Thread-1 (]: 8 of 10 START sql table model flights_dbt_silver.silver_flights ................ [RUN]
[0m13:15:40.316845 [info ] [Thread-4 (]: 7 of 10 START sql view model flights_dbt_silver.silver_airlines ................ [RUN]
[0m13:15:40.317496 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m13:15:40.317949 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airlines) - Creating connection
[0m13:15:40.318258 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m13:15:40.318509 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airlines'
[0m13:15:40.318757 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_flights
[0m13:15:40.319049 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.silver_airlines
[0m13:15:40.323344 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m13:15:40.325543 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.silver_airlines"
[0m13:15:40.326277 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_flights
[0m13:15:40.327973 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m13:15:40.329720 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m13:15:40.330018 [debug] [Thread-4 (]: Began executing node model.flights_dbt.silver_airlines
[0m13:15:40.331512 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m13:15:40.332317 [debug] [Thread-4 (]: Creating view `flight_db`.`flights_dbt_silver`.`silver_airlines`
[0m13:15:40.332865 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.silver_airlines"
[0m13:15:40.333118 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m13:15:40.333432 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_silver`.`silver_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select f.flight_id,
       f.airline_id,
       a.airline_name,
       f.origin_airport,
       f.destination_airport,
       f.departure_time,
       f.arrival_time,
       f.delay
from `flight_db`.`flights_dbt_bronze`.`bronze_flights` f
left join `flight_db`.`flights_dbt_bronze`.`bronze_airlines` a
       on f.airline_id = a.airline_id
  
[0m13:15:40.333677 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:15:40.334132 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.silver_airlines"
[0m13:15:40.334424 [debug] [Thread-4 (]: On model.flights_dbt.silver_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airlines`
  
  as (
    select
    airline
from `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  )

[0m13:15:40.334637 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:15:40.340011 [debug] [Thread-3 (]: Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql
[0m13:15:40.340277 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123428d10>]}
[0m13:15:40.340622 [error] [Thread-3 (]: 6 of 10 ERROR creating sql view model flights_dbt_silver.silver_airports ....... [[31mERROR[0m in 1.66s]
[0m13:15:40.340933 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airports
[0m13:15:40.341228 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_airports' to be skipped because of status 'error'.  Reason: Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql.
[0m13:15:41.014937 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08980-7deb-1daa-8a25-10259b5a67fb) - Created
[0m13:15:41.064295 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f08980-7df3-1542-8b0c-0b2b8c2c93c1) - Created
[0m13:15:41.466965 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airlines`
  
  as (
    select
    airline
from `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08980-7e0e-1654-a93c-a006052e2940
[0m13:15:41.468229 [debug] [Thread-4 (]: On model.flights_dbt.silver_airlines: Close
[0m13:15:41.468605 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f08980-7df3-1542-8b0c-0b2b8c2c93c1) - Closing
[0m13:15:41.575294 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
    
        create or replace table `flight_db`.`flights_dbt_silver`.`silver_flights`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select f.flight_id,
       f.airline_id,
       a.airline_name,
       f.origin_airport,
       f.destination_airport,
       f.departure_time,
       f.arrival_time,
       f.delay
from `flight_db`.`flights_dbt_bronze`.`bronze_flights` f
left join `flight_db`.`flights_dbt_bronze`.`bronze_airlines` a
       on f.airline_id = a.airline_id
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`airline_id` cannot be resolved. Did you mean one of the following? [`f`.`airline`, `a`.`airport_id`, `f`.`flight_id`, `f`.`arr_time`, `f`.`origin`]. SQLSTATE: 42703; line 27 pos 10
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`airline_id` cannot be resolved. Did you mean one of the following? [`f`.`airline`, `a`.`airport_id`, `f`.`flight_id`, `f`.`arr_time`, `f`.`origin`]. SQLSTATE: 42703; line 27 pos 10
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`airline_id` cannot be resolved. Did you mean one of the following? [`f`.`airline`, `a`.`airport_id`, `f`.`flight_id`, `f`.`arr_time`, `f`.`origin`]. SQLSTATE: 42703; line 27 pos 10
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08980-7e08-1f46-b0c6-a025e3c71e5c
[0m13:15:41.640300 [debug] [Thread-2 (]: SQL status: OK in 2.950 seconds
[0m13:15:41.642805 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08980-7d1b-13a9-825f-055f1f356d14, command-id=01f08980-7d36-15c8-a028-83784fae33e8) - Closing
[0m13:15:41.644166 [debug] [Thread-2 (]: Applying tags to relation None
[0m13:15:41.660523 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: Close
[0m13:15:41.660942 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08980-7deb-1daa-8a25-10259b5a67fb) - Closing
[0m13:15:41.675620 [debug] [Thread-4 (]: Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql
[0m13:15:41.871239 [debug] [Thread-2 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m13:15:41.872027 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08980-7d1b-13a9-825f-055f1f356d14) - Closing
[0m13:15:41.877327 [debug] [Thread-1 (]: Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`airline_id` cannot be resolved. Did you mean one of the following? [`f`.`airline`, `a`.`airport_id`, `f`.`flight_id`, `f`.`arr_time`, `f`.`origin`]. SQLSTATE: 42703; line 27 pos 10
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m13:15:42.072950 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123432a50>]}
[0m13:15:42.073964 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123431010>]}
[0m13:15:42.074579 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123452e70>]}
[0m13:15:42.075544 [error] [Thread-4 (]: 7 of 10 ERROR creating sql view model flights_dbt_silver.silver_airlines ....... [[31mERROR[0m in 1.75s]
[0m13:15:42.076380 [error] [Thread-1 (]: 8 of 10 ERROR creating sql table model flights_dbt_silver.silver_flights ....... [[31mERROR[0m in 1.76s]
[0m13:15:42.077008 [info ] [Thread-2 (]: 5 of 10 OK created sql table model flights_dbt.my_first_dbt_model .............. [[32mOK[0m in 3.40s]
[0m13:15:42.077567 [debug] [Thread-4 (]: Finished running node model.flights_dbt.silver_airlines
[0m13:15:42.078058 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_flights
[0m13:15:42.078494 [debug] [Thread-2 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m13:15:42.078962 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_airlines' to be skipped because of status 'error'.  Reason: Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql.
[0m13:15:42.079581 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_flights' to be skipped because of status 'error'.  Reason: Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`airline_id` cannot be resolved. Did you mean one of the following? [`f`.`airline`, `a`.`airport_id`, `f`.`flight_id`, `f`.`arr_time`, `f`.`origin`]. SQLSTATE: 42703; line 27 pos 10
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql.
[0m13:15:42.080485 [debug] [Thread-4 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m13:15:42.080840 [debug] [Thread-3 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m13:15:42.081187 [info ] [Thread-4 (]: 10 of 10 SKIP relation flights_dbt_gold.gold_flight_metrics .................... [[33mSKIP[0m]
[0m13:15:42.081671 [info ] [Thread-3 (]: 9 of 10 START sql view model flights_dbt.my_second_dbt_model ................... [RUN]
[0m13:15:42.082076 [debug] [Thread-4 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m13:15:42.082688 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m13:15:42.083126 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m13:15:42.083455 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m13:15:42.087378 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m13:15:42.088280 [debug] [Thread-3 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m13:15:42.090191 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:15:42.091882 [debug] [Thread-3 (]: Creating view `flight_db`.`flights_dbt`.`my_second_dbt_model`
[0m13:15:42.092617 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m13:15:42.093241 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m13:15:42.093581 [debug] [Thread-3 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`flights_dbt`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`flights_dbt`.`my_first_dbt_model`
where id = 1
  )

[0m13:15:42.093884 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:15:42.802427 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-7efb-115d-97fa-c6eb47263f16) - Created
[0m13:15:43.479203 [debug] [Thread-3 (]: SQL status: OK in 1.380 seconds
[0m13:15:43.481624 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08980-7efb-115d-97fa-c6eb47263f16, command-id=01f08980-7f1b-12f2-a51b-7f06103d11ff) - Closing
[0m13:15:43.482507 [debug] [Thread-3 (]: Applying tags to relation None
[0m13:15:43.483456 [debug] [Thread-3 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m13:15:43.483738 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-7efb-115d-97fa-c6eb47263f16) - Closing
[0m13:15:43.684215 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd202d83a-3203-43a3-9120-4021ca4303d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1232e9d30>]}
[0m13:15:43.685500 [info ] [Thread-3 (]: 9 of 10 OK created sql view model flights_dbt.my_second_dbt_model .............. [[32mOK[0m in 1.60s]
[0m13:15:43.686185 [debug] [Thread-3 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m13:15:43.688134 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:15:43.688536 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:15:43.689104 [info ] [MainThread]: 
[0m13:15:43.689439 [info ] [MainThread]: Finished running 5 table models, 5 view models in 0 hours 0 minutes and 11.65 seconds (11.65s).
[0m13:15:43.691148 [debug] [MainThread]: Command end result
[0m13:15:43.730235 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:15:43.732244 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:15:43.736786 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m13:15:43.736965 [info ] [MainThread]: 
[0m13:15:43.737171 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m13:15:43.737316 [info ] [MainThread]: 
[0m13:15:43.737509 [error] [MainThread]: [31mFailure in model silver_airports (models/silver/silver_airports.sql)[0m
[0m13:15:43.737684 [error] [MainThread]:   Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql
[0m13:15:43.737807 [info ] [MainThread]: 
[0m13:15:43.737955 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_airports.sql
[0m13:15:43.738075 [info ] [MainThread]: 
[0m13:15:43.738226 [error] [MainThread]: [31mFailure in model silver_airlines (models/silver/silver_airlines.sql)[0m
[0m13:15:43.738377 [error] [MainThread]:   Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql
[0m13:15:43.738496 [info ] [MainThread]: 
[0m13:15:43.738636 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_airlines.sql
[0m13:15:43.738750 [info ] [MainThread]: 
[0m13:15:43.738894 [error] [MainThread]: [31mFailure in model silver_flights (models/silver/silver_flights.sql)[0m
[0m13:15:43.739041 [error] [MainThread]:   Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`airline_id` cannot be resolved. Did you mean one of the following? [`f`.`airline`, `a`.`airport_id`, `f`.`flight_id`, `f`.`arr_time`, `f`.`origin`]. SQLSTATE: 42703; line 27 pos 10
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m13:15:43.739158 [info ] [MainThread]: 
[0m13:15:43.739294 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_flights.sql
[0m13:15:43.739404 [info ] [MainThread]: 
[0m13:15:43.739551 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=3 SKIP=1 NO-OP=0 TOTAL=10
[0m13:15:43.743215 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 12.725232, "process_in_blocks": "0", "process_kernel_time": 0.287052, "process_mem_max_rss": "257441792", "process_out_blocks": "0", "process_user_time": 3.569646}
[0m13:15:43.743426 [debug] [MainThread]: Command `dbt run` failed at 13:15:43.743382 after 12.73 seconds
[0m13:15:43.743607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107106cf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123432a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123497c50>]}
[0m13:15:43.743761 [debug] [MainThread]: Flushing usage events
[0m13:15:44.506443 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:18:22.801986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a4b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a63890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a63b10>]}


============================== 13:18:22.804275 | 4b2a7b4f-ed31-42e8-a845-e032124ff427 ==============================
[0m13:18:22.804275 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:18:22.804522 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'indirect_selection': 'eager', 'fail_fast': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run --target dev', 'version_check': 'True', 'debug': 'False', 'warn_error': 'None', 'log_format': 'default', 'write_json': 'True', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'quiet': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'use_colors': 'True', 'no_print': 'None', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'static_parser': 'True', 'introspect': 'True'}
[0m13:18:23.098067 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:18:23.098257 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:18:23.098349 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:18:23.526602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d93360>]}
[0m13:18:23.545188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c6aad0>]}
[0m13:18:23.545409 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:18:23.593177 [debug] [MainThread]: checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092, vars: {}, profile: , target: dev, version: 1.10.9
[0m13:18:23.650100 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:18:23.650343 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/silver/silver_flights.sql
[0m13:18:23.741738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f917d50>]}
[0m13:18:23.773973 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:18:23.774958 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:18:23.781177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f50e6c0>]}
[0m13:18:23.781325 [info ] [MainThread]: Found 10 models, 4 data tests, 3 sources, 686 macros
[0m13:18:23.781431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa15fd0>]}
[0m13:18:23.782202 [info ] [MainThread]: 
[0m13:18:23.782322 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:18:23.782404 [info ] [MainThread]: 
[0m13:18:23.782582 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:18:23.782675 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:18:23.785047 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:18:23.785179 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:18:23.785322 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:18:23.788640 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:18:23.788817 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:18:23.788953 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:18:23.789119 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:18:23.789241 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:18:23.789330 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:18:23.790131 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:18:23.790242 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:18:23.790336 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:23.790981 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:18:23.791072 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:18:23.791678 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:18:23.792255 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:18:23.792335 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:23.792432 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:18:23.792520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:23.792642 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:24.696638 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-df7a-130c-b15e-249416b615c3) - Created
[0m13:18:24.699203 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-df7c-12b0-a4a1-869b57afc7e1) - Created
[0m13:18:24.701744 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-df7b-1f7e-a879-f47e9b023d80) - Created
[0m13:18:24.703088 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-df7a-1ed5-98f6-3eaf149cd00d) - Created
[0m13:18:25.386207 [debug] [ThreadPool]: SQL status: OK in 1.590 seconds
[0m13:18:25.386679 [debug] [ThreadPool]: SQL status: OK in 1.590 seconds
[0m13:18:25.386968 [debug] [ThreadPool]: SQL status: OK in 1.600 seconds
[0m13:18:25.387213 [debug] [ThreadPool]: SQL status: OK in 1.590 seconds
[0m13:18:25.396692 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-df7c-12b0-a4a1-869b57afc7e1, command-id=01f08980-df97-1ad2-a958-daea40af4a38) - Closing
[0m13:18:25.397726 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-df7a-130c-b15e-249416b615c3, command-id=01f08980-df9a-1070-8bb5-9fd507ef36ec) - Closing
[0m13:18:25.398486 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-df7a-1ed5-98f6-3eaf149cd00d, command-id=01f08980-df9a-1646-92e9-3171d9813d92) - Closing
[0m13:18:25.398905 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:18:25.399588 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-df7b-1f7e-a879-f47e9b023d80, command-id=01f08980-df9e-1a0d-9470-a7a408e4da08) - Closing
[0m13:18:25.399989 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-df7c-12b0-a4a1-869b57afc7e1) - Closing
[0m13:18:25.597796 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:18:25.598669 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-df7a-130c-b15e-249416b615c3) - Closing
[0m13:18:25.790153 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:18:25.791138 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-df7a-1ed5-98f6-3eaf149cd00d) - Closing
[0m13:18:25.972566 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:18:25.973554 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-df7b-1f7e-a879-f47e9b023d80) - Closing
[0m13:18:26.184721 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m13:18:26.185461 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m13:18:26.185749 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m13:18:26.186120 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m13:18:26.186417 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m13:18:26.186754 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m13:18:26.194722 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m13:18:26.195015 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m13:18:26.196991 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m13:18:26.197935 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m13:18:26.201342 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m13:18:26.202595 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m13:18:26.203916 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m13:18:26.204093 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:26.204285 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m13:18:26.204480 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m13:18:26.204677 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m13:18:26.204990 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:26.205129 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:26.205322 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:26.940643 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-e0d2-1b61-8095-706687727f66) - Created
[0m13:18:26.943002 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-e0d2-1d17-b988-f872e78a4605) - Created
[0m13:18:26.991433 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-e0da-1c42-b249-8e89ace0f6aa) - Created
[0m13:18:26.996113 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-e0da-1dc3-8bad-64d7e73e4bee) - Created
[0m13:18:27.442957 [debug] [ThreadPool]: SQL status: OK in 1.240 seconds
[0m13:18:27.445813 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-e0d2-1b61-8095-706687727f66, command-id=01f08980-e0ec-1718-a817-9a4916e67ed1) - Closing
[0m13:18:27.446436 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m13:18:27.446643 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-e0d2-1b61-8095-706687727f66) - Closing
[0m13:18:27.524305 [debug] [ThreadPool]: SQL status: OK in 1.320 seconds
[0m13:18:27.526781 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-e0d2-1d17-b988-f872e78a4605, command-id=01f08980-e0ec-1fd2-8868-a945e1d5e92b) - Closing
[0m13:18:27.588949 [debug] [ThreadPool]: SQL status: OK in 1.380 seconds
[0m13:18:27.591409 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-e0da-1c42-b249-8e89ace0f6aa, command-id=01f08980-e0f5-1226-bb81-a19cd5053cb3) - Closing
[0m13:18:27.629455 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m13:18:27.631426 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-e0d2-1d17-b988-f872e78a4605) - Closing
[0m13:18:27.821569 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m13:18:27.822393 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-e0da-1c42-b249-8e89ace0f6aa) - Closing
[0m13:18:27.879055 [debug] [ThreadPool]: SQL status: OK in 1.670 seconds
[0m13:18:27.882012 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08980-e0da-1dc3-8bad-64d7e73e4bee, command-id=01f08980-e0f6-110d-b302-f1ec68a10557) - Closing
[0m13:18:28.057478 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m13:18:28.058450 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08980-e0da-1dc3-8bad-64d7e73e4bee) - Closing
[0m13:18:28.312201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd32b60>]}
[0m13:18:28.317154 [debug] [Thread-4 (]: Began running node model.flights_dbt.bronze_weather
[0m13:18:28.317631 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m13:18:28.317940 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m13:18:28.318214 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m13:18:28.318787 [info ] [Thread-4 (]: 4 of 10 START sql table model flights_dbt_bronze.bronze_weather ................ [RUN]
[0m13:18:28.319255 [info ] [Thread-1 (]: 1 of 10 START sql table model flights_dbt_bronze.bronze_airlines ............... [RUN]
[0m13:18:28.319669 [info ] [Thread-2 (]: 2 of 10 START sql view model flights_dbt_bronze.bronze_airports ................ [RUN]
[0m13:18:28.320136 [info ] [Thread-3 (]: 3 of 10 START sql view model flights_dbt_bronze.bronze_flights ................. [RUN]
[0m13:18:28.320873 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_weather) - Creating connection
[0m13:18:28.321303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m13:18:28.321671 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m13:18:28.322028 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m13:18:28.322292 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_weather'
[0m13:18:28.322532 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m13:18:28.322759 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m13:18:28.322980 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m13:18:28.323240 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.bronze_weather
[0m13:18:28.323481 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m13:18:28.323708 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m13:18:28.323929 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m13:18:28.332841 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m13:18:28.336462 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m13:18:28.339008 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m13:18:28.344561 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.bronze_weather"
[0m13:18:28.345610 [debug] [Thread-4 (]: Began executing node model.flights_dbt.bronze_weather
[0m13:18:28.345827 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m13:18:28.346047 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m13:18:28.346213 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m13:18:28.355755 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m13:18:28.362979 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:18:28.363857 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m13:18:28.364800 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m13:18:28.365036 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:18:28.366127 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:18:28.366390 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:18:28.366614 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:18:28.366791 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd8e990>]}
[0m13:18:28.366915 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd87cd0>]}
[0m13:18:28.367034 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd87e30>]}
[0m13:18:28.367146 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe8b750>]}
[0m13:18:28.376837 [debug] [Thread-3 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
[0m13:18:28.390933 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.bronze_weather"
[0m13:18:28.389450 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m13:18:28.391409 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
[0m13:18:28.391690 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m13:18:28.392085 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m13:18:28.404314 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m13:18:28.404440 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m13:18:28.404610 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`airports`
  
[0m13:18:28.404735 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m13:18:28.404831 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.bronze_weather"
[0m13:18:28.405439 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select flight_id, airline, origin, arr_time, arr_delay
from `flight_db`.`raw`.`flights`
  )

[0m13:18:28.405572 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:18:28.406309 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
  
  as (
    select
    flight_id,
    airline,
    origin,
    dep_time,
    arr_time,
    arr_delay
from `flight_db`.`raw`.`flights`
  )

[0m13:18:28.406451 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_weather"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_weather`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`weather`
  
[0m13:18:28.406567 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:18:28.406761 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:18:28.406876 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:18:29.163348 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-e222-160e-b470-c5e063fc7e9f) - Created
[0m13:18:29.192640 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08980-e226-18a4-bbc9-899963c70d16) - Created
[0m13:18:29.195421 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08980-e228-130d-ad42-fa10f4715224) - Created
[0m13:18:29.200807 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f08980-e22a-15ef-8ebc-d6d9c89378b7) - Created
[0m13:18:29.912468 [debug] [Thread-3 (]: SQL status: OK in 1.510 seconds
[0m13:18:29.914382 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08980-e222-160e-b470-c5e063fc7e9f, command-id=01f08980-e23f-168c-8ec1-240e7a0e5e4d) - Closing
[0m13:18:29.929045 [debug] [Thread-3 (]: Applying tags to relation None
[0m13:18:29.932981 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m13:18:29.933318 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-e222-160e-b470-c5e063fc7e9f) - Closing
[0m13:18:30.090325 [debug] [Thread-2 (]: SQL status: OK in 1.680 seconds
[0m13:18:30.091920 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08980-e228-130d-ad42-fa10f4715224, command-id=01f08980-e24e-19b8-8cc6-c9244c5e726f) - Closing
[0m13:18:30.093036 [debug] [Thread-2 (]: Applying tags to relation None
[0m13:18:30.152982 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m13:18:30.153819 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08980-e228-130d-ad42-fa10f4715224) - Closing
[0m13:18:30.363906 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f020b0>]}
[0m13:18:30.364554 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fefc910>]}
[0m13:18:30.365497 [info ] [Thread-2 (]: 2 of 10 OK created sql view model flights_dbt_bronze.bronze_airports ........... [[32mOK[0m in 2.04s]
[0m13:18:30.366652 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m13:18:30.366138 [info ] [Thread-3 (]: 3 of 10 OK created sql view model flights_dbt_bronze.bronze_flights ............ [[32mOK[0m in 2.04s]
[0m13:18:30.367148 [debug] [Thread-2 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m13:18:30.367759 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m13:18:30.368351 [info ] [Thread-2 (]: 5 of 10 START sql table model flights_dbt.my_first_dbt_model ................... [RUN]
[0m13:18:30.368795 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airports
[0m13:18:30.369460 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m13:18:30.370033 [info ] [Thread-3 (]: 6 of 10 START sql view model flights_dbt_silver.silver_airports ................ [RUN]
[0m13:18:30.370355 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m13:18:30.370778 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m13:18:30.371085 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m13:18:30.371361 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m13:18:30.373369 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_airports
[0m13:18:30.378204 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m13:18:30.382403 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m13:18:30.383389 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_airports
[0m13:18:30.383662 [debug] [Thread-2 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m13:18:30.385165 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:18:30.386868 [debug] [Thread-2 (]: MATERIALIZING TABLE
[0m13:18:30.387623 [debug] [Thread-3 (]: Creating view `flight_db`.`flights_dbt_silver`.`silver_airports`
[0m13:18:30.388882 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m13:18:30.389430 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m13:18:30.389894 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m13:18:30.390087 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m13:18:30.390315 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  )

[0m13:18:30.390557 [debug] [Thread-2 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`flights_dbt`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:18:30.390754 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:18:30.390918 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:18:31.113394 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08980-e34e-1fb5-b665-701512a38ac4) - Created
[0m13:18:31.169396 [debug] [Thread-1 (]: SQL status: OK in 2.760 seconds
[0m13:18:31.171292 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f08980-e226-18a4-bbc9-899963c70d16, command-id=01f08980-e247-1191-921b-4973e5f26d88) - Closing
[0m13:18:31.176827 [debug] [Thread-1 (]: Applying tags to relation None
[0m13:18:31.178399 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-e357-155a-8a5e-ef023a0c9d23) - Created
[0m13:18:31.194477 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m13:18:31.196454 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08980-e226-18a4-bbc9-899963c70d16) - Closing
[0m13:18:31.197004 [debug] [Thread-4 (]: SQL status: OK in 2.790 seconds
[0m13:18:31.197822 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f08980-e22a-15ef-8ebc-d6d9c89378b7, command-id=01f08980-e245-19f8-a4ee-92de7f167f98) - Closing
[0m13:18:31.198467 [debug] [Thread-4 (]: Applying tags to relation None
[0m13:18:31.399687 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: Close
[0m13:18:31.400336 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f08980-e22a-15ef-8ebc-d6d9c89378b7) - Closing
[0m13:18:31.542512 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08980-e376-14ef-8c52-0a03421ff840
[0m13:18:31.602202 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f9f1eb0>]}
[0m13:18:31.603272 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: Close
[0m13:18:31.604769 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-e357-155a-8a5e-ef023a0c9d23) - Closing
[0m13:18:31.604353 [info ] [Thread-1 (]: 1 of 10 OK created sql table model flights_dbt_bronze.bronze_airlines .......... [[32mOK[0m in 3.28s]
[0m13:18:31.606024 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m13:18:31.606487 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_flights
[0m13:18:31.607165 [info ] [Thread-1 (]: 7 of 10 START sql view model flights_dbt_silver.silver_flights ................. [RUN]
[0m13:18:31.799613 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa92990>]}
[0m13:18:31.800448 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m13:18:31.801458 [info ] [Thread-4 (]: 4 of 10 OK created sql table model flights_dbt_bronze.bronze_weather ........... [[32mOK[0m in 3.48s]
[0m13:18:31.801964 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m13:18:31.802508 [debug] [Thread-4 (]: Finished running node model.flights_dbt.bronze_weather
[0m13:18:31.802999 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_flights
[0m13:18:31.803423 [debug] [Thread-4 (]: Began running node model.flights_dbt.silver_airlines
[0m13:18:31.808557 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m13:18:31.809123 [info ] [Thread-4 (]: 8 of 10 START sql view model flights_dbt_silver.silver_airlines ................ [RUN]
[0m13:18:31.809827 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airlines) - Creating connection
[0m13:18:31.810163 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airlines'
[0m13:18:31.810530 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.silver_airlines
[0m13:18:31.813288 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.silver_airlines"
[0m13:18:31.813628 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_flights
[0m13:18:31.816078 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m13:18:31.817103 [debug] [Thread-1 (]: Creating view `flight_db`.`flights_dbt_silver`.`silver_flights`
[0m13:18:31.817858 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m13:18:31.818317 [debug] [Thread-4 (]: Began executing node model.flights_dbt.silver_airlines
[0m13:18:31.819921 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m13:18:31.820731 [debug] [Thread-4 (]: Creating view `flight_db`.`flights_dbt_silver`.`silver_airlines`
[0m13:18:31.821322 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.silver_airlines"
[0m13:18:31.821570 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m13:18:31.821967 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_flights`
  
  as (
    select
    f.flight_id,
    f.airline,
    f.origin,
    f.dep_time,
    f.arr_time,
    f.arr_delay
from `flight_db`.`flights_dbt_bronze`.`bronze_flights` as f
  )

[0m13:18:31.822338 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:18:31.822881 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.silver_airlines"
[0m13:18:31.823214 [debug] [Thread-4 (]: On model.flights_dbt.silver_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airlines`
  
  as (
    select
    airline
from `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  )

[0m13:18:31.823460 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:18:31.830000 [debug] [Thread-3 (]: Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql
[0m13:18:31.830279 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11043d0d0>]}
[0m13:18:31.830632 [error] [Thread-3 (]: 6 of 10 ERROR creating sql view model flights_dbt_silver.silver_airports ....... [[31mERROR[0m in 1.46s]
[0m13:18:31.830940 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airports
[0m13:18:31.831229 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_airports' to be skipped because of status 'error'.  Reason: Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql.
[0m13:18:32.514503 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08980-e423-119d-8b49-a336a9526f77) - Created
[0m13:18:32.515426 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f08980-e423-15c7-ab00-69a080cd6dbb) - Created
[0m13:18:32.864447 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_silver`.`silver_airlines`
  
  as (
    select
    airline
from `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08980-e440-141b-9d11-589b3afdc7be
[0m13:18:32.866415 [debug] [Thread-4 (]: On model.flights_dbt.silver_airlines: Close
[0m13:18:32.867134 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f08980-e423-15c7-ab00-69a080cd6dbb) - Closing
[0m13:18:32.899153 [debug] [Thread-2 (]: SQL status: OK in 2.510 seconds
[0m13:18:32.900192 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08980-e34e-1fb5-b665-701512a38ac4, command-id=01f08980-e368-16b5-ae27-81459c0d1669) - Closing
[0m13:18:32.901000 [debug] [Thread-2 (]: Applying tags to relation None
[0m13:18:33.065699 [debug] [Thread-2 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m13:18:33.066454 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08980-e34e-1fb5-b665-701512a38ac4) - Closing
[0m13:18:33.072212 [debug] [Thread-4 (]: Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql
[0m13:18:33.133237 [debug] [Thread-1 (]: SQL status: OK in 1.310 seconds
[0m13:18:33.134792 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f08980-e423-119d-8b49-a336a9526f77, command-id=01f08980-e43f-1dd4-8c73-e200531f754b) - Closing
[0m13:18:33.135695 [debug] [Thread-1 (]: Applying tags to relation None
[0m13:18:33.257537 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104066f0>]}
[0m13:18:33.258588 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: Close
[0m13:18:33.260163 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08980-e423-119d-8b49-a336a9526f77) - Closing
[0m13:18:33.259754 [error] [Thread-4 (]: 8 of 10 ERROR creating sql view model flights_dbt_silver.silver_airlines ....... [[31mERROR[0m in 1.45s]
[0m13:18:33.261510 [debug] [Thread-4 (]: Finished running node model.flights_dbt.silver_airlines
[0m13:18:33.262022 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_airlines' to be skipped because of status 'error'.  Reason: Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql.
[0m13:18:33.459040 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104066f0>]}
[0m13:18:33.460052 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110404ef0>]}
[0m13:18:33.461279 [info ] [Thread-2 (]: 5 of 10 OK created sql table model flights_dbt.my_first_dbt_model .............. [[32mOK[0m in 3.09s]
[0m13:18:33.461962 [info ] [Thread-1 (]: 7 of 10 OK created sql view model flights_dbt_silver.silver_flights ............ [[32mOK[0m in 1.85s]
[0m13:18:33.462706 [debug] [Thread-2 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m13:18:33.463156 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_flights
[0m13:18:33.463877 [debug] [Thread-3 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m13:18:33.464599 [debug] [Thread-4 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m13:18:33.464309 [info ] [Thread-3 (]: 9 of 10 START sql view model flights_dbt.my_second_dbt_model ................... [RUN]
[0m13:18:33.465044 [info ] [Thread-4 (]: 10 of 10 SKIP relation flights_dbt_gold.gold_flight_metrics .................... [[33mSKIP[0m]
[0m13:18:33.465669 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m13:18:33.466034 [debug] [Thread-4 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m13:18:33.466349 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m13:18:33.466746 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m13:18:33.470990 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m13:18:33.471923 [debug] [Thread-3 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m13:18:33.474059 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:18:33.475793 [debug] [Thread-3 (]: Creating view `flight_db`.`flights_dbt`.`my_second_dbt_model`
[0m13:18:33.476537 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m13:18:33.477183 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m13:18:33.477504 [debug] [Thread-3 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`flights_dbt`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`flights_dbt`.`my_first_dbt_model`
where id = 1
  )

[0m13:18:33.477799 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:18:34.247827 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-e52c-1d71-890b-505dcf7488e8) - Created
[0m13:18:34.902923 [debug] [Thread-3 (]: SQL status: OK in 1.420 seconds
[0m13:18:34.905227 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08980-e52c-1d71-890b-505dcf7488e8, command-id=01f08980-e547-1d6b-b441-e142f2ba90c6) - Closing
[0m13:18:34.906362 [debug] [Thread-3 (]: Applying tags to relation None
[0m13:18:34.907428 [debug] [Thread-3 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m13:18:34.907789 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08980-e52c-1d71-890b-505dcf7488e8) - Closing
[0m13:18:35.121348 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b2a7b4f-ed31-42e8-a845-e032124ff427', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104064b0>]}
[0m13:18:35.122709 [info ] [Thread-3 (]: 9 of 10 OK created sql view model flights_dbt.my_second_dbt_model .............. [[32mOK[0m in 1.66s]
[0m13:18:35.123393 [debug] [Thread-3 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m13:18:35.125220 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:18:35.125580 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:18:35.126122 [info ] [MainThread]: 
[0m13:18:35.126442 [info ] [MainThread]: Finished running 4 table models, 6 view models in 0 hours 0 minutes and 11.34 seconds (11.34s).
[0m13:18:35.128183 [debug] [MainThread]: Command end result
[0m13:18:35.157933 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:18:35.159366 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:18:35.163810 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m13:18:35.163965 [info ] [MainThread]: 
[0m13:18:35.164145 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m13:18:35.164280 [info ] [MainThread]: 
[0m13:18:35.164447 [error] [MainThread]: [31mFailure in model silver_airports (models/silver/silver_airports.sql)[0m
[0m13:18:35.164605 [error] [MainThread]:   Database Error in model silver_airports (models/silver/silver_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_id` cannot be resolved. Did you mean one of the following? [`flight_id`, `airline`, `origin`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airports.sql
[0m13:18:35.164730 [info ] [MainThread]: 
[0m13:18:35.164868 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_airports.sql
[0m13:18:35.164979 [info ] [MainThread]: 
[0m13:18:35.165121 [error] [MainThread]: [31mFailure in model silver_airlines (models/silver/silver_airlines.sql)[0m
[0m13:18:35.165261 [error] [MainThread]:   Database Error in model silver_airlines (models/silver/silver_airlines.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airline` cannot be resolved. Did you mean one of the following? [`city`, `state`, `airport_id`, `country`, `iata_code`]. SQLSTATE: 42703; line 9 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_airlines.sql
[0m13:18:35.165368 [info ] [MainThread]: 
[0m13:18:35.165491 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_airlines.sql
[0m13:18:35.165602 [info ] [MainThread]: 
[0m13:18:35.165722 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=2 SKIP=1 NO-OP=0 TOTAL=10
[0m13:18:35.168826 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 12.396892, "process_in_blocks": "0", "process_kernel_time": 0.296299, "process_mem_max_rss": "253362176", "process_out_blocks": "0", "process_user_time": 3.624562}
[0m13:18:35.169038 [debug] [MainThread]: Command `dbt run` failed at 13:18:35.168996 after 12.40 seconds
[0m13:18:35.169210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d326f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104a7fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104a7a10>]}
[0m13:18:35.169366 [debug] [MainThread]: Flushing usage events
[0m13:18:36.097292 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:22:10.629251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c37620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e0f890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e0fb10>]}


============================== 13:22:10.631558 | cbdc42c3-9d3c-4806-9eaa-eb786366cf2e ==============================
[0m13:22:10.631558 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:22:10.631789 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/artakerqeli/.dbt', 'quiet': 'False', 'no_print': 'None', 'empty': 'False', 'invocation_command': 'dbt run --select bronze_*', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'debug': 'False', 'partial_parse': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'cache_selected_only': 'False', 'version_check': 'True', 'introspect': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'static_parser': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None', 'use_colors': 'True', 'printer_width': '80'}
[0m13:22:10.925349 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:22:10.925533 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:22:10.925627 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:22:11.343552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cbdc42c3-9d3c-4806-9eaa-eb786366cf2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10413f360>]}
[0m13:22:11.362765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cbdc42c3-9d3c-4806-9eaa-eb786366cf2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106016ad0>]}
[0m13:22:11.363000 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:22:11.412580 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:22:11.449290 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m13:22:11.449532 [debug] [MainThread]: previous checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, current checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092
[0m13:22:11.449661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cbdc42c3-9d3c-4806-9eaa-eb786366cf2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb9b550>]}
[0m13:22:11.948720 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_flights_airports".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_flights", "airports").
  
  To fix this, change the name of one of these resources:
  - source.flights_dbt.raw_flights.airports (models/bronze/sources.yml)
  - source.flights_dbt.raw_flights.airports (models/sources.yml)
[0m13:22:11.950843 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3517078, "process_in_blocks": "0", "process_kernel_time": 0.208201, "process_mem_max_rss": "230686720", "process_out_blocks": "0", "process_user_time": 1.45802}
[0m13:22:11.951033 [debug] [MainThread]: Command `dbt run` failed at 13:22:11.950996 after 1.35 seconds
[0m13:22:11.951195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc96d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc96e40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cc3e70>]}
[0m13:22:11.951302 [debug] [MainThread]: Flushing usage events
[0m13:22:12.624103 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:23:07.488009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108543620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10971b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10971bb10>]}


============================== 13:23:07.490309 | b80611bf-3390-4271-8afa-e0cf8c5e0eb6 ==============================
[0m13:23:07.490309 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:23:07.490542 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'no_print': 'None', 'invocation_command': 'dbt run --select bronze_*', 'debug': 'False', 'use_colors': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'log_format': 'default', 'indirect_selection': 'eager', 'write_json': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'quiet': 'False', 'log_cache_events': 'False', 'version_check': 'True', 'introspect': 'True', 'static_parser': 'True'}
[0m13:23:07.788394 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:23:07.788656 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:23:07.788768 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:23:08.220730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b80611bf-3390-4271-8afa-e0cf8c5e0eb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a4b360>]}
[0m13:23:08.239576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b80611bf-3390-4271-8afa-e0cf8c5e0eb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a922ad0>]}
[0m13:23:08.239825 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:23:08.295598 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:23:08.331944 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m13:23:08.332122 [debug] [MainThread]: previous checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, current checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092
[0m13:23:08.332242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b80611bf-3390-4271-8afa-e0cf8c5e0eb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104ab550>]}
[0m13:23:08.829291 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_flights_flights".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_flights", "flights").
  
  To fix this, change the name of one of these resources:
  - source.flights_dbt.raw_flights.flights (models/bronze/sources.yml)
  - source.flights_dbt.raw_flights.flights (models/sources.yml)
[0m13:23:08.831916 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3736677, "process_in_blocks": "0", "process_kernel_time": 0.218303, "process_mem_max_rss": "231129088", "process_out_blocks": "0", "process_user_time": 1.459828}
[0m13:23:08.832110 [debug] [MainThread]: Command `dbt run` failed at 13:23:08.832074 after 1.37 seconds
[0m13:23:08.832273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105bee40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105bef30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095cfe70>]}
[0m13:23:08.832385 [debug] [MainThread]: Flushing usage events
[0m13:23:09.664675 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:24:02.095553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108773620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10978b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10978bb10>]}


============================== 13:24:02.097821 | 2618c87e-8ca5-4d35-bdad-ba0c00f0b6cb ==============================
[0m13:24:02.097821 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:24:02.098069 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'invocation_command': 'dbt run --select bronze_*', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'fail_fast': 'False', 'version_check': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'use_colors': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'write_json': 'True', 'target_path': 'None', 'empty': 'False', 'static_parser': 'True', 'debug': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'warn_error': 'None', 'cache_selected_only': 'False', 'no_print': 'None', 'partial_parse': 'True'}
[0m13:24:02.397339 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:24:02.397535 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:24:02.397634 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:24:02.814324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2618c87e-8ca5-4d35-bdad-ba0c00f0b6cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108abb360>]}
[0m13:24:02.833224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2618c87e-8ca5-4d35-bdad-ba0c00f0b6cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a996ad0>]}
[0m13:24:02.833463 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:24:02.887702 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:24:02.924413 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m13:24:02.924584 [debug] [MainThread]: previous checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, current checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092
[0m13:24:02.924708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2618c87e-8ca5-4d35-bdad-ba0c00f0b6cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110693550>]}
[0m13:24:03.423835 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_flights_weather".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_flights", "weather").
  
  To fix this, change the name of one of these resources:
  - source.flights_dbt.raw_flights.weather (models/bronze/sources.yml)
  - source.flights_dbt.raw_flights.weather (models/sources.yml)
[0m13:24:03.425963 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3601688, "process_in_blocks": "0", "process_kernel_time": 0.221526, "process_mem_max_rss": "231702528", "process_out_blocks": "0", "process_user_time": 1.455663}
[0m13:24:03.426154 [debug] [MainThread]: Command `dbt run` failed at 13:24:03.426118 after 1.36 seconds
[0m13:24:03.426330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11079af30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11079b020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10963fe70>]}
[0m13:24:03.426443 [debug] [MainThread]: Flushing usage events
[0m13:24:04.103594 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:25:42.123240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106bab890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106babb10>]}


============================== 13:25:42.125548 | 582ebb1d-bfeb-4110-91d0-c33bf13dc2cc ==============================
[0m13:25:42.125548 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:25:42.125769 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'log_format': 'default', 'cache_selected_only': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'static_parser': 'True', 'introspect': 'True', 'partial_parse': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select bronze_*', 'profiles_dir': '/Users/artakerqeli/.dbt', 'use_colors': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'target_path': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'empty': 'False', 'fail_fast': 'False'}
[0m13:25:42.422754 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:25:42.422956 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:25:42.423055 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:25:42.863307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '582ebb1d-bfeb-4110-91d0-c33bf13dc2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ed3360>]}
[0m13:25:42.882876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '582ebb1d-bfeb-4110-91d0-c33bf13dc2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107db2ad0>]}
[0m13:25:42.883125 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:25:42.931862 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:25:42.968259 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m13:25:42.968449 [debug] [MainThread]: previous checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, current checksum: 29ea1ce3a69c23697e5b867170111b74ec8b8d94cb5dc8274e6fc1c7cd223092
[0m13:25:42.968575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '582ebb1d-bfeb-4110-91d0-c33bf13dc2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d937550>]}
[0m13:25:43.570491 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.flights_dbt.bronze_airports' (models/bronze/bronze_airports.sql) depends on a source named 'raw_flights.airports' which was not found
[0m13:25:43.574019 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.4836204, "process_in_blocks": "0", "process_kernel_time": 0.219257, "process_mem_max_rss": "232751104", "process_out_blocks": "0", "process_user_time": 1.570942}
[0m13:25:43.574192 [debug] [MainThread]: Command `dbt run` failed at 13:25:43.574158 after 1.48 seconds
[0m13:25:43.574343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da30500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da30c80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de5c2f0>]}
[0m13:25:43.574463 [debug] [MainThread]: Flushing usage events
[0m13:25:44.176219 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:26:03.462128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107973620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108983890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108983b10>]}


============================== 13:26:03.464366 | 9c145777-d5da-4c27-b815-f4b31e2dbb09 ==============================
[0m13:26:03.464366 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:26:03.464617 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'log_format': 'default', 'printer_width': '80', 'debug': 'False', 'invocation_command': 'dbt clean', 'cache_selected_only': 'False', 'introspect': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'write_json': 'True', 'target_path': 'None'}
[0m13:26:03.502475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9c145777-d5da-4c27-b815-f4b31e2dbb09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cb7360>]}
[0m13:26:03.513043 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.078587204, "process_in_blocks": "0", "process_kernel_time": 0.103239, "process_mem_max_rss": "110051328", "process_out_blocks": "0", "process_user_time": 0.490562}
[0m13:26:03.513242 [debug] [MainThread]: Command `dbt clean` succeeded at 13:26:03.513196 after 0.08 seconds
[0m13:26:03.513371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10882aad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10882a690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b29350>]}
[0m13:26:03.513486 [debug] [MainThread]: Flushing usage events
[0m13:26:04.047365 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:26:08.065498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107be7620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108bff890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108bffb10>]}


============================== 13:26:08.067237 | d0909dd7-7767-4023-8509-d3e342208f38 ==============================
[0m13:26:08.067237 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:26:08.067467 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'warn_error': 'None', 'cache_selected_only': 'False', 'use_colors': 'True', 'write_json': 'True', 'no_print': 'None', 'partial_parse': 'True', 'log_cache_events': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'introspect': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'log_format': 'default', 'profiles_dir': '/Users/artakerqeli/.dbt', 'empty': 'False', 'quiet': 'False', 'invocation_command': 'dbt run --select bronze_*', 'target_path': 'None', 'use_experimental_parser': 'False', 'version_check': 'True'}
[0m13:26:08.380014 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:26:08.380292 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:26:08.380422 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:26:08.783876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd0909dd7-7767-4023-8509-d3e342208f38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f2f360>]}
[0m13:26:08.803254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd0909dd7-7767-4023-8509-d3e342208f38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e06ad0>]}
[0m13:26:08.803480 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:26:08.853245 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:26:08.853539 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:26:08.853656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd0909dd7-7767-4023-8509-d3e342208f38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f747550>]}
[0m13:26:09.367625 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_flights_flights".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_flights", "flights").
  
  To fix this, change the name of one of these resources:
  - source.flights_dbt.raw_flights.flights (models/bronze/sources.yml)
  - source.flights_dbt.raw_flights.flights (models/sources.yml)
[0m13:26:09.369585 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.330718, "process_in_blocks": "0", "process_kernel_time": 0.189441, "process_mem_max_rss": "230227968", "process_out_blocks": "0", "process_user_time": 1.477376}
[0m13:26:09.369789 [debug] [MainThread]: Command `dbt run` failed at 13:26:09.369749 after 1.33 seconds
[0m13:26:09.369965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fbbe120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fbbef30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ab3e70>]}
[0m13:26:09.370082 [debug] [MainThread]: Flushing usage events
[0m13:26:09.929308 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:27:16.611993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103853620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10486b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10486bb10>]}


============================== 13:27:16.614430 | cfb35f2e-204f-4d41-bc13-b68ea9d62110 ==============================
[0m13:27:16.614430 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:27:16.614679 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'write_json': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select bronze_*', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'log_format': 'default', 'static_parser': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'version_check': 'True', 'empty': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'debug': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'no_print': 'None', 'cache_selected_only': 'False', 'indirect_selection': 'eager'}
[0m13:27:16.914230 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:27:16.914441 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:27:16.914548 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:27:17.347545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cfb35f2e-204f-4d41-bc13-b68ea9d62110', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b9b360>]}
[0m13:27:17.366914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cfb35f2e-204f-4d41-bc13-b68ea9d62110', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a76ad0>]}
[0m13:27:17.367169 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:27:17.415827 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:27:17.416160 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:27:17.416286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cfb35f2e-204f-4d41-bc13-b68ea9d62110', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3ab550>]}
[0m13:27:17.926874 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_flights_flights".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_flights", "flights").
  
  To fix this, change the name of one of these resources:
  - source.flights_dbt.raw_flights.flights (models/bronze/sources.yml)
  - source.flights_dbt.raw_flights.flights (models/sources_backup.yml)
[0m13:27:17.929638 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3484278, "process_in_blocks": "0", "process_kernel_time": 0.203903, "process_mem_max_rss": "231161856", "process_out_blocks": "0", "process_user_time": 1.443744}
[0m13:27:17.929872 [debug] [MainThread]: Command `dbt run` failed at 13:27:17.929834 after 1.35 seconds
[0m13:27:17.930083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b82a120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b82af30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10471fe70>]}
[0m13:27:17.930197 [debug] [MainThread]: Flushing usage events
[0m13:27:18.575085 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:27:31.874387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061b3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071cb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071cbb10>]}


============================== 13:27:31.876597 | 8d0092f1-e94a-4e78-829a-d9c832b43746 ==============================
[0m13:27:31.876597 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:27:31.876834 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'target_path': 'None', 'log_cache_events': 'False', 'printer_width': '80', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select bronze_*', 'warn_error': 'None', 'write_json': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'log_format': 'default', 'quiet': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'empty': 'False', 'use_colors': 'True', 'static_parser': 'True', 'debug': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs'}
[0m13:27:32.173980 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:27:32.174172 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:27:32.174271 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:27:32.590749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8d0092f1-e94a-4e78-829a-d9c832b43746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064fb360>]}
[0m13:27:32.610729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8d0092f1-e94a-4e78-829a-d9c832b43746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083d6ad0>]}
[0m13:27:32.610978 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:27:32.661133 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:27:32.661460 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:27:32.661591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8d0092f1-e94a-4e78-829a-d9c832b43746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd13550>]}
[0m13:27:33.232348 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.flights_dbt.bronze_airlines' (models/bronze/bronze_airlines.sql) depends on a source named 'raw_flights.airlines' which was not found
[0m13:27:33.235315 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.388696, "process_in_blocks": "0", "process_kernel_time": 0.208832, "process_mem_max_rss": "234422272", "process_out_blocks": "0", "process_user_time": 1.51349}
[0m13:27:33.235496 [debug] [MainThread]: Command `dbt run` failed at 13:27:33.235458 after 1.39 seconds
[0m13:27:33.235657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e19e6c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e554140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10707fe70>]}
[0m13:27:33.235770 [debug] [MainThread]: Flushing usage events
[0m13:27:33.869555 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:28:07.027123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10791b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108933890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108933b10>]}


============================== 13:28:07.029589 | 73f4297b-0515-493a-b22f-451ff56b1fc7 ==============================
[0m13:28:07.029589 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:28:07.029819 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'no_print': 'None', 'partial_parse': 'True', 'log_format': 'default', 'version_check': 'True', 'empty': 'False', 'invocation_command': 'dbt run --select bronze_*', 'warn_error': 'None', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/artakerqeli/.dbt', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'static_parser': 'True', 'write_json': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'debug': 'False', 'quiet': 'False', 'use_colors': 'True'}
[0m13:28:07.353563 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:28:07.353762 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:28:07.353863 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:28:07.768223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '73f4297b-0515-493a-b22f-451ff56b1fc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c63360>]}
[0m13:28:07.787867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '73f4297b-0515-493a-b22f-451ff56b1fc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b3aad0>]}
[0m13:28:07.788104 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:28:07.838443 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:28:07.838755 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:28:07.838876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '73f4297b-0515-493a-b22f-451ff56b1fc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d14f550>]}
[0m13:28:08.493647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '73f4297b-0515-493a-b22f-451ff56b1fc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082363f0>]}
[0m13:28:08.527742 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:28:08.528641 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:28:08.533524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '73f4297b-0515-493a-b22f-451ff56b1fc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d91eb30>]}
[0m13:28:08.533674 [info ] [MainThread]: Found 10 models, 4 data tests, 4 sources, 686 macros
[0m13:28:08.533771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '73f4297b-0515-493a-b22f-451ff56b1fc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d74aea0>]}
[0m13:28:08.534055 [warn ] [MainThread]: The selection criterion 'bronze_*' does not match any enabled nodes
[0m13:28:08.534453 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m13:28:08.534642 [debug] [MainThread]: Command end result
[0m13:28:08.546984 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:28:08.547642 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:28:08.548685 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m13:28:08.550708 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.5554061, "process_in_blocks": "0", "process_kernel_time": 0.219851, "process_mem_max_rss": "237748224", "process_out_blocks": "0", "process_user_time": 1.650452}
[0m13:28:08.550859 [debug] [MainThread]: Command `dbt run` succeeded at 13:28:08.550829 after 1.56 seconds
[0m13:28:08.550995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086eae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d7d49f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d7d4c00>]}
[0m13:28:08.551101 [debug] [MainThread]: Flushing usage events
[0m13:28:09.099322 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:29:00.843922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1039d3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bab890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104babb10>]}


============================== 13:29:00.846444 | fdfd7741-2bd6-446f-b5cb-d3df3a86bab9 ==============================
[0m13:29:00.846444 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:29:00.846687 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'log_format': 'default', 'empty': 'False', 'quiet': 'False', 'introspect': 'True', 'version_check': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'invocation_command': 'dbt run --models path:models/bronze', 'no_print': 'None', 'fail_fast': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'log_cache_events': 'False', 'write_json': 'True', 'static_parser': 'True'}
[0m13:29:00.846875 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m13:29:00.847006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ba6b10>]}
[0m13:29:01.150254 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:29:01.150449 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:29:01.150544 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:29:01.581360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105db6ad0>]}
[0m13:29:01.600638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd4fb50>]}
[0m13:29:01.600879 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:29:01.655722 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:29:01.715470 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:29:01.715633 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:29:01.733149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2f27b0>]}
[0m13:29:01.768677 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:29:01.769623 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:29:01.774599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4f7a10>]}
[0m13:29:01.774767 [info ] [MainThread]: Found 10 models, 4 data tests, 4 sources, 686 macros
[0m13:29:01.774887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e547790>]}
[0m13:29:01.775675 [info ] [MainThread]: 
[0m13:29:01.775794 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:29:01.775876 [info ] [MainThread]: 
[0m13:29:01.776069 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:29:01.776164 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:29:01.778552 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:29:01.778671 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:29:01.782683 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:29:01.782793 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:29:01.782876 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:29:02.550172 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-5ba7-1c5e-9d7a-79c6e283f4c6) - Created
[0m13:29:03.240924 [debug] [ThreadPool]: SQL status: OK in 1.460 seconds
[0m13:29:03.248348 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08982-5ba7-1c5e-9d7a-79c6e283f4c6, command-id=01f08982-5bcd-1236-9922-2a6ce61f833c) - Closing
[0m13:29:03.248817 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:29:03.249020 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-5ba7-1c5e-9d7a-79c6e283f4c6) - Closing
[0m13:29:03.445154 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m13:29:03.445870 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m13:29:03.446194 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m13:29:03.446643 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m13:29:03.446976 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m13:29:03.447324 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m13:29:03.449170 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m13:29:03.463181 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m13:29:03.464302 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m13:29:03.464522 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m13:29:03.466405 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m13:29:03.466666 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m13:29:03.466909 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m13:29:03.468136 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m13:29:03.468350 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m13:29:03.468544 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:29:03.468695 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:29:03.468888 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m13:29:03.469058 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:29:03.469498 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:29:04.164269 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-5ca2-12aa-81e7-23980d6cf572) - Created
[0m13:29:04.217545 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-5caa-1025-b4fd-726bdf1f4c19) - Created
[0m13:29:04.220275 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-5ca7-1e09-966a-cfac7730f283) - Created
[0m13:29:04.236214 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-5cac-1978-a367-6c15400cdbd1) - Created
[0m13:29:04.756989 [debug] [ThreadPool]: SQL status: OK in 1.290 seconds
[0m13:29:04.761266 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08982-5ca2-12aa-81e7-23980d6cf572, command-id=01f08982-5cbf-1ef3-889c-6b9d8cce913a) - Closing
[0m13:29:04.761975 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m13:29:04.762246 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-5ca2-12aa-81e7-23980d6cf572) - Closing
[0m13:29:04.879303 [debug] [ThreadPool]: SQL status: OK in 1.410 seconds
[0m13:29:04.881448 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08982-5caa-1025-b4fd-726bdf1f4c19, command-id=01f08982-5cc4-13c4-a768-eaca622ab3c0) - Closing
[0m13:29:04.944094 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m13:29:04.944984 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-5caa-1025-b4fd-726bdf1f4c19) - Closing
[0m13:29:04.949306 [debug] [ThreadPool]: SQL status: OK in 1.480 seconds
[0m13:29:04.951800 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08982-5cac-1978-a367-6c15400cdbd1, command-id=01f08982-5cc8-1a5f-9d71-ea752aa7eb92) - Closing
[0m13:29:05.014490 [debug] [ThreadPool]: SQL status: OK in 1.540 seconds
[0m13:29:05.017619 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08982-5ca7-1e09-966a-cfac7730f283, command-id=01f08982-5cc5-12a8-ab20-fe86d2609ef2) - Closing
[0m13:29:05.141870 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m13:29:05.143555 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-5cac-1978-a367-6c15400cdbd1) - Closing
[0m13:29:05.338632 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m13:29:05.339774 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-5ca7-1e09-966a-cfac7730f283) - Closing
[0m13:29:05.543555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5c1190>]}
[0m13:29:05.548660 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m13:29:05.549098 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m13:29:05.549417 [debug] [Thread-4 (]: Began running node model.flights_dbt.bronze_weather
[0m13:29:05.549699 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m13:29:05.550304 [info ] [Thread-1 (]: 1 of 4 START sql view model flights_dbt_bronze.bronze_airlines ................. [RUN]
[0m13:29:05.550756 [info ] [Thread-2 (]: 2 of 4 START sql view model flights_dbt_bronze.bronze_airports ................. [RUN]
[0m13:29:05.551165 [info ] [Thread-4 (]: 4 of 4 START sql table model flights_dbt_bronze.bronze_weather ................. [RUN]
[0m13:29:05.551584 [info ] [Thread-3 (]: 3 of 4 START sql view model flights_dbt_bronze.bronze_flights .................. [RUN]
[0m13:29:05.552293 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m13:29:05.552766 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m13:29:05.553149 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_weather) - Creating connection
[0m13:29:05.553525 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m13:29:05.553808 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m13:29:05.554077 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m13:29:05.554308 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_weather'
[0m13:29:05.554539 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m13:29:05.554800 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m13:29:05.555042 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m13:29:05.555270 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.bronze_weather
[0m13:29:05.555497 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m13:29:05.563251 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m13:29:05.566131 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m13:29:05.568086 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.bronze_weather"
[0m13:29:05.570040 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m13:29:05.571129 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m13:29:05.571421 [debug] [Thread-4 (]: Began executing node model.flights_dbt.bronze_weather
[0m13:29:05.571620 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m13:29:05.571800 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m13:29:05.581249 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:29:05.588535 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m13:29:05.591430 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m13:29:05.592371 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m13:29:05.593456 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:29:05.593694 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:29:05.593910 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:29:05.594438 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e44e830>]}
[0m13:29:05.594180 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:29:05.594621 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd44050>]}
[0m13:29:05.594747 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd453b0>]}
[0m13:29:05.601620 [debug] [Thread-3 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
[0m13:29:05.601757 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3de250>]}
[0m13:29:05.603838 [debug] [Thread-1 (]: Dropping relation `flight_db`.`flights_dbt_bronze`.`bronze_airlines` because it is of type table
[0m13:29:05.613820 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m13:29:05.620575 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.bronze_weather"
[0m13:29:05.623014 [debug] [Thread-1 (]: Applying DROP to: `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
[0m13:29:05.623301 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
[0m13:29:05.625068 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m13:29:05.625361 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m13:29:05.625505 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
drop table if exists `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
[0m13:29:05.625673 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:29:05.626034 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m13:29:05.626139 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m13:29:05.626296 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select 
    airport_id,
    iata_code,
    airport_name,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m13:29:05.626400 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.bronze_weather"
[0m13:29:05.626533 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
  
  as (
    select
    flight_id,
    airline,
    origin,
    dep_time,
    arr_time,
    arr_delay
from `flight_db`.`raw`.`flights`
  )

[0m13:29:05.626646 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:29:05.626761 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_weather"} */

  
    
        create or replace table `flight_db`.`flights_dbt_bronze`.`bronze_weather`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select *
from `flight_db`.`raw`.`weather`
  
[0m13:29:05.626865 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:29:05.627024 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:29:06.340718 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08982-5dee-1909-9380-0864ab76b011) - Created
[0m13:29:06.346106 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08982-5ded-1f7b-bd38-46575e331da6) - Created
[0m13:29:06.349100 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08982-5dec-1c76-adb3-154575a7f43e) - Created
[0m13:29:06.404939 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f08982-5df4-1a04-a88a-9a2ea7878791) - Created
[0m13:29:06.701717 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select 
    airport_id,
    iata_code,
    airport_name,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 11 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08982-5e09-1607-83b5-1e5206362431
[0m13:29:06.703250 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m13:29:06.703614 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08982-5ded-1f7b-bd38-46575e331da6) - Closing
[0m13:29:06.847153 [debug] [Thread-1 (]: SQL status: OK in 1.220 seconds
[0m13:29:06.849361 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f08982-5dec-1c76-adb3-154575a7f43e, command-id=01f08982-5e0a-1923-b440-31d66239fdb9) - Closing
[0m13:29:06.850248 [debug] [Thread-1 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
[0m13:29:06.850982 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m13:29:06.894837 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m13:29:06.895572 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
  as (
    { config(materialized='table') }}

select *
from `flight_db`.`raw`.`airlines`
  )

[0m13:29:06.906439 [debug] [Thread-2 (]: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 11 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m13:29:06.908740 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd958b0>]}
[0m13:29:06.909385 [error] [Thread-2 (]: 2 of 4 ERROR creating sql view model flights_dbt_bronze.bronze_airports ........ [[31mERROR[0m in 1.35s]
[0m13:29:06.909892 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m13:29:06.910401 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airports' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 11 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql.
[0m13:29:07.024379 [debug] [Thread-3 (]: SQL status: OK in 1.400 seconds
[0m13:29:07.026817 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08982-5dee-1909-9380-0864ab76b011, command-id=01f08982-5e08-12f2-9154-64449b2a7aa2) - Closing
[0m13:29:07.036016 [debug] [Thread-3 (]: Applying tags to relation None
[0m13:29:07.038687 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m13:29:07.038908 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08982-5dee-1909-9380-0864ab76b011) - Closing
[0m13:29:07.242875 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e573470>]}
[0m13:29:07.244592 [info ] [Thread-3 (]: 3 of 4 OK created sql view model flights_dbt_bronze.bronze_flights ............. [[32mOK[0m in 1.69s]
[0m13:29:07.245360 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m13:29:07.248131 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
  as (
    { config(materialized='table') }}

select *
from `flight_db`.`raw`.`airlines`
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
  as (
    { config(materialized='table') }}
----^^^

select *
from `flight_db`.`raw`.`airlines`
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
  as (
    { config(materialized='table') }}
----^^^

select *
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
  as (
    { config(materialized='table') }}
----^^^

select *
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f08982-5e5d-15c4-9e3b-215ce0d6e15d
[0m13:29:07.249328 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m13:29:07.249662 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08982-5dec-1c76-adb3-154575a7f43e) - Closing
[0m13:29:07.450236 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
    
    as (
      { config(materialized='table') }}
  ----^^^
  
  select *
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m13:29:07.451184 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10df0a4b0>]}
[0m13:29:07.452048 [error] [Thread-1 (]: 1 of 4 ERROR creating sql view model flights_dbt_bronze.bronze_airlines ........ [[31mERROR[0m in 1.90s]
[0m13:29:07.452605 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m13:29:07.453135 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
    
    as (
      { config(materialized='table') }}
  ----^^^
  
  select *
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m13:29:08.149624 [debug] [Thread-4 (]: SQL status: OK in 2.520 seconds
[0m13:29:08.151056 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f08982-5df4-1a04-a88a-9a2ea7878791, command-id=01f08982-5e13-1328-82db-a75e1f009882) - Closing
[0m13:29:08.154252 [debug] [Thread-4 (]: Applying tags to relation None
[0m13:29:08.165677 [debug] [Thread-4 (]: On model.flights_dbt.bronze_weather: Close
[0m13:29:08.165892 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f08982-5df4-1a04-a88a-9a2ea7878791) - Closing
[0m13:29:08.360377 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fdfd7741-2bd6-446f-b5cb-d3df3a86bab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5cab10>]}
[0m13:29:08.362160 [info ] [Thread-4 (]: 4 of 4 OK created sql table model flights_dbt_bronze.bronze_weather ............ [[32mOK[0m in 2.81s]
[0m13:29:08.363027 [debug] [Thread-4 (]: Finished running node model.flights_dbt.bronze_weather
[0m13:29:08.364820 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:29:08.365160 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:29:08.365653 [info ] [MainThread]: 
[0m13:29:08.365952 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 6.59 seconds (6.59s).
[0m13:29:08.367069 [debug] [MainThread]: Command end result
[0m13:29:08.408761 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:29:08.409699 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:29:08.412717 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m13:29:08.412845 [info ] [MainThread]: 
[0m13:29:08.412988 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m13:29:08.413097 [info ] [MainThread]: 
[0m13:29:08.413244 [error] [MainThread]: [31mFailure in model bronze_airports (models/bronze/bronze_airports.sql)[0m
[0m13:29:08.413381 [error] [MainThread]:   Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `airport_name` cannot be resolved. Did you mean one of the following? [`airport_id`, `iata_code`, `state`, `city`, `country`]. SQLSTATE: 42703; line 11 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m13:29:08.413484 [info ] [MainThread]: 
[0m13:29:08.413599 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airports.sql
[0m13:29:08.413697 [info ] [MainThread]: 
[0m13:29:08.413820 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m13:29:08.413953 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
    
    as (
      { config(materialized='table') }}
  ----^^^
  
  select *
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m13:29:08.414059 [info ] [MainThread]: 
[0m13:29:08.414177 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m13:29:08.414270 [info ] [MainThread]: 
[0m13:29:08.414391 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=4
[0m13:29:08.414617 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:29:08.417597 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.607183, "process_in_blocks": "0", "process_kernel_time": 0.264439, "process_mem_max_rss": "249528320", "process_out_blocks": "0", "process_user_time": 2.378173}
[0m13:29:08.417842 [debug] [MainThread]: Command `dbt run` failed at 13:29:08.417797 after 7.61 seconds
[0m13:29:08.418029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3d64b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3d5970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3d5c70>]}
[0m13:29:08.418174 [debug] [MainThread]: Flushing usage events
[0m13:29:09.177950 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:31:47.734843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10412b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057fbb10>]}


============================== 13:31:47.737087 | acba9206-4bad-4baa-b440-a68bc9f2ab12 ==============================
[0m13:31:47.737087 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:31:47.737348 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'warn_error': 'None', 'quiet': 'False', 'static_parser': 'True', 'debug': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'introspect': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'no_print': 'None', 'use_colors': 'True', 'partial_parse': 'True', 'log_format': 'default', 'version_check': 'True', 'empty': 'False', 'write_json': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'fail_fast': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'invocation_command': 'dbt run --models path:models/bronze', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m13:31:47.737536 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m13:31:47.737662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'acba9206-4bad-4baa-b440-a68bc9f2ab12', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057f6b10>]}
[0m13:31:48.029824 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:31:48.030004 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:31:48.030096 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:31:48.464845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'acba9206-4bad-4baa-b440-a68bc9f2ab12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107902ad0>]}
[0m13:31:48.483800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'acba9206-4bad-4baa-b440-a68bc9f2ab12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11664bb50>]}
[0m13:31:48.484032 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:31:48.540272 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:31:48.595983 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m13:31:48.596387 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/sources.yml
[0m13:31:48.596495 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m13:31:48.596593 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m13:31:48.596678 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:31:48.700050 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.flights_dbt.bronze_weather' (models/bronze/bronze_weather.sql) depends on a source named 'raw_flights.weather' which was not found
[0m13:31:48.700357 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:31:48.702396 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0004498, "process_in_blocks": "0", "process_kernel_time": 0.222583, "process_mem_max_rss": "228311040", "process_out_blocks": "0", "process_user_time": 1.07631}
[0m13:31:48.702555 [debug] [MainThread]: Command `dbt run` failed at 13:31:48.702522 after 1.00 seconds
[0m13:31:48.702715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116af3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e29f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056afe70>]}
[0m13:31:48.702827 [debug] [MainThread]: Flushing usage events
[0m13:31:49.343027 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:32:06.448043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108757620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10992f890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10992fb10>]}


============================== 13:32:06.450260 | 4a93336e-0ff5-4e13-9b27-d6282bd21d5a ==============================
[0m13:32:06.450260 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:32:06.450515 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'invocation_command': 'dbt run --models path:models/bronze', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'fail_fast': 'False', 'version_check': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'use_colors': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'write_json': 'True', 'target_path': 'None', 'empty': 'False', 'static_parser': 'True', 'debug': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'warn_error': 'None', 'cache_selected_only': 'False', 'no_print': 'None', 'partial_parse': 'True'}
[0m13:32:06.450712 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m13:32:06.450838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '4a93336e-0ff5-4e13-9b27-d6282bd21d5a', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10992ab10>]}
[0m13:32:06.777458 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:32:06.777672 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:32:06.777775 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:32:07.197528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4a93336e-0ff5-4e13-9b27-d6282bd21d5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab36ad0>]}
[0m13:32:07.217231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4a93336e-0ff5-4e13-9b27-d6282bd21d5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102f7b50>]}
[0m13:32:07.217506 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:32:07.268575 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:32:07.325402 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 4 files changed.
[0m13:32:07.325630 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/bronze_weather_backup.sql
[0m13:32:07.325898 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/sources.yml
[0m13:32:07.325984 [debug] [MainThread]: Partial parsing: deleted file: flights_dbt://models/bronze/bronze_weather.sql
[0m13:32:07.326075 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m13:32:07.326166 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m13:32:07.326250 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:32:07.433106 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.flights_dbt.bronze_weather_backup' (models/bronze/bronze_weather_backup.sql) depends on a source named 'raw_flights.weather' which was not found
[0m13:32:07.433416 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:32:07.435893 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0154735, "process_in_blocks": "0", "process_kernel_time": 0.20761, "process_mem_max_rss": "231112704", "process_out_blocks": "0", "process_user_time": 1.136265}
[0m13:32:07.436070 [debug] [MainThread]: Command `dbt run` failed at 13:32:07.436036 after 1.02 seconds
[0m13:32:07.436232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11069b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109d1f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1097e3e70>]}
[0m13:32:07.436347 [debug] [MainThread]: Flushing usage events
[0m13:32:08.119727 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:32:30.952811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10440f620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105427890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105427b10>]}


============================== 13:32:30.955116 | cf5b4c6a-2c34-4bd0-80d7-9bd15998b1ed ==============================
[0m13:32:30.955116 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:32:30.955357 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'empty': 'False', 'use_colors': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'introspect': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'no_print': 'None', 'version_check': 'True', 'indirect_selection': 'eager', 'quiet': 'False', 'invocation_command': 'dbt run --select models/bronze/bronze_airlines.sql models/bronze/bronze_airports.sql models/bronze/bronze_flights.sql models/bronze/bronze_weather_backup.sql', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'log_format': 'default', 'debug': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'cache_selected_only': 'False', 'fail_fast': 'False', 'static_parser': 'True', 'printer_width': '80'}
[0m13:32:31.247569 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:32:31.247792 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:32:31.247908 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:32:31.674485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cf5b4c6a-2c34-4bd0-80d7-9bd15998b1ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104757360>]}
[0m13:32:31.694271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cf5b4c6a-2c34-4bd0-80d7-9bd15998b1ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106632ad0>]}
[0m13:32:31.694520 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:32:31.745561 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:32:31.801594 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 4 files changed.
[0m13:32:31.801808 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/bronze/bronze_weather_backup.sql
[0m13:32:31.802084 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/sources.yml
[0m13:32:31.802169 [debug] [MainThread]: Partial parsing: deleted file: flights_dbt://models/bronze/bronze_weather.sql
[0m13:32:31.802257 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m13:32:31.802339 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:32:31.802416 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m13:32:31.907823 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.flights_dbt.bronze_weather_backup' (models/bronze/bronze_weather_backup.sql) depends on a source named 'raw_flights.weather' which was not found
[0m13:32:31.910421 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9870109, "process_in_blocks": "0", "process_kernel_time": 0.212225, "process_mem_max_rss": "232062976", "process_out_blocks": "0", "process_user_time": 1.113243}
[0m13:32:31.910601 [debug] [MainThread]: Command `dbt run` failed at 13:32:31.910567 after 0.99 seconds
[0m13:32:31.910767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c32fc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c32f850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c30f890>]}
[0m13:32:31.910885 [debug] [MainThread]: Flushing usage events
[0m13:32:32.582376 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:32:52.203930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10636f620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107387890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107387b10>]}


============================== 13:32:52.206130 | 191bba8b-1b33-4c47-907c-a10d241826ba ==============================
[0m13:32:52.206130 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:32:52.206372 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'partial_parse': 'True', 'warn_error': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'quiet': 'False', 'debug': 'False', 'empty': 'False', 'indirect_selection': 'eager', 'introspect': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --select models/bronze/ --exclude models/bronze/bronze_weather_backup.sql', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'printer_width': '80', 'no_print': 'None', 'version_check': 'True', 'use_colors': 'True', 'target_path': 'None'}
[0m13:32:52.500002 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:32:52.500202 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:32:52.500304 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:32:52.906763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066b7360>]}
[0m13:32:52.926331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10858aad0>]}
[0m13:32:52.926560 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:32:52.982015 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:32:53.038214 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 4 files changed.
[0m13:32:53.038621 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/sources.yml
[0m13:32:53.038732 [debug] [MainThread]: Partial parsing: deleted file: flights_dbt://models/bronze/bronze_weather.sql
[0m13:32:53.038842 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airports.sql
[0m13:32:53.038939 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m13:32:53.039024 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:32:53.171755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e26b650>]}
[0m13:32:53.205431 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:32:53.206389 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:32:53.211740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e242210>]}
[0m13:32:53.211894 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m13:32:53.211995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e530bb0>]}
[0m13:32:53.212563 [warn ] [MainThread]: The selection criterion 'models/bronze/bronze_weather_backup.sql' does not match any enabled nodes
[0m13:32:53.213062 [info ] [MainThread]: 
[0m13:32:53.213179 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:32:53.213260 [info ] [MainThread]: 
[0m13:32:53.213449 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:32:53.213556 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:32:53.215911 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:32:53.216027 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:32:53.219497 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:32:53.219613 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:32:53.219705 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:32:54.017468 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-e5a1-12f6-b57f-a24a9b2ec9ae) - Created
[0m13:32:54.448256 [debug] [ThreadPool]: SQL status: OK in 1.230 seconds
[0m13:32:54.455142 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08982-e5a1-12f6-b57f-a24a9b2ec9ae, command-id=01f08982-e5c3-1e62-b42d-635b71726568) - Closing
[0m13:32:54.455586 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:32:54.455774 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-e5a1-12f6-b57f-a24a9b2ec9ae) - Closing
[0m13:32:54.655804 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m13:32:54.656785 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m13:32:54.657218 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m13:32:54.657824 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m13:32:54.658592 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m13:32:54.658879 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m13:32:54.661549 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m13:32:54.668383 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m13:32:54.674267 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m13:32:54.676077 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m13:32:54.677768 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m13:32:54.679029 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m13:32:54.679272 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m13:32:54.679510 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m13:32:54.679731 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m13:32:54.679946 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m13:32:54.680126 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:32:54.680268 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:32:54.680403 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:32:54.680542 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:32:55.398387 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-e678-189a-81be-dd5d9faf4276) - Created
[0m13:32:55.440536 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-e67c-1b46-9cff-a1dba1e55a3c) - Created
[0m13:32:55.462002 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-e680-146d-a1e7-e2438167863d) - Created
[0m13:32:55.464670 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-e682-11be-b2ab-4777fac11db0) - Created
[0m13:32:55.899649 [debug] [ThreadPool]: SQL status: OK in 1.220 seconds
[0m13:32:55.903002 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08982-e67c-1b46-9cff-a1dba1e55a3c, command-id=01f08982-e698-1b17-9d37-94959982c939) - Closing
[0m13:32:55.903657 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m13:32:55.903949 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-e67c-1b46-9cff-a1dba1e55a3c) - Closing
[0m13:32:55.974789 [debug] [ThreadPool]: SQL status: OK in 1.290 seconds
[0m13:32:55.976845 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08982-e678-189a-81be-dd5d9faf4276, command-id=01f08982-e692-11b3-b079-304006e00bc0) - Closing
[0m13:32:56.002629 [debug] [ThreadPool]: SQL status: OK in 1.320 seconds
[0m13:32:56.004095 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08982-e680-146d-a1e7-e2438167863d, command-id=01f08982-e6a3-1987-b46e-b25daa230e6a) - Closing
[0m13:32:56.077680 [debug] [ThreadPool]: SQL status: OK in 1.400 seconds
[0m13:32:56.080212 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08982-e682-11be-b2ab-4777fac11db0, command-id=01f08982-e69e-12b7-90b8-3be6246e11af) - Closing
[0m13:32:56.106218 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m13:32:56.106989 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-e678-189a-81be-dd5d9faf4276) - Closing
[0m13:32:56.304495 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m13:32:56.305914 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-e680-146d-a1e7-e2438167863d) - Closing
[0m13:32:56.508745 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m13:32:56.510144 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08982-e682-11be-b2ab-4777fac11db0) - Closing
[0m13:32:56.716808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e675d90>]}
[0m13:32:56.721241 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m13:32:56.721690 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m13:32:56.722018 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m13:32:56.722585 [info ] [Thread-2 (]: 2 of 3 START sql view model flights_dbt_bronze.bronze_airports ................. [RUN]
[0m13:32:56.723112 [info ] [Thread-3 (]: 3 of 3 START sql view model flights_dbt_bronze.bronze_flights .................. [RUN]
[0m13:32:56.723625 [info ] [Thread-1 (]: 1 of 3 START sql view model flights_dbt_bronze.bronze_airlines ................. [RUN]
[0m13:32:56.724241 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m13:32:56.724721 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m13:32:56.725208 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m13:32:56.725600 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m13:32:56.725915 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m13:32:56.726231 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m13:32:56.726601 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m13:32:56.726920 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m13:32:56.727230 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m13:32:56.742628 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m13:32:56.744787 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m13:32:56.746925 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m13:32:56.747869 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m13:32:56.748086 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m13:32:56.748280 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m13:32:56.757553 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:32:56.758643 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m13:32:56.759583 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m13:32:56.760645 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:32:56.760910 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:32:56.761132 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:32:56.761338 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7ee690>]}
[0m13:32:56.761491 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e77be30>]}
[0m13:32:56.761629 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e860050>]}
[0m13:32:56.769536 [debug] [Thread-2 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
[0m13:32:56.770410 [debug] [Thread-3 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
[0m13:32:56.770795 [debug] [Thread-1 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
[0m13:32:56.774886 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m13:32:56.775221 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m13:32:56.775521 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m13:32:56.775834 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m13:32:56.775956 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m13:32:56.776101 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
  
  as (
    select
    flight_id,
    airline,
    origin,
    dest,
    dep_time,
    arr_time,
    arr_delay
from `flight_db`.`raw`.`flights`
  )

[0m13:32:56.776266 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m13:32:56.776398 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m13:32:56.776514 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:32:56.776627 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:32:56.776757 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
  as (
    select
    iata_code as airline_code,
    name as airline_name
from `flight_db`.`raw`.`airlines`
  )

[0m13:32:56.777023 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:32:57.431481 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08982-e7ae-1144-8212-db8d081d2644) - Created
[0m13:32:57.452456 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08982-e7af-1e4f-810c-67c4de457bea) - Created
[0m13:32:57.502330 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08982-e7b6-1cb4-9b6f-dc3fdb1165e9) - Created
[0m13:32:57.832293 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
  as (
    select
    iata_code as airline_code,
    name as airline_name
from `flight_db`.`raw`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 11 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 11 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 11 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08982-e7c8-1089-b4c5-61b608d952d4
[0m13:32:57.833976 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m13:32:57.834302 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08982-e7ae-1144-8212-db8d081d2644) - Closing
[0m13:32:57.904314 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
  
  as (
    select
    flight_id,
    airline,
    origin,
    dest,
    dep_time,
    arr_time,
    arr_delay
from `flight_db`.`raw`.`flights`
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 12 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08982-e7cb-1bc2-b464-f46d0e049195
[0m13:32:58.031321 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m13:32:58.032286 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08982-e7af-1e4f-810c-67c4de457bea) - Closing
[0m13:32:58.043932 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 11 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m13:32:58.178597 [debug] [Thread-2 (]: SQL status: OK in 1.400 seconds
[0m13:32:58.180324 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08982-e7b6-1cb4-9b6f-dc3fdb1165e9, command-id=01f08982-e7d4-14d0-be3d-880db63183fd) - Closing
[0m13:32:58.193363 [debug] [Thread-2 (]: Applying tags to relation None
[0m13:32:58.249065 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m13:32:58.250560 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08982-e7b6-1cb4-9b6f-dc3fdb1165e9) - Closing
[0m13:32:58.256990 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106679ed0>]}
[0m13:32:58.258113 [debug] [Thread-3 (]: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m13:32:58.258949 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model flights_dbt_bronze.bronze_airlines ........ [[31mERROR[0m in 1.52s]
[0m13:32:58.259591 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m13:32:58.260125 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 11 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m13:32:58.448990 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e659c50>]}
[0m13:32:58.449988 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '191bba8b-1b33-4c47-907c-a10d241826ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e68d780>]}
[0m13:32:58.451050 [error] [Thread-3 (]: 3 of 3 ERROR creating sql view model flights_dbt_bronze.bronze_flights ......... [[31mERROR[0m in 1.72s]
[0m13:32:58.451711 [info ] [Thread-2 (]: 2 of 3 OK created sql view model flights_dbt_bronze.bronze_airports ............ [[32mOK[0m in 1.73s]
[0m13:32:58.452513 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m13:32:58.452996 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m13:32:58.453471 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_flights' to be skipped because of status 'error'.  Reason: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql.
[0m13:32:58.455265 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:32:58.455586 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:32:58.456076 [info ] [MainThread]: 
[0m13:32:58.456391 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 5.24 seconds (5.24s).
[0m13:32:58.457340 [debug] [MainThread]: Command end result
[0m13:32:58.491122 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:32:58.492130 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:32:58.495544 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m13:32:58.495688 [info ] [MainThread]: 
[0m13:32:58.495846 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m13:32:58.495969 [info ] [MainThread]: 
[0m13:32:58.496128 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m13:32:58.496289 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 11 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m13:32:58.496409 [info ] [MainThread]: 
[0m13:32:58.496544 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m13:32:58.496651 [info ] [MainThread]: 
[0m13:32:58.496791 [error] [MainThread]: [31mFailure in model bronze_flights (models/bronze/bronze_flights.sql)[0m
[0m13:32:58.496932 [error] [MainThread]:   Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `dest` cannot be resolved. Did you mean one of the following? [`dep_time`, `origin`, `airline`, `arr_delay`, `arr_time`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m13:32:58.497038 [info ] [MainThread]: 
[0m13:32:58.497166 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_flights.sql
[0m13:32:58.497264 [info ] [MainThread]: 
[0m13:32:58.497394 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=3
[0m13:32:58.499964 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 6.324051, "process_in_blocks": "0", "process_kernel_time": 0.253838, "process_mem_max_rss": "246415360", "process_out_blocks": "0", "process_user_time": 2.270712}
[0m13:32:58.500159 [debug] [MainThread]: Command `dbt run` failed at 13:32:58.500118 after 6.32 seconds
[0m13:32:58.500324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10663ae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8d6ab0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dfe47d0>]}
[0m13:32:58.500467 [debug] [MainThread]: Flushing usage events
[0m13:32:59.151423 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:33:42.414736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103887620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10489f890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10489fb10>]}


============================== 13:33:42.416951 | 359bccd5-28b0-4a33-a2f6-bd44f4c58f83 ==============================
[0m13:33:42.416951 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:33:42.417186 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'log_cache_events': 'False', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'write_json': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'no_print': 'None', 'target_path': 'None', 'fail_fast': 'False', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --select models/bronze/bronze_flights.sql', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'version_check': 'True', 'static_parser': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error': 'None', 'use_colors': 'True', 'debug': 'False', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt'}
[0m13:33:42.712837 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:33:42.713035 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:33:42.713129 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:33:43.158566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '359bccd5-28b0-4a33-a2f6-bd44f4c58f83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bcf360>]}
[0m13:33:43.178093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '359bccd5-28b0-4a33-a2f6-bd44f4c58f83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105aaaad0>]}
[0m13:33:43.178369 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:33:43.226897 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:33:43.287384 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:33:43.287654 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_flights.sql
[0m13:33:43.380666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '359bccd5-28b0-4a33-a2f6-bd44f4c58f83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d412950>]}
[0m13:33:43.413561 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:33:43.414602 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:33:43.420145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '359bccd5-28b0-4a33-a2f6-bd44f4c58f83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d3ee030>]}
[0m13:33:43.420297 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m13:33:43.420403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '359bccd5-28b0-4a33-a2f6-bd44f4c58f83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e071fd0>]}
[0m13:33:43.421116 [info ] [MainThread]: 
[0m13:33:43.421235 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:33:43.421315 [info ] [MainThread]: 
[0m13:33:43.421493 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:33:43.421584 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:33:43.421875 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:33:43.421985 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:33:43.425323 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:33:43.425433 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:33:43.425520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:44.074921 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-037b-1e14-a4a0-fb01e5606c6e) - Created
[0m13:33:44.520061 [debug] [ThreadPool]: SQL status: OK in 1.090 seconds
[0m13:33:44.529582 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08983-037b-1e14-a4a0-fb01e5606c6e, command-id=01f08983-0395-16e7-a7d6-5dd7c0d6d3b7) - Closing
[0m13:33:44.530182 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:33:44.530418 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-037b-1e14-a4a0-fb01e5606c6e) - Closing
[0m13:33:44.730038 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m13:33:44.731480 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m13:33:44.731955 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m13:33:44.732824 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m13:33:44.733156 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m13:33:44.733771 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m13:33:44.736308 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m13:33:44.745607 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m13:33:44.746520 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m13:33:44.746724 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m13:33:44.748162 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m13:33:44.748392 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m13:33:44.748616 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m13:33:44.749737 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m13:33:44.749921 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m13:33:44.750092 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:44.750220 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:44.750386 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m13:33:44.750527 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:44.750957 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:45.565810 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-045f-175d-9259-9d22894b78d1) - Created
[0m13:33:45.569842 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-045f-17ba-aa35-10541bb73aaa) - Created
[0m13:33:45.577837 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-0461-113c-90bf-177f9506ab71) - Created
[0m13:33:45.602633 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-0464-18ff-8d23-284a825a4735) - Created
[0m13:33:46.049105 [debug] [ThreadPool]: SQL status: OK in 1.300 seconds
[0m13:33:46.052643 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08983-045f-175d-9259-9d22894b78d1, command-id=01f08983-0479-1098-89ba-802106b2f378) - Closing
[0m13:33:46.053526 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m13:33:46.053848 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-045f-175d-9259-9d22894b78d1) - Closing
[0m13:33:46.074303 [debug] [ThreadPool]: SQL status: OK in 1.320 seconds
[0m13:33:46.076247 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08983-0461-113c-90bf-177f9506ab71, command-id=01f08983-047c-15ae-99a9-2377ca7c49a1) - Closing
[0m13:33:46.109048 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m13:33:46.111630 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08983-045f-17ba-aa35-10541bb73aaa, command-id=01f08983-0479-1672-b9db-26b431903574) - Closing
[0m13:33:46.191778 [debug] [ThreadPool]: SQL status: OK in 1.440 seconds
[0m13:33:46.194640 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08983-0464-18ff-8d23-284a825a4735, command-id=01f08983-0480-186f-a9cb-6932e40da2a5) - Closing
[0m13:33:46.248391 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m13:33:46.249919 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-0461-113c-90bf-177f9506ab71) - Closing
[0m13:33:46.441676 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m13:33:46.442658 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-045f-17ba-aa35-10541bb73aaa) - Closing
[0m13:33:46.648893 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m13:33:46.650033 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-0464-18ff-8d23-284a825a4735) - Closing
[0m13:33:46.848161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '359bccd5-28b0-4a33-a2f6-bd44f4c58f83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6f6000>]}
[0m13:33:46.853220 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_flights
[0m13:33:46.853996 [info ] [Thread-1 (]: 1 of 1 START sql view model flights_dbt_bronze.bronze_flights .................. [RUN]
[0m13:33:46.854623 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m13:33:46.854977 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m13:33:46.855293 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_flights
[0m13:33:46.864469 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m13:33:46.865691 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_flights
[0m13:33:46.878759 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m13:33:46.880192 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:33:46.880513 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '359bccd5-28b0-4a33-a2f6-bd44f4c58f83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e07e2d0>]}
[0m13:33:46.888820 [debug] [Thread-1 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
[0m13:33:46.893318 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m13:33:46.893895 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m13:33:46.894092 [debug] [Thread-1 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m13:33:46.894243 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:33:47.562755 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08983-058e-1f6d-9dbb-6b7f0a30dc9e) - Created
[0m13:33:48.252291 [debug] [Thread-1 (]: SQL status: OK in 1.360 seconds
[0m13:33:48.254421 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f08983-058e-1f6d-9dbb-6b7f0a30dc9e, command-id=01f08983-05ab-1061-9c54-c4e242778fbd) - Closing
[0m13:33:48.264659 [debug] [Thread-1 (]: Applying tags to relation None
[0m13:33:48.267641 [debug] [Thread-1 (]: On model.flights_dbt.bronze_flights: Close
[0m13:33:48.267900 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08983-058e-1f6d-9dbb-6b7f0a30dc9e) - Closing
[0m13:33:48.474420 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '359bccd5-28b0-4a33-a2f6-bd44f4c58f83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d362b0>]}
[0m13:33:48.475455 [info ] [Thread-1 (]: 1 of 1 OK created sql view model flights_dbt_bronze.bronze_flights ............. [[32mOK[0m in 1.62s]
[0m13:33:48.475990 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_flights
[0m13:33:48.477532 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:33:48.477841 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:33:48.478225 [info ] [MainThread]: 
[0m13:33:48.478560 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 5.06 seconds (5.06s).
[0m13:33:48.479149 [debug] [MainThread]: Command end result
[0m13:33:48.503721 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:33:48.504682 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:33:48.508035 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m13:33:48.508201 [info ] [MainThread]: 
[0m13:33:48.508378 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:33:48.508504 [info ] [MainThread]: 
[0m13:33:48.508643 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m13:33:48.511713 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.1259556, "process_in_blocks": "0", "process_kernel_time": 0.25678, "process_mem_max_rss": "247169024", "process_out_blocks": "0", "process_user_time": 2.054727}
[0m13:33:48.511920 [debug] [MainThread]: Command `dbt run` succeeded at 13:33:48.511874 after 6.13 seconds
[0m13:33:48.512082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e11a850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e03ff50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cfe3a50>]}
[0m13:33:48.512229 [debug] [MainThread]: Flushing usage events
[0m13:33:49.131606 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:34:39.213912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fb3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10518b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10518bb10>]}


============================== 13:34:39.216153 | 66e83614-dc9c-449f-97cf-8fd489fb9d93 ==============================
[0m13:34:39.216153 [info ] [MainThread]: Running with dbt=1.10.9
[0m13:34:39.216378 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'introspect': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'indirect_selection': 'eager', 'static_parser': 'True', 'log_cache_events': 'False', 'printer_width': '80', 'profiles_dir': '/Users/artakerqeli/.dbt', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'invocation_command': 'dbt run --select models/bronze/bronze_airlines.sql', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'warn_error': 'None', 'quiet': 'False', 'debug': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'version_check': 'True', 'empty': 'False', 'log_format': 'default', 'fail_fast': 'False'}
[0m13:34:39.512163 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:34:39.512356 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:34:39.512456 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:34:39.946474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '66e83614-dc9c-449f-97cf-8fd489fb9d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044b7360>]}
[0m13:34:39.965579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '66e83614-dc9c-449f-97cf-8fd489fb9d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106392ad0>]}
[0m13:34:39.965807 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m13:34:40.015076 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m13:34:40.074189 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:34:40.074438 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m13:34:40.167451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '66e83614-dc9c-449f-97cf-8fd489fb9d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf16950>]}
[0m13:34:40.200798 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:34:40.201699 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:34:40.207104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '66e83614-dc9c-449f-97cf-8fd489fb9d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bef2030>]}
[0m13:34:40.207259 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m13:34:40.207366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '66e83614-dc9c-449f-97cf-8fd489fb9d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c37dfd0>]}
[0m13:34:40.208110 [info ] [MainThread]: 
[0m13:34:40.208229 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:34:40.208310 [info ] [MainThread]: 
[0m13:34:40.208485 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:34:40.208578 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:34:40.208864 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m13:34:40.208957 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m13:34:40.212301 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m13:34:40.212403 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m13:34:40.212480 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:34:40.938931 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-255f-193a-b18d-065d1efa3bb7) - Created
[0m13:34:41.640267 [debug] [ThreadPool]: SQL status: OK in 1.430 seconds
[0m13:34:41.648622 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08983-255f-193a-b18d-065d1efa3bb7, command-id=01f08983-257a-1b9d-b3ef-a8ae0f288de1) - Closing
[0m13:34:41.649126 [debug] [ThreadPool]: On list_flight_db: Close
[0m13:34:41.649339 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-255f-193a-b18d-065d1efa3bb7) - Closing
[0m13:34:41.843570 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_bronze) - Creating connection
[0m13:34:41.843798 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_bronze'
[0m13:34:41.844740 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt) - Creating connection
[0m13:34:41.849244 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_bronze"
[0m13:34:41.849508 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_gold) - Creating connection
[0m13:34:41.849627 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt'
[0m13:34:41.849833 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_flights_dbt_silver) - Creating connection
[0m13:34:41.850016 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_bronze'

  
[0m13:34:41.850164 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_gold'
[0m13:34:41.851031 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt"
[0m13:34:41.851160 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_flights_dbt_silver'
[0m13:34:41.851273 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:34:41.852175 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_gold"
[0m13:34:41.852316 [debug] [ThreadPool]: On list_flight_db_flights_dbt: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt'

  
[0m13:34:41.853037 [debug] [ThreadPool]: Using databricks connection "list_flight_db_flights_dbt_silver"
[0m13:34:41.853271 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_gold"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_gold'

  
[0m13:34:41.853437 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:34:41.853565 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_flights_dbt_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'flights_dbt_silver'

  
[0m13:34:41.853668 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:34:41.853823 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:34:42.544668 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-2655-1cf7-9740-9f15a69159a2) - Created
[0m13:34:42.596657 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-265d-194f-b225-e6d61fd9f1f4) - Created
[0m13:34:42.613069 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-265f-1835-8c8e-59ff595bf974) - Created
[0m13:34:42.629695 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-265f-1a10-9952-c2069de72753) - Created
[0m13:34:43.031933 [debug] [ThreadPool]: SQL status: OK in 1.180 seconds
[0m13:34:43.034865 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08983-265d-194f-b225-e6d61fd9f1f4, command-id=01f08983-2678-1156-bc8e-24de6301d63f) - Closing
[0m13:34:43.035250 [debug] [ThreadPool]: On list_flight_db_flights_dbt_gold: Close
[0m13:34:43.037270 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-265d-194f-b225-e6d61fd9f1f4) - Closing
[0m13:34:43.037933 [debug] [ThreadPool]: SQL status: OK in 1.180 seconds
[0m13:34:43.039429 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08983-2655-1cf7-9740-9f15a69159a2, command-id=01f08983-266f-15b2-aea0-5cc0cf9ccdb3) - Closing
[0m13:34:43.168019 [debug] [ThreadPool]: SQL status: OK in 1.320 seconds
[0m13:34:43.170255 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08983-265f-1a10-9952-c2069de72753, command-id=01f08983-267d-1e0b-9870-585cc133b795) - Closing
[0m13:34:43.185847 [debug] [ThreadPool]: SQL status: OK in 1.330 seconds
[0m13:34:43.187976 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08983-265f-1835-8c8e-59ff595bf974, command-id=01f08983-267a-1db6-8e93-ca1c0228170f) - Closing
[0m13:34:43.232482 [debug] [ThreadPool]: On list_flight_db_flights_dbt: Close
[0m13:34:43.233200 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-2655-1cf7-9740-9f15a69159a2) - Closing
[0m13:34:43.431016 [debug] [ThreadPool]: On list_flight_db_flights_dbt_bronze: Close
[0m13:34:43.432845 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-265f-1a10-9952-c2069de72753) - Closing
[0m13:34:43.663674 [debug] [ThreadPool]: On list_flight_db_flights_dbt_silver: Close
[0m13:34:43.665630 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08983-265f-1835-8c8e-59ff595bf974) - Closing
[0m13:34:43.866991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '66e83614-dc9c-449f-97cf-8fd489fb9d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c327450>]}
[0m13:34:43.871714 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m13:34:43.872444 [info ] [Thread-1 (]: 1 of 1 START sql view model flights_dbt_bronze.bronze_airlines ................. [RUN]
[0m13:34:43.873078 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m13:34:43.873414 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m13:34:43.873719 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m13:34:43.882697 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m13:34:43.883681 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m13:34:43.896995 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m13:34:43.898640 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:34:43.898991 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '66e83614-dc9c-449f-97cf-8fd489fb9d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c386390>]}
[0m13:34:43.907470 [debug] [Thread-1 (]: Creating view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
[0m13:34:43.912121 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m13:34:43.912590 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m13:34:43.912766 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
  as (
    select iata_code, airline_name from `flight_db`.`raw`.`airlines`
  )

[0m13:34:43.912909 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:34:44.576464 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08983-278b-19fa-82bb-56deb3984c9b) - Created
[0m13:34:45.035959 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`flights_dbt_bronze`.`bronze_airlines`
  
  as (
    select iata_code, airline_name from `flight_db`.`raw`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08983-27a6-1c1c-879f-215684d5ff26
[0m13:34:45.037076 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m13:34:45.037437 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08983-278b-19fa-82bb-56deb3984c9b) - Closing
[0m13:34:45.248773 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m13:34:45.251801 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66e83614-dc9c-449f-97cf-8fd489fb9d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf8f490>]}
[0m13:34:45.252605 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model flights_dbt_bronze.bronze_airlines ........ [[31mERROR[0m in 1.38s]
[0m13:34:45.253158 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m13:34:45.253674 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m13:34:45.255696 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:34:45.256003 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:34:45.256353 [info ] [MainThread]: 
[0m13:34:45.256602 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 5.05 seconds (5.05s).
[0m13:34:45.257115 [debug] [MainThread]: Command end result
[0m13:34:45.278748 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m13:34:45.279699 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m13:34:45.282685 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m13:34:45.282827 [info ] [MainThread]: 
[0m13:34:45.282990 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:34:45.283117 [info ] [MainThread]: 
[0m13:34:45.283277 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m13:34:45.283432 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m13:34:45.283555 [info ] [MainThread]: 
[0m13:34:45.283683 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m13:34:45.283789 [info ] [MainThread]: 
[0m13:34:45.283914 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m13:34:45.286409 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 6.1025395, "process_in_blocks": "0", "process_kernel_time": 0.252853, "process_mem_max_rss": "245252096", "process_out_blocks": "0", "process_user_time": 1.898099}
[0m13:34:45.286602 [debug] [MainThread]: Command `dbt run` failed at 13:34:45.286563 after 6.10 seconds
[0m13:34:45.286762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c4177f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c32ef00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c1f2bd0>]}
[0m13:34:45.286898 [debug] [MainThread]: Flushing usage events
[0m13:34:45.906068 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:37:45.871003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10402b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058f3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058f3b10>]}


============================== 15:37:45.873342 | 70c9c618-96b8-495f-87a4-2cc8af76dbb3 ==============================
[0m15:37:45.873342 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:37:45.873572 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'quiet': 'False', 'version_check': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt debug', 'introspect': 'True', 'debug': 'False', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'indirect_selection': 'eager', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'static_parser': 'True', 'use_colors': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'log_format': 'default', 'target_path': 'None', 'log_cache_events': 'False', 'no_print': 'None'}
[0m15:37:45.879993 [info ] [MainThread]: dbt version: 1.10.9
[0m15:37:45.880114 [info ] [MainThread]: python version: 3.13.0
[0m15:37:45.880203 [info ] [MainThread]: python path: /Users/artakerqeli/flights_dbt/.venv/bin/python3.13
[0m15:37:45.880282 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m15:37:46.176122 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:37:46.176305 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:37:46.176400 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:37:46.761029 [info ] [MainThread]: Using profiles dir at /Users/artakerqeli/.dbt
[0m15:37:46.761234 [info ] [MainThread]: Using profiles.yml file at /Users/artakerqeli/.dbt/profiles.yml
[0m15:37:46.761342 [info ] [MainThread]: Using dbt_project.yml file at /Users/artakerqeli/flights_dbt/flights_dbt/dbt_project.yml
[0m15:37:46.761430 [info ] [MainThread]: adapter type: databricks
[0m15:37:46.761511 [info ] [MainThread]: adapter version: 1.10.11
[0m15:37:46.794331 [info ] [MainThread]: Configuration:
[0m15:37:46.794516 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:37:46.794614 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:37:46.794699 [info ] [MainThread]: Required dependencies:
[0m15:37:46.794830 [debug] [MainThread]: Executing "git --help"
[0m15:37:46.811166 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:37:46.811508 [debug] [MainThread]: STDERR: "b''"
[0m15:37:46.811610 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:37:46.811702 [info ] [MainThread]: Connection:
[0m15:37:46.811825 [info ] [MainThread]:   host: https://dbc-8fd4d429-f882.cloud.databricks.com
[0m15:37:46.811908 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/2444393c45259518
[0m15:37:46.811983 [info ] [MainThread]:   catalog: flight_db
[0m15:37:46.812060 [info ] [MainThread]:   schema: bronze
[0m15:37:46.812308 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m15:37:46.855698 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m15:37:46.856053 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m15:37:46.856168 [debug] [MainThread]: Using databricks connection "debug"
[0m15:37:46.856261 [debug] [MainThread]: On debug: select 1 as id
[0m15:37:46.856341 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:37:47.629545 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f08994-5822-1db2-9ecc-4544e9174790) - Created
[0m15:37:48.984882 [debug] [MainThread]: SQL status: OK in 2.130 seconds
[0m15:37:48.986768 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f08994-5822-1db2-9ecc-4544e9174790, command-id=01f08994-5840-1003-86fd-49dd1a4447de) - Closing
[0m15:37:48.987579 [debug] [MainThread]: On debug: Close
[0m15:37:48.987929 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f08994-5822-1db2-9ecc-4544e9174790) - Closing
[0m15:37:49.209436 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:37:49.210498 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:37:49.217794 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 3.3761554, "process_in_blocks": "0", "process_kernel_time": 0.217765, "process_mem_max_rss": "225427456", "process_out_blocks": "0", "process_user_time": 0.983911}
[0m15:37:49.218390 [debug] [MainThread]: Command `dbt debug` succeeded at 15:37:49.218269 after 3.38 seconds
[0m15:37:49.218838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e821ba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e803410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db6bf00>]}
[0m15:37:49.219236 [debug] [MainThread]: Flushing usage events
[0m15:37:49.882053 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:38:09.955360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103be3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056fbb10>]}


============================== 15:38:09.957566 | e97bdb22-c4ba-451a-a998-64406b89ac8e ==============================
[0m15:38:09.957566 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:38:09.957801 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/artakerqeli/.dbt', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'quiet': 'False', 'invocation_command': 'dbt run --select bronze_*', 'debug': 'False', 'use_colors': 'True', 'fail_fast': 'False', 'log_format': 'default', 'warn_error': 'None', 'no_print': 'None', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'introspect': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'target_path': 'None', 'version_check': 'True'}
[0m15:38:10.248227 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:38:10.248415 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:38:10.248507 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:38:10.671571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e97bdb22-c4ba-451a-a998-64406b89ac8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104523360>]}
[0m15:38:10.690718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e97bdb22-c4ba-451a-a998-64406b89ac8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107602ad0>]}
[0m15:38:10.690951 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m15:38:10.740503 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:38:10.777941 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m15:38:10.778163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e97bdb22-c4ba-451a-a998-64406b89ac8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116d13450>]}
[0m15:38:11.419883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e97bdb22-c4ba-451a-a998-64406b89ac8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e1d040>]}
[0m15:38:11.451903 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:38:11.453539 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:38:11.458121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e97bdb22-c4ba-451a-a998-64406b89ac8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116dbf3f0>]}
[0m15:38:11.458272 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m15:38:11.458370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e97bdb22-c4ba-451a-a998-64406b89ac8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116f3e5b0>]}
[0m15:38:11.458650 [warn ] [MainThread]: The selection criterion 'bronze_*' does not match any enabled nodes
[0m15:38:11.459051 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m15:38:11.459229 [debug] [MainThread]: Command end result
[0m15:38:11.470837 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:38:11.471380 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:38:11.472496 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m15:38:11.474311 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.5468624, "process_in_blocks": "0", "process_kernel_time": 0.212302, "process_mem_max_rss": "232701952", "process_out_blocks": "0", "process_user_time": 1.667535}
[0m15:38:11.474465 [debug] [MainThread]: Command `dbt run` succeeded at 15:38:11.474435 after 1.55 seconds
[0m15:38:11.474597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053b2e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116dd3cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116dd3ee0>]}
[0m15:38:11.474700 [debug] [MainThread]: Flushing usage events
[0m15:38:12.068878 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:40:11.401896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cb3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072fbb10>]}


============================== 15:40:11.404374 | 82c801db-975e-4098-b7bc-9428033288a2 ==============================
[0m15:40:11.404374 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:40:11.404620 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'quiet': 'False', 'use_colors': 'True', 'indirect_selection': 'eager', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'static_parser': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'log_cache_events': 'False', 'partial_parse': 'True', 'invocation_command': 'dbt run --select path:models/bronze', 'profiles_dir': '/Users/artakerqeli/.dbt', 'debug': 'False', 'empty': 'False', 'write_json': 'True', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'log_format': 'default', 'target_path': 'None'}
[0m15:40:11.700677 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:40:11.700871 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:40:11.700967 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:40:12.145885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061bb360>]}
[0m15:40:12.164457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112306ad0>]}
[0m15:40:12.164687 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m15:40:12.217977 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:40:12.253574 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:40:12.253750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120413450>]}
[0m15:40:12.887478 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db.gold
- models.flight_db
- models.flight_db.silver
- models.flight_db.bronze
[0m15:40:12.890918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12050d040>]}
[0m15:40:12.922938 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:40:12.924035 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:40:12.929686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1204cb3f0>]}
[0m15:40:12.929842 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m15:40:12.929940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12063e5b0>]}
[0m15:40:12.930657 [info ] [MainThread]: 
[0m15:40:12.930764 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:40:12.930847 [info ] [MainThread]: 
[0m15:40:12.931018 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:40:12.931106 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:40:12.933436 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m15:40:12.933552 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m15:40:12.936913 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m15:40:12.937021 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m15:40:12.937102 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:40:13.708803 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08994-af32-15ed-80b2-d4a9ca0bd5c5) - Created
[0m15:40:14.257789 [debug] [ThreadPool]: SQL status: OK in 1.320 seconds
[0m15:40:14.267784 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08994-af32-15ed-80b2-d4a9ca0bd5c5, command-id=01f08994-af56-109a-a3ac-52d445857da1) - Closing
[0m15:40:14.268406 [debug] [ThreadPool]: On list_flight_db: Close
[0m15:40:14.268654 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08994-af32-15ed-80b2-d4a9ca0bd5c5) - Closing
[0m15:40:14.462054 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_bronze) - Creating connection
[0m15:40:14.463020 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_bronze'
[0m15:40:14.478799 [debug] [ThreadPool]: Using databricks connection "list_flight_db_bronze"
[0m15:40:14.479288 [debug] [ThreadPool]: On list_flight_db_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'bronze'

  
[0m15:40:14.479586 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:40:15.277249 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08994-b020-1865-839d-d0b292b89efd) - Created
[0m15:40:16.537762 [debug] [ThreadPool]: SQL status: OK in 2.060 seconds
[0m15:40:16.542100 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08994-b020-1865-839d-d0b292b89efd, command-id=01f08994-b048-1fb0-a1d5-81d475d68539) - Closing
[0m15:40:16.542763 [debug] [ThreadPool]: On list_flight_db_bronze: Close
[0m15:40:16.543026 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08994-b020-1865-839d-d0b292b89efd) - Closing
[0m15:40:16.778101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1206ea5d0>]}
[0m15:40:16.783567 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m15:40:16.784092 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m15:40:16.784451 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m15:40:16.785076 [info ] [Thread-1 (]: 1 of 3 START sql view model bronze.bronze_airlines ............................. [RUN]
[0m15:40:16.785540 [info ] [Thread-2 (]: 2 of 3 START sql view model bronze.bronze_airports ............................. [RUN]
[0m15:40:16.786018 [info ] [Thread-3 (]: 3 of 3 START sql view model bronze.bronze_flights .............................. [RUN]
[0m15:40:16.786720 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m15:40:16.787208 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m15:40:16.787648 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m15:40:16.787995 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m15:40:16.788302 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m15:40:16.788591 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m15:40:16.788902 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m15:40:16.789290 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m15:40:16.789612 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m15:40:16.798104 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m15:40:16.801608 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m15:40:16.805181 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m15:40:16.806076 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m15:40:16.816410 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m15:40:16.817507 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:40:16.817821 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120bf0c00>]}
[0m15:40:16.818056 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m15:40:16.825260 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m15:40:16.826867 [debug] [Thread-3 (]: Creating view `flight_db`.`bronze`.`bronze_flights`
[0m15:40:16.827367 [debug] [Thread-2 (]: Creating view `flight_db`.`bronze`.`bronze_airports`
[0m15:40:16.827532 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m15:40:16.832380 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m15:40:16.832810 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m15:40:16.833750 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m15:40:16.834265 [debug] [Thread-1 (]: Creating view `flight_db`.`bronze`.`bronze_airlines`
[0m15:40:16.834597 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m15:40:16.834809 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m15:40:16.834951 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m15:40:16.835107 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m15:40:16.835265 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m15:40:16.835383 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m15:40:16.835509 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:40:16.835640 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:40:16.835775 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select iata_code, airline_name from `flight_db`.`raw`.`airlines`
  )

[0m15:40:16.836031 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:40:17.510943 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08994-b17c-172f-a275-102f8a0a768f) - Created
[0m15:40:17.561510 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08994-b180-17b8-81b3-97bc7d7668c3) - Created
[0m15:40:17.638542 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08994-b189-1d52-92f0-95880c9a2961) - Created
[0m15:40:18.084203 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select iata_code, airline_name from `flight_db`.`raw`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08994-b196-109c-b0d9-c0a4df91bb96
[0m15:40:18.086302 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m15:40:18.086791 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08994-b17c-172f-a275-102f8a0a768f) - Closing
[0m15:40:18.301800 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:40:18.304530 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10617ce10>]}
[0m15:40:18.305387 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model bronze.bronze_airlines .................... [[31mERROR[0m in 1.52s]
[0m15:40:18.305914 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m15:40:18.306382 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m15:40:20.145693 [debug] [Thread-3 (]: SQL status: OK in 3.310 seconds
[0m15:40:20.147395 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08994-b180-17b8-81b3-97bc7d7668c3, command-id=01f08994-b19f-1738-a1e4-0bcca6df953f) - Closing
[0m15:40:20.164707 [debug] [Thread-3 (]: Applying tags to relation None
[0m15:40:20.165225 [debug] [Thread-2 (]: SQL status: OK in 3.330 seconds
[0m15:40:20.168504 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m15:40:20.169410 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08994-b189-1d52-92f0-95880c9a2961, command-id=01f08994-b1ab-142c-8376-825983da35cc) - Closing
[0m15:40:20.169692 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08994-b180-17b8-81b3-97bc7d7668c3) - Closing
[0m15:40:20.170234 [debug] [Thread-2 (]: Applying tags to relation None
[0m15:40:20.380310 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m15:40:20.381314 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08994-b189-1d52-92f0-95880c9a2961) - Closing
[0m15:40:20.615059 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1201ca650>]}
[0m15:40:20.616100 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82c801db-975e-4098-b7bc-9428033288a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120266e50>]}
[0m15:40:20.617221 [info ] [Thread-3 (]: 3 of 3 OK created sql view model bronze.bronze_flights ......................... [[32mOK[0m in 3.83s]
[0m15:40:20.618492 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m15:40:20.617965 [info ] [Thread-2 (]: 2 of 3 OK created sql view model bronze.bronze_airports ........................ [[32mOK[0m in 3.83s]
[0m15:40:20.619219 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m15:40:20.620909 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:40:20.621244 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:40:20.621726 [info ] [MainThread]: 
[0m15:40:20.622047 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 7.69 seconds (7.69s).
[0m15:40:20.623003 [debug] [MainThread]: Command end result
[0m15:40:20.653261 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:40:20.654703 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:40:20.658836 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m15:40:20.658989 [info ] [MainThread]: 
[0m15:40:20.659162 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:40:20.659302 [info ] [MainThread]: 
[0m15:40:20.659480 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m15:40:20.659644 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 40
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:40:20.659763 [info ] [MainThread]: 
[0m15:40:20.659895 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:40:20.660004 [info ] [MainThread]: 
[0m15:40:20.660138 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m15:40:20.662922 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.294206, "process_in_blocks": "0", "process_kernel_time": 0.262448, "process_mem_max_rss": "238485504", "process_out_blocks": "0", "process_user_time": 2.10914}
[0m15:40:20.663149 [debug] [MainThread]: Command `dbt run` failed at 15:40:20.663103 after 9.29 seconds
[0m15:40:20.663346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a2fb30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12134fbf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12134fb30>]}
[0m15:40:20.663516 [debug] [MainThread]: Flushing usage events
[0m15:40:21.551376 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:41:41.660834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114783620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115efb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115efbb10>]}


============================== 15:41:41.663141 | b8b817b0-924c-45dc-bce4-56d26c7d3ced ==============================
[0m15:41:41.663141 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:41:41.663384 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'printer_width': '80', 'warn_error': 'None', 'write_json': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'empty': 'False', 'debug': 'False', 'target_path': 'None', 'no_print': 'None', 'static_parser': 'True', 'introspect': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'partial_parse': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run --select path:models/bronze'}
[0m15:41:41.954640 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:41:41.954825 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:41:41.954919 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:41:42.398966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e23360>]}
[0m15:41:42.417739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120802ad0>]}
[0m15:41:42.417968 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m15:41:42.466649 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:41:42.522637 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m15:41:42.523003 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/sources.yml
[0m15:41:42.523116 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m15:41:42.648863 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db.silver
- models.flight_db.bronze
- models.flight_db
- models.flight_db.gold
[0m15:41:42.652335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127313550>]}
[0m15:41:42.685206 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:41:42.686302 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:41:42.692077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1272ee210>]}
[0m15:41:42.692236 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m15:41:42.692335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12710b770>]}
[0m15:41:42.693202 [info ] [MainThread]: 
[0m15:41:42.693389 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:41:42.693490 [info ] [MainThread]: 
[0m15:41:42.693712 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:41:42.693810 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:41:42.696199 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m15:41:42.696344 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m15:41:42.699874 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m15:41:42.700004 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m15:41:42.700097 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:41:43.416555 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08994-e4ae-1b1c-b6f7-e7545eb74f71) - Created
[0m15:41:43.982273 [debug] [ThreadPool]: SQL status: OK in 1.280 seconds
[0m15:41:43.992650 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08994-e4ae-1b1c-b6f7-e7545eb74f71, command-id=01f08994-e4ca-1767-b1a6-503433bd44bd) - Closing
[0m15:41:43.993217 [debug] [ThreadPool]: On list_flight_db: Close
[0m15:41:43.993464 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08994-e4ae-1b1c-b6f7-e7545eb74f71) - Closing
[0m15:41:44.201491 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_bronze) - Creating connection
[0m15:41:44.202098 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_bronze'
[0m15:41:44.216104 [debug] [ThreadPool]: Using databricks connection "list_flight_db_bronze"
[0m15:41:44.216511 [debug] [ThreadPool]: On list_flight_db_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'bronze'

  
[0m15:41:44.216770 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:41:44.948954 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08994-e596-1e4e-bf14-d7e99d3b337a) - Created
[0m15:41:45.951323 [debug] [ThreadPool]: SQL status: OK in 1.730 seconds
[0m15:41:45.954996 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08994-e596-1e4e-bf14-d7e99d3b337a, command-id=01f08994-e5b5-11ca-a509-6a44228fa440) - Closing
[0m15:41:45.955901 [debug] [ThreadPool]: On list_flight_db_bronze: Close
[0m15:41:45.956210 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08994-e596-1e4e-bf14-d7e99d3b337a) - Closing
[0m15:41:46.191563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12772a0d0>]}
[0m15:41:46.196932 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m15:41:46.197370 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m15:41:46.197638 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m15:41:46.198194 [info ] [Thread-3 (]: 3 of 3 START sql view model bronze.bronze_flights .............................. [RUN]
[0m15:41:46.198537 [info ] [Thread-2 (]: 2 of 3 START sql view model bronze.bronze_airports ............................. [RUN]
[0m15:41:46.198886 [info ] [Thread-1 (]: 1 of 3 START sql view model bronze.bronze_airlines ............................. [RUN]
[0m15:41:46.199544 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m15:41:46.200054 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m15:41:46.200447 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m15:41:46.200740 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m15:41:46.201013 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m15:41:46.201266 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m15:41:46.201543 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m15:41:46.201808 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m15:41:46.202055 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m15:41:46.208788 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m15:41:46.211467 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m15:41:46.214419 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m15:41:46.215279 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m15:41:46.215497 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m15:41:46.215663 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m15:41:46.224946 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m15:41:46.226341 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m15:41:46.227254 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m15:41:46.228259 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:41:46.228545 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:41:46.228833 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:41:46.229060 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1273eeed0>]}
[0m15:41:46.229207 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127917960>]}
[0m15:41:46.229344 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1279175f0>]}
[0m15:41:46.236899 [debug] [Thread-2 (]: Creating view `flight_db`.`bronze`.`bronze_airports`
[0m15:41:46.238100 [debug] [Thread-3 (]: Creating view `flight_db`.`bronze`.`bronze_flights`
[0m15:41:46.238479 [debug] [Thread-1 (]: Creating view `flight_db`.`bronze`.`bronze_airlines`
[0m15:41:46.242370 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m15:41:46.242691 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m15:41:46.242986 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m15:41:46.243449 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m15:41:46.243572 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m15:41:46.243674 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m15:41:46.243808 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m15:41:46.243966 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m15:41:46.244103 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

[0m15:41:46.244230 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:41:46.244342 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:41:46.244450 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:41:46.894753 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08994-e6c3-15e0-b692-130e68a49e05) - Created
[0m15:41:46.932240 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08994-e6c9-13c7-a8b0-120156decfc3) - Created
[0m15:41:46.989893 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08994-e6ce-1c9e-a716-e05a2d73c999) - Created
[0m15:41:47.349358 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08994-e6dd-106f-a60b-7dd1b5eadd87
[0m15:41:47.350634 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m15:41:47.350972 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08994-e6c3-15e0-b692-130e68a49e05) - Closing
[0m15:41:47.548830 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:41:47.550726 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114de9ed0>]}
[0m15:41:47.551301 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model bronze.bronze_airlines .................... [[31mERROR[0m in 1.35s]
[0m15:41:47.551761 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m15:41:47.552137 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m15:41:47.706036 [debug] [Thread-2 (]: SQL status: OK in 1.460 seconds
[0m15:41:47.708537 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08994-e6c9-13c7-a8b0-120156decfc3, command-id=01f08994-e6e4-156d-b309-0d2ff68e7c05) - Closing
[0m15:41:47.717786 [debug] [Thread-2 (]: Applying tags to relation None
[0m15:41:47.720187 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m15:41:47.720423 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08994-e6c9-13c7-a8b0-120156decfc3) - Closing
[0m15:41:47.755079 [debug] [Thread-3 (]: SQL status: OK in 1.510 seconds
[0m15:41:47.755905 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08994-e6ce-1c9e-a716-e05a2d73c999, command-id=01f08994-e6ec-1ee9-9291-4afce9781bbd) - Closing
[0m15:41:47.756367 [debug] [Thread-3 (]: Applying tags to relation None
[0m15:41:47.915050 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m15:41:47.916045 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08994-e6ce-1c9e-a716-e05a2d73c999) - Closing
[0m15:41:48.125170 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12710df60>]}
[0m15:41:48.126208 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8b817b0-924c-45dc-bce4-56d26c7d3ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127980e50>]}
[0m15:41:48.127407 [info ] [Thread-2 (]: 2 of 3 OK created sql view model bronze.bronze_airports ........................ [[32mOK[0m in 1.92s]
[0m15:41:48.128706 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m15:41:48.128144 [info ] [Thread-3 (]: 3 of 3 OK created sql view model bronze.bronze_flights ......................... [[32mOK[0m in 1.93s]
[0m15:41:48.129417 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m15:41:48.131122 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:41:48.131490 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:41:48.131991 [info ] [MainThread]: 
[0m15:41:48.132312 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 5.44 seconds (5.44s).
[0m15:41:48.133275 [debug] [MainThread]: Command end result
[0m15:41:48.170017 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:41:48.170942 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:41:48.174042 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m15:41:48.174175 [info ] [MainThread]: 
[0m15:41:48.174324 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:41:48.174438 [info ] [MainThread]: 
[0m15:41:48.174593 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m15:41:48.174748 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:41:48.174862 [info ] [MainThread]: 
[0m15:41:48.174991 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:41:48.175094 [info ] [MainThread]: 
[0m15:41:48.175219 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m15:41:48.178186 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 6.5496726, "process_in_blocks": "0", "process_kernel_time": 0.254681, "process_mem_max_rss": "243957760", "process_out_blocks": "0", "process_user_time": 1.679902}
[0m15:41:48.178406 [debug] [MainThread]: Command `dbt run` failed at 15:41:48.178365 after 6.55 seconds
[0m15:41:48.178581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1134d1c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127cb3a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127cb3a10>]}
[0m15:41:48.178724 [debug] [MainThread]: Flushing usage events
[0m15:41:49.136738 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:45:52.488219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10656b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082fbb10>]}


============================== 15:45:52.491357 | 7f510f90-396b-4c06-a8da-95a646c9ca4e ==============================
[0m15:45:52.491357 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:45:52.491616 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'version_check': 'True', 'use_colors': 'True', 'write_json': 'True', 'static_parser': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'quiet': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'cache_selected_only': 'False', 'partial_parse': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'no_print': 'None', 'invocation_command': 'dbt run --select path:models/bronze', 'log_format': 'default', 'warn_error': 'None'}
[0m15:45:52.784558 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:45:52.784752 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:45:52.784847 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:45:53.225519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a6b360>]}
[0m15:45:53.244650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a202ad0>]}
[0m15:45:53.244932 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m15:45:53.294474 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:45:53.353128 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:45:53.353297 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:45:53.356397 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db.bronze
- models.flight_db
- models.flight_db.silver
- models.flight_db.gold
[0m15:45:53.371181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118412d50>]}
[0m15:45:53.406603 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:45:53.407925 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:45:53.413356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff0b890>]}
[0m15:45:53.413524 [info ] [MainThread]: Found 9 models, 4 data tests, 3 sources, 686 macros
[0m15:45:53.413628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185f7850>]}
[0m15:45:53.414392 [info ] [MainThread]: 
[0m15:45:53.414507 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:45:53.414592 [info ] [MainThread]: 
[0m15:45:53.414786 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:45:53.414877 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:45:53.417228 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m15:45:53.417347 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m15:45:53.421638 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m15:45:53.421748 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m15:45:53.421831 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:45:54.205407 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08995-7a2a-125f-af17-bcde6a4300d8) - Created
[0m15:45:54.678680 [debug] [ThreadPool]: SQL status: OK in 1.260 seconds
[0m15:45:54.691910 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08995-7a2a-125f-af17-bcde6a4300d8, command-id=01f08995-7a4a-1598-8c93-9ccf8505a4b3) - Closing
[0m15:45:54.692400 [debug] [ThreadPool]: On list_flight_db: Close
[0m15:45:54.692601 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08995-7a2a-125f-af17-bcde6a4300d8) - Closing
[0m15:45:54.882859 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_bronze) - Creating connection
[0m15:45:54.883494 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_bronze'
[0m15:45:54.896291 [debug] [ThreadPool]: Using databricks connection "list_flight_db_bronze"
[0m15:45:54.897084 [debug] [ThreadPool]: On list_flight_db_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'bronze'

  
[0m15:45:54.897589 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:45:55.592342 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08995-7afd-1451-ab4b-7c3dc5a6515c) - Created
[0m15:45:56.219479 [debug] [ThreadPool]: SQL status: OK in 1.320 seconds
[0m15:45:56.223747 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08995-7afd-1451-ab4b-7c3dc5a6515c, command-id=01f08995-7b19-17eb-b886-f1e7435382cb) - Closing
[0m15:45:56.224723 [debug] [ThreadPool]: On list_flight_db_bronze: Close
[0m15:45:56.225118 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08995-7afd-1451-ab4b-7c3dc5a6515c) - Closing
[0m15:45:56.446645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199542c0>]}
[0m15:45:56.452032 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m15:45:56.452521 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m15:45:56.452898 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m15:45:56.453873 [info ] [Thread-1 (]: 1 of 3 START sql view model bronze.bronze_airlines ............................. [RUN]
[0m15:45:56.454542 [info ] [Thread-2 (]: 2 of 3 START sql view model bronze.bronze_airports ............................. [RUN]
[0m15:45:56.456802 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m15:45:56.455589 [info ] [Thread-3 (]: 3 of 3 START sql view model bronze.bronze_flights .............................. [RUN]
[0m15:45:56.457522 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m15:45:56.457873 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m15:45:56.458332 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m15:45:56.458690 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m15:45:56.459037 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m15:45:56.459345 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m15:45:56.459635 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m15:45:56.467946 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m15:45:56.468319 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m15:45:56.472719 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m15:45:56.474487 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m15:45:56.475147 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m15:45:56.475471 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m15:45:56.475725 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m15:45:56.486242 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m15:45:56.487404 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m15:45:56.488625 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m15:45:56.489836 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:45:56.490191 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:45:56.490765 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11851d3d0>]}
[0m15:45:56.490469 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:45:56.490981 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119901f40>]}
[0m15:45:56.497970 [debug] [Thread-2 (]: Creating view `flight_db`.`bronze`.`bronze_airports`
[0m15:45:56.498113 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199b5bd0>]}
[0m15:45:56.498498 [debug] [Thread-3 (]: Creating view `flight_db`.`bronze`.`bronze_flights`
[0m15:45:56.503094 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m15:45:56.503549 [debug] [Thread-1 (]: Creating view `flight_db`.`bronze`.`bronze_airlines`
[0m15:45:56.503881 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m15:45:56.504258 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m15:45:56.504807 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m15:45:56.504950 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m15:45:56.505119 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m15:45:56.505255 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m15:45:56.505409 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

[0m15:45:56.505553 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:45:56.505691 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m15:45:56.505818 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:45:56.506022 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:45:57.193006 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08995-7bf3-198c-871f-1962fc3e3006) - Created
[0m15:45:57.209680 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08995-7bf3-1858-bb6d-1a9a0ec60c66) - Created
[0m15:45:57.288525 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08995-7bfa-1a2f-9194-867b5e17dc5f) - Created
[0m15:45:57.604604 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08995-7c0e-147d-9afb-e53b9e66fe53
[0m15:45:57.606004 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m15:45:57.606429 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08995-7bf3-198c-871f-1962fc3e3006) - Closing
[0m15:45:57.815880 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:45:57.818616 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a31ed0>]}
[0m15:45:57.819480 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model bronze.bronze_airlines .................... [[31mERROR[0m in 1.36s]
[0m15:45:57.820195 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m15:45:57.820921 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m15:45:57.855809 [debug] [Thread-3 (]: SQL status: OK in 1.350 seconds
[0m15:45:57.857108 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08995-7bf3-1858-bb6d-1a9a0ec60c66, command-id=01f08995-7c0f-14be-a6fe-d1bd4c3ff103) - Closing
[0m15:45:57.868861 [debug] [Thread-3 (]: Applying tags to relation None
[0m15:45:57.871458 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m15:45:57.871727 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08995-7bf3-1858-bb6d-1a9a0ec60c66) - Closing
[0m15:45:57.967660 [debug] [Thread-2 (]: SQL status: OK in 1.460 seconds
[0m15:45:57.969176 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08995-7bfa-1a2f-9194-867b5e17dc5f, command-id=01f08995-7c1c-1eac-a6c7-b07623ccab9e) - Closing
[0m15:45:57.970053 [debug] [Thread-2 (]: Applying tags to relation None
[0m15:45:58.064978 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m15:45:58.065775 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08995-7bfa-1a2f-9194-867b5e17dc5f) - Closing
[0m15:45:58.314186 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119963e70>]}
[0m15:45:58.315223 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f510f90-396b-4c06-a8da-95a646c9ca4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199638c0>]}
[0m15:45:58.316407 [info ] [Thread-3 (]: 3 of 3 OK created sql view model bronze.bronze_flights ......................... [[32mOK[0m in 1.86s]
[0m15:45:58.317649 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m15:45:58.317127 [info ] [Thread-2 (]: 2 of 3 OK created sql view model bronze.bronze_airports ........................ [[32mOK[0m in 1.86s]
[0m15:45:58.318368 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m15:45:58.320056 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:45:58.320373 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:45:58.320856 [info ] [MainThread]: 
[0m15:45:58.321175 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 4.91 seconds (4.91s).
[0m15:45:58.322093 [debug] [MainThread]: Command end result
[0m15:45:58.357185 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:45:58.358281 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:45:58.361506 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m15:45:58.361664 [info ] [MainThread]: 
[0m15:45:58.361822 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:45:58.361950 [info ] [MainThread]: 
[0m15:45:58.362124 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m15:45:58.362283 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:45:58.362449 [info ] [MainThread]: 
[0m15:45:58.362640 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:45:58.362773 [info ] [MainThread]: 
[0m15:45:58.362911 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m15:45:58.365366 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 5.9079065, "process_in_blocks": "0", "process_kernel_time": 0.256215, "process_mem_max_rss": "244776960", "process_out_blocks": "0", "process_user_time": 1.568345}
[0m15:45:58.365573 [debug] [MainThread]: Command `dbt run` failed at 15:45:58.365535 after 5.91 seconds
[0m15:45:58.365744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10727d490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9b3a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9b39b0>]}
[0m15:45:58.365887 [debug] [MainThread]: Flushing usage events
[0m15:46:00.106824 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:48:17.305902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10788f620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088a7890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088a7b10>]}


============================== 15:48:17.308176 | 02215ef9-e5b5-40e8-b6cc-ed96bc86f25f ==============================
[0m15:48:17.308176 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:48:17.308419 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'introspect': 'True', 'no_print': 'None', 'empty': 'False', 'log_format': 'default', 'use_colors': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'version_check': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'indirect_selection': 'eager', 'partial_parse': 'True', 'static_parser': 'True', 'target_path': 'None', 'debug': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'quiet': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'log_cache_events': 'False'}
[0m15:48:17.618643 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:48:17.618842 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:48:17.618948 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:48:18.050625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '02215ef9-e5b5-40e8-b6cc-ed96bc86f25f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bd7360>]}
[0m15:48:18.070423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '02215ef9-e5b5-40e8-b6cc-ed96bc86f25f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ab2ad0>]}
[0m15:48:18.070709 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m15:48:18.120240 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:48:18.157587 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:48:18.157821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '02215ef9-e5b5-40e8-b6cc-ed96bc86f25f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f7a7450>]}
[0m15:48:18.938227 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.flights_dbt.bronze_airlines' (models/bronze/bronze_airlines.sql) depends on a source named 'raw_flights.airlines' which was not found
[0m15:48:18.941461 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.6663156, "process_in_blocks": "0", "process_kernel_time": 0.249476, "process_mem_max_rss": "229425152", "process_out_blocks": "0", "process_user_time": 1.635061}
[0m15:48:18.941660 [debug] [MainThread]: Command `dbt run` failed at 15:48:18.941620 after 1.67 seconds
[0m15:48:18.941812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f9f76b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f9f77a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcdd8d0>]}
[0m15:48:18.941930 [debug] [MainThread]: Flushing usage events
[0m15:48:19.613919 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:50:25.110710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046ab620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056c3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056c3b10>]}


============================== 15:50:25.113030 | a62eba9f-c411-4cbd-b13e-9dc75cb186d2 ==============================
[0m15:50:25.113030 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:50:25.113256 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'introspect': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select path:models/bronze', 'no_print': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'write_json': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'debug': 'False', 'printer_width': '80', 'version_check': 'True', 'empty': 'False', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'target_path': 'None', 'static_parser': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'fail_fast': 'False'}
[0m15:50:25.407180 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:50:25.407374 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:50:25.407470 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:50:25.836536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049ef360>]}
[0m15:50:25.855614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068caad0>]}
[0m15:50:25.855847 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m15:50:25.906558 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:50:25.943079 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:50:25.943271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c577450>]}
[0m15:50:26.579596 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db.silver
- models.flight_db
- models.flight_db.bronze
- models.flight_db.gold
[0m15:50:26.583188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c6945f0>]}
[0m15:50:26.614795 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:50:26.616197 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:50:26.621820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c631b70>]}
[0m15:50:26.621981 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m15:50:26.622083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c76c7a0>]}
[0m15:50:26.622831 [info ] [MainThread]: 
[0m15:50:26.622948 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:50:26.623038 [info ] [MainThread]: 
[0m15:50:26.623209 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:50:26.623298 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:50:26.625554 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m15:50:26.625665 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m15:50:26.629354 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m15:50:26.629520 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m15:50:26.629620 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:50:27.376456 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-1cfb-1ae5-96fe-f7199955e6cc) - Created
[0m15:50:27.885043 [debug] [ThreadPool]: SQL status: OK in 1.260 seconds
[0m15:50:27.893782 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08996-1cfb-1ae5-96fe-f7199955e6cc, command-id=01f08996-1d17-151a-9a50-c4335434f4db) - Closing
[0m15:50:27.894434 [debug] [ThreadPool]: On list_flight_db: Close
[0m15:50:27.894731 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-1cfb-1ae5-96fe-f7199955e6cc) - Closing
[0m15:50:28.098428 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_bronze) - Creating connection
[0m15:50:28.099007 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_bronze'
[0m15:50:28.112471 [debug] [ThreadPool]: Using databricks connection "list_flight_db_bronze"
[0m15:50:28.112851 [debug] [ThreadPool]: On list_flight_db_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'bronze'

  
[0m15:50:28.113111 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:50:28.897953 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-1dde-18ea-b250-e753a99805fc) - Created
[0m15:50:29.470086 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m15:50:29.473513 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08996-1dde-18ea-b250-e753a99805fc, command-id=01f08996-1e01-1071-9426-80c35d0a9dd9) - Closing
[0m15:50:29.474443 [debug] [ThreadPool]: On list_flight_db_bronze: Close
[0m15:50:29.474820 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-1dde-18ea-b250-e753a99805fc) - Closing
[0m15:50:29.698404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c84a5d0>]}
[0m15:50:29.703826 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m15:50:29.704273 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m15:50:29.704625 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m15:50:29.705216 [info ] [Thread-1 (]: 1 of 3 START sql view model bronze.bronze_airlines ............................. [RUN]
[0m15:50:29.705715 [info ] [Thread-3 (]: 3 of 3 START sql view model bronze.bronze_flights .............................. [RUN]
[0m15:50:29.706236 [info ] [Thread-2 (]: 2 of 3 START sql view model bronze.bronze_airports ............................. [RUN]
[0m15:50:29.706939 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m15:50:29.707428 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m15:50:29.707897 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m15:50:29.708243 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m15:50:29.708572 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m15:50:29.708939 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m15:50:29.709287 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m15:50:29.709611 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m15:50:29.709961 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m15:50:29.718655 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m15:50:29.722668 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m15:50:29.726226 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m15:50:29.727602 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m15:50:29.727914 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m15:50:29.728196 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m15:50:29.738981 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m15:50:29.740104 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m15:50:29.741115 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m15:50:29.742275 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:50:29.742610 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:50:29.742886 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:50:29.743112 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb5cc00>]}
[0m15:50:29.743275 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c3304b0>]}
[0m15:50:29.743426 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c332f30>]}
[0m15:50:29.750959 [debug] [Thread-1 (]: Creating view `flight_db`.`bronze`.`bronze_airlines`
[0m15:50:29.751438 [debug] [Thread-2 (]: Creating view `flight_db`.`bronze`.`bronze_airports`
[0m15:50:29.751842 [debug] [Thread-3 (]: Creating view `flight_db`.`bronze`.`bronze_flights`
[0m15:50:29.756367 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m15:50:29.756745 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m15:50:29.757072 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m15:50:29.757711 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m15:50:29.757835 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m15:50:29.757983 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m15:50:29.758087 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m15:50:29.758228 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m15:50:29.758358 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:50:29.758484 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

[0m15:50:29.758597 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:50:29.758800 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:50:30.457987 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08996-1ed3-1567-a9f4-3a8d718230c2) - Created
[0m15:50:30.467598 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08996-1ed3-15e4-a2d6-67117230d015) - Created
[0m15:50:30.504418 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08996-1ed8-1482-b816-7026ccfc1624) - Created
[0m15:50:30.985095 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08996-1ef6-10c4-887c-782d35927ea4
[0m15:50:30.986498 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m15:50:30.986906 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08996-1ed8-1482-b816-7026ccfc1624) - Closing
[0m15:50:31.118385 [debug] [Thread-3 (]: SQL status: OK in 1.360 seconds
[0m15:50:31.121441 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08996-1ed3-1567-a9f4-3a8d718230c2, command-id=01f08996-1eee-19e9-a281-1b01df43acc9) - Closing
[0m15:50:31.121863 [debug] [Thread-2 (]: SQL status: OK in 1.360 seconds
[0m15:50:31.134552 [debug] [Thread-3 (]: Applying tags to relation None
[0m15:50:31.135483 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08996-1ed3-15e4-a2d6-67117230d015, command-id=01f08996-1eee-1e8b-a132-2ba802bbd3e5) - Closing
[0m15:50:31.138145 [debug] [Thread-2 (]: Applying tags to relation None
[0m15:50:31.203418 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m15:50:31.204060 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08996-1ed3-1567-a9f4-3a8d718230c2) - Closing
[0m15:50:31.215130 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:50:31.413320 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m15:50:31.414386 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08996-1ed3-15e4-a2d6-67117230d015) - Closing
[0m15:50:31.604502 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c864d00>]}
[0m15:50:31.605037 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cf2aac0>]}
[0m15:50:31.605313 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a62eba9f-c411-4cbd-b13e-9dc75cb186d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cf195b0>]}
[0m15:50:31.606119 [info ] [Thread-3 (]: 3 of 3 OK created sql view model bronze.bronze_flights ......................... [[32mOK[0m in 1.89s]
[0m15:50:31.606637 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model bronze.bronze_airlines .................... [[31mERROR[0m in 1.89s]
[0m15:50:31.607272 [info ] [Thread-2 (]: 2 of 3 OK created sql view model bronze.bronze_airports ........................ [[32mOK[0m in 1.90s]
[0m15:50:31.607765 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m15:50:31.608186 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m15:50:31.608554 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m15:50:31.609141 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m15:50:31.611362 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:50:31.611747 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:50:31.612190 [info ] [MainThread]: 
[0m15:50:31.612478 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 4.99 seconds (4.99s).
[0m15:50:31.613303 [debug] [MainThread]: Command end result
[0m15:50:31.643080 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:50:31.644028 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:50:31.647635 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m15:50:31.647769 [info ] [MainThread]: 
[0m15:50:31.647923 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:50:31.648043 [info ] [MainThread]: 
[0m15:50:31.648202 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m15:50:31.648354 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:50:31.648480 [info ] [MainThread]: 
[0m15:50:31.648608 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:50:31.648715 [info ] [MainThread]: 
[0m15:50:31.648843 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m15:50:31.651783 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 6.574121, "process_in_blocks": "0", "process_kernel_time": 0.248809, "process_mem_max_rss": "247201792", "process_out_blocks": "0", "process_user_time": 2.124153}
[0m15:50:31.651974 [debug] [MainThread]: Command `dbt run` failed at 15:50:31.651934 after 6.57 seconds
[0m15:50:31.652138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7630b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c38c5f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c38c7d0>]}
[0m15:50:31.652276 [debug] [MainThread]: Flushing usage events
[0m15:50:32.432188 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:51:20.295970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b83620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4fbb10>]}


============================== 15:51:20.298274 | 6c56cae6-943a-40c9-b70e-d1cd7377ddb8 ==============================
[0m15:51:20.298274 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:51:20.298497 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'debug': 'False', 'write_json': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'empty': 'False', 'partial_parse': 'True', 'quiet': 'False', 'warn_error': 'None', 'static_parser': 'True', 'invocation_command': 'dbt run --select path:models/bronze', 'target_path': 'None', 'version_check': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'printer_width': '80', 'introspect': 'True', 'cache_selected_only': 'False'}
[0m15:51:20.591998 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:51:20.592185 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:51:20.592278 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:51:21.022185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6c56cae6-943a-40c9-b70e-d1cd7377ddb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10932b360>]}
[0m15:51:21.041394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6c56cae6-943a-40c9-b70e-d1cd7377ddb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cc02ad0>]}
[0m15:51:21.041634 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m15:51:21.092592 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:51:21.148676 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:51:21.149047 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/sources.yml
[0m15:51:21.277012 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db
- models.flight_db.silver
- models.flight_db.gold
- models.flight_db.bronze
[0m15:51:21.280553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6c56cae6-943a-40c9-b70e-d1cd7377ddb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c60f550>]}
[0m15:51:21.313849 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:51:21.314893 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:51:21.319841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6c56cae6-943a-40c9-b70e-d1cd7377ddb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c5ee030>]}
[0m15:51:21.319997 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m15:51:21.320094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c56cae6-943a-40c9-b70e-d1cd7377ddb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c408590>]}
[0m15:51:21.320928 [info ] [MainThread]: 
[0m15:51:21.321060 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:51:21.321144 [info ] [MainThread]: 
[0m15:51:21.321368 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:51:21.321461 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:51:21.323703 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m15:51:21.323805 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m15:51:21.327481 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m15:51:21.327615 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m15:51:21.327709 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:51:22.116661 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-3d98-1441-97f9-be3a33a1051c) - Created
[0m15:51:22.575578 [debug] [ThreadPool]: SQL status: OK in 1.250 seconds
[0m15:51:22.589051 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08996-3d98-1441-97f9-be3a33a1051c, command-id=01f08996-3dbd-10b9-9afb-2f3353158a6d) - Closing
[0m15:51:22.589569 [debug] [ThreadPool]: On list_flight_db: Close
[0m15:51:22.589789 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-3d98-1441-97f9-be3a33a1051c) - Closing
[0m15:51:22.816524 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_bronze) - Creating connection
[0m15:51:22.817117 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_bronze'
[0m15:51:22.829488 [debug] [ThreadPool]: Using databricks connection "list_flight_db_bronze"
[0m15:51:22.829928 [debug] [ThreadPool]: On list_flight_db_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'bronze'

  
[0m15:51:22.830218 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:51:23.670880 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-3e85-1133-a7cc-6cc398454926) - Created
[0m15:51:24.148682 [debug] [ThreadPool]: SQL status: OK in 1.320 seconds
[0m15:51:24.152979 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08996-3e85-1133-a7cc-6cc398454926, command-id=01f08996-3ea7-15e1-bbca-e81f6d556da6) - Closing
[0m15:51:24.153824 [debug] [ThreadPool]: On list_flight_db_bronze: Close
[0m15:51:24.154130 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-3e85-1133-a7cc-6cc398454926) - Closing
[0m15:51:24.393684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c56cae6-943a-40c9-b70e-d1cd7377ddb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cb22000>]}
[0m15:51:24.398973 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m15:51:24.399388 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m15:51:24.399656 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m15:51:24.400206 [info ] [Thread-3 (]: 3 of 3 START sql view model bronze.bronze_flights .............................. [RUN]
[0m15:51:24.400553 [info ] [Thread-1 (]: 1 of 3 START sql view model bronze.bronze_airlines ............................. [RUN]
[0m15:51:24.400907 [info ] [Thread-2 (]: 2 of 3 START sql view model bronze.bronze_airports ............................. [RUN]
[0m15:51:24.401479 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m15:51:24.402013 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m15:51:24.402429 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m15:51:24.402709 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m15:51:24.402973 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m15:51:24.403227 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m15:51:24.403506 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m15:51:24.403775 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m15:51:24.404014 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m15:51:24.410643 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m15:51:24.413683 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m15:51:24.416623 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m15:51:24.417456 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m15:51:24.426581 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m15:51:24.427810 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:51:24.428129 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6c56cae6-943a-40c9-b70e-d1cd7377ddb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c81ebd0>]}
[0m15:51:24.428385 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m15:51:24.435894 [debug] [Thread-1 (]: Creating view `flight_db`.`bronze`.`bronze_airlines`
[0m15:51:24.436860 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m15:51:24.437020 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m15:51:24.441368 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m15:51:24.441816 [debug] [Thread-3 (]: Creating view `flight_db`.`bronze`.`bronze_flights`
[0m15:51:24.442765 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m15:51:24.443132 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m15:51:24.443545 [debug] [Thread-2 (]: Creating view `flight_db`.`bronze`.`bronze_airports`
[0m15:51:24.443906 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m15:51:24.444071 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m15:51:24.444185 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m15:51:24.444347 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`flights_dbt`.`airlines`
  )

[0m15:51:24.444508 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`flights_dbt`.`flights`
  )

[0m15:51:24.444656 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:51:24.444768 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m15:51:24.444900 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:51:24.445112 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`flights_dbt`.`airports`
  )

[0m15:51:24.445326 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:51:25.162832 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08996-3f6e-1073-9c98-867154e0565e) - Created
[0m15:51:25.165043 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08996-3f6c-1b8a-a8f7-22bb4b86f81d) - Created
[0m15:51:25.190221 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08996-3f71-144d-ac41-fceba753cec4) - Created
[0m15:51:25.611777 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`flights_dbt`.`flights`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08996-3f8a-1243-9a03-456628290bcf
[0m15:51:25.613901 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m15:51:25.614591 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08996-3f6e-1073-9c98-867154e0565e) - Closing
[0m15:51:25.629065 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`flights_dbt`.`airports`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08996-3f89-1cca-8bda-353f7e0f7edd
[0m15:51:25.687941 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`flights_dbt`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08996-3f8e-15c0-b43d-be09360923f7
[0m15:51:25.818488 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m15:51:25.819266 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08996-3f6c-1b8a-a8f7-22bb4b86f81d) - Closing
[0m15:51:25.830793 [debug] [Thread-3 (]: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m15:51:26.015664 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m15:51:26.016143 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08996-3f71-144d-ac41-fceba753cec4) - Closing
[0m15:51:26.020551 [debug] [Thread-2 (]: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m15:51:26.231806 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c56cae6-943a-40c9-b70e-d1cd7377ddb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e02c730>]}
[0m15:51:26.232345 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c56cae6-943a-40c9-b70e-d1cd7377ddb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c9cbe30>]}
[0m15:51:26.233525 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:51:26.234230 [error] [Thread-3 (]: 3 of 3 ERROR creating sql view model bronze.bronze_flights ..................... [[31mERROR[0m in 1.82s]
[0m15:51:26.234705 [error] [Thread-2 (]: 2 of 3 ERROR creating sql view model bronze.bronze_airports .................... [[31mERROR[0m in 1.82s]
[0m15:51:26.235194 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c56cae6-943a-40c9-b70e-d1cd7377ddb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c42e1d0>]}
[0m15:51:26.235697 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m15:51:26.236090 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m15:51:26.237028 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_flights' to be skipped because of status 'error'.  Reason: Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql.
[0m15:51:26.236548 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model bronze.bronze_airlines .................... [[31mERROR[0m in 1.83s]
[0m15:51:26.238316 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airports' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql.
[0m15:51:26.238913 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m15:51:26.239441 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m15:51:26.240783 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:51:26.241112 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:51:26.241546 [info ] [MainThread]: 
[0m15:51:26.241840 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 4.92 seconds (4.92s).
[0m15:51:26.242711 [debug] [MainThread]: Command end result
[0m15:51:26.271454 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:51:26.272399 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:51:26.276231 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m15:51:26.276373 [info ] [MainThread]: 
[0m15:51:26.276520 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m15:51:26.276637 [info ] [MainThread]: 
[0m15:51:26.276781 [error] [MainThread]: [31mFailure in model bronze_flights (models/bronze/bronze_flights.sql)[0m
[0m15:51:26.276931 [error] [MainThread]:   Database Error in model bronze_flights (models/bronze/bronze_flights.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_flights.sql
[0m15:51:26.277038 [info ] [MainThread]: 
[0m15:51:26.277157 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_flights.sql
[0m15:51:26.277252 [info ] [MainThread]: 
[0m15:51:26.277372 [error] [MainThread]: [31mFailure in model bronze_airports (models/bronze/bronze_airports.sql)[0m
[0m15:51:26.277504 [error] [MainThread]:   Database Error in model bronze_airports (models/bronze/bronze_airports.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airports` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 14 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airports.sql
[0m15:51:26.277606 [info ] [MainThread]: 
[0m15:51:26.277722 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airports.sql
[0m15:51:26.277812 [info ] [MainThread]: 
[0m15:51:26.277931 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m15:51:26.278060 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`flights_dbt`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:51:26.278159 [info ] [MainThread]: 
[0m15:51:26.278272 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:51:26.278365 [info ] [MainThread]: 
[0m15:51:26.278480 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=3
[0m15:51:26.281384 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 6.01577, "process_in_blocks": "0", "process_kernel_time": 0.25118, "process_mem_max_rss": "242221056", "process_out_blocks": "0", "process_user_time": 1.654414}
[0m15:51:26.281575 [debug] [MainThread]: Command `dbt run` failed at 15:51:26.281539 after 6.02 seconds
[0m15:51:26.281735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a539ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10471c4b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ebb230>]}
[0m15:51:26.281871 [debug] [MainThread]: Flushing usage events
[0m15:51:27.013708 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:53:37.622902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10440f620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055e7890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055e7b10>]}


============================== 15:53:37.625181 | 42b04adf-525f-41d1-93b5-e3582ec79d09 ==============================
[0m15:53:37.625181 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:53:37.625422 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'debug': 'False', 'quiet': 'False', 'use_experimental_parser': 'False', 'warn_error': 'None', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'cache_selected_only': 'False', 'use_colors': 'True', 'no_print': 'None', 'partial_parse': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'version_check': 'True', 'log_format': 'default', 'static_parser': 'True', 'write_json': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select path:models/bronze', 'indirect_selection': 'eager', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'printer_width': '80'}
[0m15:53:37.919533 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:53:37.919732 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:53:37.919826 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:53:38.352278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104917360>]}
[0m15:53:38.371566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067f2ad0>]}
[0m15:53:38.371800 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m15:53:38.423032 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:53:38.481261 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:53:38.481674 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/sources.yml
[0m15:53:38.612814 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db
- models.flight_db.silver
- models.flight_db.gold
- models.flight_db.bronze
[0m15:53:38.616438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c36b550>]}
[0m15:53:38.650884 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:53:38.651998 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:53:38.657403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c34e300>]}
[0m15:53:38.657563 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m15:53:38.657668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c168590>]}
[0m15:53:38.658420 [info ] [MainThread]: 
[0m15:53:38.658536 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:53:38.658621 [info ] [MainThread]: 
[0m15:53:38.658802 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:53:38.658894 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:53:38.661128 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m15:53:38.661244 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m15:53:38.664660 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m15:53:38.664766 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m15:53:38.664846 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:53:39.482588 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-8f78-1ef8-9a6a-5789b6127e01) - Created
[0m15:53:39.861500 [debug] [ThreadPool]: SQL status: OK in 1.200 seconds
[0m15:53:39.870844 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08996-8f78-1ef8-9a6a-5789b6127e01, command-id=01f08996-8f99-1bef-a43a-f14bf4152621) - Closing
[0m15:53:39.871564 [debug] [ThreadPool]: On list_flight_db: Close
[0m15:53:39.871869 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-8f78-1ef8-9a6a-5789b6127e01) - Closing
[0m15:53:40.099004 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_bronze) - Creating connection
[0m15:53:40.099968 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_bronze'
[0m15:53:40.112011 [debug] [ThreadPool]: Using databricks connection "list_flight_db_bronze"
[0m15:53:40.112421 [debug] [ThreadPool]: On list_flight_db_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'bronze'

  
[0m15:53:40.112634 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:53:40.951340 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-9058-11ac-a41f-d624ba546b3d) - Created
[0m15:53:41.636620 [debug] [ThreadPool]: SQL status: OK in 1.520 seconds
[0m15:53:41.639763 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08996-9058-11ac-a41f-d624ba546b3d, command-id=01f08996-9079-1cc1-958a-7865239f46d7) - Closing
[0m15:53:41.640608 [debug] [ThreadPool]: On list_flight_db_bronze: Close
[0m15:53:41.641066 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08996-9058-11ac-a41f-d624ba546b3d) - Closing
[0m15:53:41.886607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c792000>]}
[0m15:53:41.891002 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m15:53:41.891527 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m15:53:41.891910 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m15:53:41.892536 [info ] [Thread-1 (]: 1 of 3 START sql view model bronze.bronze_airlines ............................. [RUN]
[0m15:53:41.893162 [info ] [Thread-2 (]: 2 of 3 START sql view model bronze.bronze_airports ............................. [RUN]
[0m15:53:41.893741 [info ] [Thread-3 (]: 3 of 3 START sql view model bronze.bronze_flights .............................. [RUN]
[0m15:53:41.894450 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m15:53:41.894954 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m15:53:41.895368 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m15:53:41.895697 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m15:53:41.895989 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m15:53:41.896260 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m15:53:41.896583 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m15:53:41.896872 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m15:53:41.897153 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m15:53:41.905316 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m15:53:41.910265 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m15:53:41.912308 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m15:53:41.913401 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m15:53:41.913725 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m15:53:41.914020 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m15:53:41.924385 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m15:53:41.925435 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m15:53:41.926556 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m15:53:41.927704 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:53:41.927975 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:53:41.928206 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:53:41.928434 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c44f050>]}
[0m15:53:41.928595 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c86fa10>]}
[0m15:53:41.928738 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c86f5f0>]}
[0m15:53:41.939779 [debug] [Thread-2 (]: Creating view `flight_db`.`bronze`.`bronze_airports`
[0m15:53:41.940033 [debug] [Thread-3 (]: Creating view `flight_db`.`bronze`.`bronze_flights`
[0m15:53:41.940462 [debug] [Thread-1 (]: Creating view `flight_db`.`bronze`.`bronze_airlines`
[0m15:53:41.945003 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m15:53:41.945377 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m15:53:41.945723 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m15:53:41.946319 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m15:53:41.946514 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m15:53:41.946635 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m15:53:41.946781 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

[0m15:53:41.946952 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m15:53:41.947109 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m15:53:41.947246 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:53:41.947367 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:53:41.947485 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:53:42.621530 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08996-915c-1201-b934-fb910132aeb1) - Created
[0m15:53:42.679242 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08996-9164-1f87-9c49-da20b8fbc395) - Created
[0m15:53:42.754287 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08996-916c-110e-8e62-71e9edb6e234) - Created
[0m15:53:43.217201 [debug] [Thread-2 (]: SQL status: OK in 1.270 seconds
[0m15:53:43.219689 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08996-915c-1201-b934-fb910132aeb1, command-id=01f08996-9176-1e59-b133-b09d535304e9) - Closing
[0m15:53:43.231103 [debug] [Thread-2 (]: Applying tags to relation None
[0m15:53:43.234450 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m15:53:43.234704 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08996-915c-1201-b934-fb910132aeb1) - Closing
[0m15:53:43.315649 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08996-918c-1f3d-b68b-e8e5bcb82191
[0m15:53:43.366488 [debug] [Thread-3 (]: SQL status: OK in 1.420 seconds
[0m15:53:43.368849 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08996-9164-1f87-9c49-da20b8fbc395, command-id=01f08996-9181-1faa-8766-674a5ba19351) - Closing
[0m15:53:43.370040 [debug] [Thread-3 (]: Applying tags to relation None
[0m15:53:43.437392 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m15:53:43.438315 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08996-916c-110e-8e62-71e9edb6e234) - Closing
[0m15:53:43.682805 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m15:53:43.683897 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08996-9164-1f87-9c49-da20b8fbc395) - Closing
[0m15:53:43.695886 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:53:43.899742 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c897bd0>]}
[0m15:53:43.900282 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cd77f50>]}
[0m15:53:43.900589 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '42b04adf-525f-41d1-93b5-e3582ec79d09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7958d0>]}
[0m15:53:43.901452 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model bronze.bronze_airlines .................... [[31mERROR[0m in 2.00s]
[0m15:53:43.902070 [info ] [Thread-2 (]: 2 of 3 OK created sql view model bronze.bronze_airports ........................ [[32mOK[0m in 2.00s]
[0m15:53:43.903231 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m15:53:43.902622 [info ] [Thread-3 (]: 3 of 3 OK created sql view model bronze.bronze_flights ......................... [[32mOK[0m in 2.00s]
[0m15:53:43.903779 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m15:53:43.904326 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m15:53:43.904789 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m15:53:43.907189 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:53:43.907478 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:53:43.907931 [info ] [MainThread]: 
[0m15:53:43.908202 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 5.25 seconds (5.25s).
[0m15:53:43.908969 [debug] [MainThread]: Command end result
[0m15:53:43.940266 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:53:43.941273 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:53:43.944447 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m15:53:43.944591 [info ] [MainThread]: 
[0m15:53:43.944742 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:53:43.944859 [info ] [MainThread]: 
[0m15:53:43.945007 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m15:53:43.945158 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:53:43.945268 [info ] [MainThread]: 
[0m15:53:43.945392 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m15:53:43.945492 [info ] [MainThread]: 
[0m15:53:43.945620 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m15:53:43.948798 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 6.355151, "process_in_blocks": "0", "process_kernel_time": 0.254921, "process_mem_max_rss": "245350400", "process_out_blocks": "0", "process_user_time": 1.662297}
[0m15:53:43.949111 [debug] [MainThread]: Command `dbt run` failed at 15:53:43.949060 after 6.36 seconds
[0m15:53:43.949304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102e48ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cd8bb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cd8bb30>]}
[0m15:53:43.949456 [debug] [MainThread]: Flushing usage events
[0m15:53:44.702623 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:57:56.112718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103ba7620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051fbb10>]}


============================== 15:57:56.115055 | 9e52d6af-f9b5-443b-ab31-76e3c7978658 ==============================
[0m15:57:56.115055 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:57:56.115288 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'invocation_command': 'dbt run --select path:models/bronze --exclude bronze_airlines', 'fail_fast': 'False', 'partial_parse': 'True', 'debug': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'log_format': 'default', 'printer_width': '80', 'version_check': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'quiet': 'False', 'empty': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'no_print': 'None'}
[0m15:57:56.407753 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:57:56.407934 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:57:56.408028 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:57:56.869330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9e52d6af-f9b5-443b-ab31-76e3c7978658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10402b360>]}
[0m15:57:56.888685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9e52d6af-f9b5-443b-ab31-76e3c7978658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10790aad0>]}
[0m15:57:56.888935 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m15:57:56.938941 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:57:56.996459 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:57:56.996630 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:57:56.999591 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db.gold
- models.flight_db.bronze
- models.flight_db
- models.flight_db.silver
[0m15:57:57.014323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9e52d6af-f9b5-443b-ab31-76e3c7978658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed0ad50>]}
[0m15:57:57.049972 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:57:57.050913 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:57:57.055891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9e52d6af-f9b5-443b-ab31-76e3c7978658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e807890>]}
[0m15:57:57.056062 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m15:57:57.056170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e52d6af-f9b5-443b-ab31-76e3c7978658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eef7bd0>]}
[0m15:57:57.056968 [info ] [MainThread]: 
[0m15:57:57.057085 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:57:57.057167 [info ] [MainThread]: 
[0m15:57:57.057355 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:57:57.057445 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:57:57.059703 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m15:57:57.059817 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m15:57:57.063812 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m15:57:57.063925 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m15:57:57.064009 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:57:57.782841 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-2972-18d8-a82b-3fa32a37fd18) - Created
[0m15:57:58.115535 [debug] [ThreadPool]: SQL status: OK in 1.050 seconds
[0m15:57:58.125465 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08997-2972-18d8-a82b-3fa32a37fd18, command-id=01f08997-298f-115f-a7fd-f6b3534cd46c) - Closing
[0m15:57:58.126001 [debug] [ThreadPool]: On list_flight_db: Close
[0m15:57:58.126231 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-2972-18d8-a82b-3fa32a37fd18) - Closing
[0m15:57:58.319093 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_bronze) - Creating connection
[0m15:57:58.319728 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_bronze'
[0m15:57:58.332430 [debug] [ThreadPool]: Using databricks connection "list_flight_db_bronze"
[0m15:57:58.332915 [debug] [ThreadPool]: On list_flight_db_bronze: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'bronze'

  
[0m15:57:58.333181 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:57:59.004502 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-2a2e-10fd-bd07-649c49ed2878) - Created
[0m15:57:59.474353 [debug] [ThreadPool]: SQL status: OK in 1.140 seconds
[0m15:57:59.478011 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08997-2a2e-10fd-bd07-649c49ed2878, command-id=01f08997-2a49-15df-a99c-ec880d53a403) - Closing
[0m15:57:59.478844 [debug] [ThreadPool]: On list_flight_db_bronze: Close
[0m15:57:59.479151 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-2a2e-10fd-bd07-649c49ed2878) - Closing
[0m15:57:59.686187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e52d6af-f9b5-443b-ab31-76e3c7978658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eff46d0>]}
[0m15:57:59.691408 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airports
[0m15:57:59.692141 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_flights
[0m15:57:59.692926 [info ] [Thread-1 (]: 1 of 2 START sql view model bronze.bronze_airports ............................. [RUN]
[0m15:57:59.693637 [info ] [Thread-2 (]: 2 of 2 START sql view model bronze.bronze_flights .............................. [RUN]
[0m15:57:59.694702 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m15:57:59.695335 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m15:57:59.695689 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m15:57:59.695984 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m15:57:59.696323 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airports
[0m15:57:59.696600 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_flights
[0m15:57:59.705000 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m15:57:59.709547 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m15:57:59.710528 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airports
[0m15:57:59.710827 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_flights
[0m15:57:59.728746 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m15:57:59.730626 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m15:57:59.731633 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:57:59.732145 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '9e52d6af-f9b5-443b-ab31-76e3c7978658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f020dd0>]}
[0m15:57:59.731899 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:57:59.738713 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '9e52d6af-f9b5-443b-ab31-76e3c7978658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10effe0a0>]}
[0m15:57:59.739635 [debug] [Thread-1 (]: Creating view `flight_db`.`bronze`.`bronze_airports`
[0m15:57:59.740045 [debug] [Thread-2 (]: Creating view `flight_db`.`bronze`.`bronze_flights`
[0m15:57:59.744285 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m15:57:59.744620 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m15:57:59.745160 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m15:57:59.745310 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m15:57:59.745463 [debug] [Thread-2 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m15:57:59.745615 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`bronze`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m15:57:59.745743 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:57:59.745859 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:58:00.392621 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08997-2b03-1bf7-809f-3b1364e358c4) - Created
[0m15:58:00.401280 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08997-2b04-1958-b3f3-de79b09c7280) - Created
[0m15:58:01.090160 [debug] [Thread-2 (]: SQL status: OK in 1.340 seconds
[0m15:58:01.091788 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08997-2b03-1bf7-809f-3b1364e358c4, command-id=01f08997-2b1c-1fa6-800f-a7d18f5b8915) - Closing
[0m15:58:01.105222 [debug] [Thread-2 (]: Applying tags to relation None
[0m15:58:01.107680 [debug] [Thread-2 (]: On model.flights_dbt.bronze_flights: Close
[0m15:58:01.108014 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08997-2b03-1bf7-809f-3b1364e358c4) - Closing
[0m15:58:01.109893 [debug] [Thread-1 (]: SQL status: OK in 1.360 seconds
[0m15:58:01.110848 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f08997-2b04-1958-b3f3-de79b09c7280, command-id=01f08997-2b20-1c45-a1fb-4058791fc884) - Closing
[0m15:58:01.111430 [debug] [Thread-1 (]: Applying tags to relation None
[0m15:58:01.308670 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airports: Close
[0m15:58:01.309484 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08997-2b04-1958-b3f3-de79b09c7280) - Closing
[0m15:58:01.518033 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e52d6af-f9b5-443b-ab31-76e3c7978658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f0becb0>]}
[0m15:58:01.518616 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e52d6af-f9b5-443b-ab31-76e3c7978658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f08ed50>]}
[0m15:58:01.519539 [info ] [Thread-1 (]: 1 of 2 OK created sql view model bronze.bronze_airports ........................ [[32mOK[0m in 1.82s]
[0m15:58:01.520853 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airports
[0m15:58:01.520299 [info ] [Thread-2 (]: 2 of 2 OK created sql view model bronze.bronze_flights ......................... [[32mOK[0m in 1.82s]
[0m15:58:01.521665 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_flights
[0m15:58:01.523536 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:58:01.523919 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:58:01.524434 [info ] [MainThread]: 
[0m15:58:01.524814 [info ] [MainThread]: Finished running 2 view models in 0 hours 0 minutes and 4.47 seconds (4.47s).
[0m15:58:01.525600 [debug] [MainThread]: Command end result
[0m15:58:01.554276 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m15:58:01.555443 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m15:58:01.558935 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m15:58:01.559089 [info ] [MainThread]: 
[0m15:58:01.559290 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:58:01.559411 [info ] [MainThread]: 
[0m15:58:01.559558 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m15:58:01.562375 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.479676, "process_in_blocks": "0", "process_kernel_time": 0.246446, "process_mem_max_rss": "234651648", "process_out_blocks": "0", "process_user_time": 1.409426}
[0m15:58:01.562616 [debug] [MainThread]: Command `dbt run` succeeded at 15:58:01.562572 after 5.48 seconds
[0m15:58:01.562795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105239f60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1007184b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed7fe90>]}
[0m15:58:01.562944 [debug] [MainThread]: Flushing usage events
[0m15:58:02.302697 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:00:31.611646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bcb620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104be3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104be3b10>]}


============================== 16:00:31.614019 | 755505e1-db61-42a7-a1d0-c555c149b023 ==============================
[0m16:00:31.614019 [info ] [MainThread]: Running with dbt=1.10.9
[0m16:00:31.614258 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'cache_selected_only': 'False', 'quiet': 'False', 'write_json': 'True', 'introspect': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'empty': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'target_path': 'None', 'no_print': 'None', 'static_parser': 'True', 'invocation_command': 'dbt run --select path:models/bronze', 'use_colors': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'debug': 'False', 'partial_parse': 'True'}
[0m16:00:31.910346 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:00:31.910528 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:00:31.910625 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:00:32.351868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f13360>]}
[0m16:00:32.371221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105deaad0>]}
[0m16:00:32.371450 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m16:00:32.425815 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m16:00:32.463193 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:00:32.463373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba8f450>]}
[0m16:00:33.114451 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db
- models.flight_db.bronze
- models.flight_db.silver
- models.flight_db.gold
[0m16:00:33.118293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b693200>]}
[0m16:00:33.151450 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:00:33.152476 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:00:33.158009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba990f0>]}
[0m16:00:33.158169 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m16:00:33.158273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd13860>]}
[0m16:00:33.159005 [info ] [MainThread]: 
[0m16:00:33.159118 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:00:33.159205 [info ] [MainThread]: 
[0m16:00:33.159375 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:00:33.159464 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:00:33.161804 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m16:00:33.161924 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m16:00:33.165403 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m16:00:33.165541 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m16:00:33.165630 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:00:33.848308 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-867a-15f8-90c5-f732758ff048) - Created
[0m16:00:34.318919 [debug] [ThreadPool]: SQL status: OK in 1.150 seconds
[0m16:00:34.327631 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08997-867a-15f8-90c5-f732758ff048, command-id=01f08997-8694-1198-86f8-ee0a5df613fc) - Closing
[0m16:00:34.328214 [debug] [ThreadPool]: On list_flight_db: Close
[0m16:00:34.328483 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-867a-15f8-90c5-f732758ff048) - Closing
[0m16:00:34.529517 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m16:00:34.530246 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m16:00:34.544396 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m16:00:34.544772 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m16:00:34.545021 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:00:35.204046 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-8747-1667-bf18-78312561c605) - Created
[0m16:00:35.701942 [debug] [ThreadPool]: SQL status: OK in 1.160 seconds
[0m16:00:35.705234 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08997-8747-1667-bf18-78312561c605, command-id=01f08997-8762-19b4-ac81-adc1ec42da2e) - Closing
[0m16:00:35.705780 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m16:00:35.706015 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-8747-1667-bf18-78312561c605) - Closing
[0m16:00:35.895444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be8aed0>]}
[0m16:00:35.900992 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m16:00:35.901475 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m16:00:35.901804 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m16:00:35.902335 [info ] [Thread-1 (]: 1 of 3 START sql view model raw.bronze_airlines ................................ [RUN]
[0m16:00:35.902797 [info ] [Thread-2 (]: 2 of 3 START sql view model raw.bronze_airports ................................ [RUN]
[0m16:00:35.903253 [info ] [Thread-3 (]: 3 of 3 START sql view model raw.bronze_flights ................................. [RUN]
[0m16:00:35.903887 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m16:00:35.904349 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m16:00:35.904721 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m16:00:35.905006 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m16:00:35.905235 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m16:00:35.905457 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m16:00:35.905706 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m16:00:35.905945 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m16:00:35.906183 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m16:00:35.913793 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m16:00:35.916751 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m16:00:35.919911 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m16:00:35.920797 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m16:00:35.921057 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m16:00:35.921335 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m16:00:35.931190 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m16:00:35.932300 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m16:00:35.933305 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m16:00:35.934450 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:00:35.934726 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:00:35.934948 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:00:35.935146 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c074cb0>]}
[0m16:00:35.935292 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b853390>]}
[0m16:00:35.935434 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8518b0>]}
[0m16:00:35.943384 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m16:00:35.944162 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m16:00:35.944545 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m16:00:35.948579 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m16:00:35.948915 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m16:00:35.949211 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m16:00:35.949544 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m16:00:35.949685 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m16:00:35.949787 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m16:00:35.949923 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m16:00:35.950068 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m16:00:35.950205 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

[0m16:00:35.950325 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:00:35.950435 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:00:35.950541 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:00:36.644426 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08997-8823-1680-a133-d0e0f924786f) - Created
[0m16:00:36.656583 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08997-8825-1d63-9452-5acef5716063) - Created
[0m16:00:36.674812 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08997-8829-1077-ad06-60f1eb1e7167) - Created
[0m16:00:37.050675 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08997-8841-1079-8c07-18b05f4bf44b
[0m16:00:37.051866 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m16:00:37.052180 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08997-8823-1680-a133-d0e0f924786f) - Closing
[0m16:00:37.158331 [debug] [Thread-2 (]: SQL status: OK in 1.210 seconds
[0m16:00:37.160882 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08997-8825-1d63-9452-5acef5716063, command-id=01f08997-8842-15a5-9d54-56b587579d21) - Closing
[0m16:00:37.169801 [debug] [Thread-2 (]: Applying tags to relation None
[0m16:00:37.183659 [debug] [Thread-3 (]: SQL status: OK in 1.230 seconds
[0m16:00:37.184279 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08997-8829-1077-ad06-60f1eb1e7167, command-id=01f08997-8844-1d46-b3bf-e6f33d6befa8) - Closing
[0m16:00:37.184690 [debug] [Thread-3 (]: Applying tags to relation None
[0m16:00:37.258629 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m16:00:37.259246 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08997-8825-1d63-9452-5acef5716063) - Closing
[0m16:00:37.271162 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m16:00:37.856217 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m16:00:37.857313 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08997-8829-1077-ad06-60f1eb1e7167) - Closing
[0m16:00:38.068927 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c5269e0>]}
[0m16:00:38.069559 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c526c10>]}
[0m16:00:38.069903 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '755505e1-db61-42a7-a1d0-c555c149b023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c00a0f0>]}
[0m16:00:38.070744 [info ] [Thread-2 (]: 2 of 3 OK created sql view model raw.bronze_airports ........................... [[32mOK[0m in 2.16s]
[0m16:00:38.071494 [info ] [Thread-3 (]: 3 of 3 OK created sql view model raw.bronze_flights ............................ [[32mOK[0m in 2.16s]
[0m16:00:38.072635 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m16:00:38.072019 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model raw.bronze_airlines ....................... [[31mERROR[0m in 2.16s]
[0m16:00:38.073178 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m16:00:38.073866 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m16:00:38.074425 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m16:00:38.076700 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:00:38.077016 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:00:38.077467 [info ] [MainThread]: 
[0m16:00:38.077744 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 4.92 seconds (4.92s).
[0m16:00:38.078548 [debug] [MainThread]: Command end result
[0m16:00:38.103056 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:00:38.104046 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:00:38.107245 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m16:00:38.107393 [info ] [MainThread]: 
[0m16:00:38.107559 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m16:00:38.107691 [info ] [MainThread]: 
[0m16:00:38.107861 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m16:00:38.108027 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m16:00:38.108159 [info ] [MainThread]: 
[0m16:00:38.108301 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m16:00:38.108416 [info ] [MainThread]: 
[0m16:00:38.108555 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m16:00:38.111625 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 6.532, "process_in_blocks": "0", "process_kernel_time": 0.252797, "process_mem_max_rss": "246497280", "process_out_blocks": "0", "process_user_time": 2.198606}
[0m16:00:38.111820 [debug] [MainThread]: Command `dbt run` failed at 16:00:38.111777 after 6.53 seconds
[0m16:00:38.111988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102ad5790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c55f9b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c55f950>]}
[0m16:00:38.112143 [debug] [MainThread]: Flushing usage events
[0m16:00:38.872465 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:01:35.120918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f23620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096f3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096f3b10>]}


============================== 16:01:35.123276 | 0d7fb354-a10f-4f97-ad0f-7d895956ec84 ==============================
[0m16:01:35.123276 [info ] [MainThread]: Running with dbt=1.10.9
[0m16:01:35.123519 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'static_parser': 'True', 'log_format': 'default', 'empty': 'None', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'fail_fast': 'False', 'introspect': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'version_check': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt debug', 'indirect_selection': 'eager', 'warn_error': 'None', 'quiet': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'printer_width': '80', 'no_print': 'None', 'partial_parse': 'True'}
[0m16:01:35.129570 [info ] [MainThread]: dbt version: 1.10.9
[0m16:01:35.129711 [info ] [MainThread]: python version: 3.13.0
[0m16:01:35.129803 [info ] [MainThread]: python path: /Users/artakerqeli/flights_dbt/.venv/bin/python3.13
[0m16:01:35.129885 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m16:01:35.428733 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:01:35.428935 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:01:35.429032 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:01:35.805312 [info ] [MainThread]: Using profiles dir at /Users/artakerqeli/.dbt
[0m16:01:35.805577 [info ] [MainThread]: Using profiles.yml file at /Users/artakerqeli/.dbt/profiles.yml
[0m16:01:35.805689 [info ] [MainThread]: Using dbt_project.yml file at /Users/artakerqeli/flights_dbt/flights_dbt/dbt_project.yml
[0m16:01:35.805781 [info ] [MainThread]: adapter type: databricks
[0m16:01:35.805864 [info ] [MainThread]: adapter version: 1.10.11
[0m16:01:35.840088 [info ] [MainThread]: Configuration:
[0m16:01:35.840308 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m16:01:35.840417 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m16:01:35.840500 [info ] [MainThread]: Required dependencies:
[0m16:01:35.840624 [debug] [MainThread]: Executing "git --help"
[0m16:01:35.857455 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:01:35.857853 [debug] [MainThread]: STDERR: "b''"
[0m16:01:35.857967 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m16:01:35.858063 [info ] [MainThread]: Connection:
[0m16:01:35.858191 [info ] [MainThread]:   host: https://dbc-8fd4d429-f882.cloud.databricks.com
[0m16:01:35.858282 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/2444393c45259518
[0m16:01:35.858360 [info ] [MainThread]:   catalog: flight_db
[0m16:01:35.858437 [info ] [MainThread]:   schema: raw
[0m16:01:35.858721 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m16:01:35.901753 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m16:01:35.901999 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m16:01:35.902111 [debug] [MainThread]: Using databricks connection "debug"
[0m16:01:35.902207 [debug] [MainThread]: On debug: select 1 as id
[0m16:01:35.902292 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:01:36.549952 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f08997-abd8-1c7d-b00a-b6df7b36e8db) - Created
[0m16:01:36.849474 [debug] [MainThread]: SQL status: OK in 0.950 seconds
[0m16:01:36.851344 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f08997-abd8-1c7d-b00a-b6df7b36e8db, command-id=01f08997-abf7-1640-9bdc-d1a325dddac7) - Closing
[0m16:01:36.852239 [debug] [MainThread]: On debug: Close
[0m16:01:36.852615 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f08997-abd8-1c7d-b00a-b6df7b36e8db) - Closing
[0m16:01:37.072594 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m16:01:37.073434 [info ] [MainThread]: [32mAll checks passed![0m
[0m16:01:37.081648 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 1.9894836, "process_in_blocks": "0", "process_kernel_time": 0.213984, "process_mem_max_rss": "224378880", "process_out_blocks": "0", "process_user_time": 0.985849}
[0m16:01:37.082710 [debug] [MainThread]: Command `dbt debug` succeeded at 16:01:37.082529 after 1.99 seconds
[0m16:01:37.083301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b51dba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b4f7410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b26bf00>]}
[0m16:01:37.083818 [debug] [MainThread]: Flushing usage events
[0m16:01:37.704588 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:01:41.917325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108153620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10932b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10932bb10>]}


============================== 16:01:41.919422 | 63ebe742-37bd-47a1-b231-663f9d0c8ad9 ==============================
[0m16:01:41.919422 [info ] [MainThread]: Running with dbt=1.10.9
[0m16:01:41.919648 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'invocation_command': 'dbt run --select path:models/bronze', 'write_json': 'True', 'static_parser': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'introspect': 'True', 'target_path': 'None', 'use_colors': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'no_print': 'None', 'indirect_selection': 'eager', 'warn_error': 'None', 'log_format': 'default', 'quiet': 'False', 'empty': 'False', 'debug': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80'}
[0m16:01:42.220994 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:01:42.221191 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:01:42.221286 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:01:42.527448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108653360>]}
[0m16:01:42.547093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a536ad0>]}
[0m16:01:42.547332 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m16:01:42.598427 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m16:01:42.653602 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:01:42.653763 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:01:42.656802 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db.silver
- models.flight_db.gold
- models.flight_db.bronze
- models.flight_db
[0m16:01:42.671847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100aad50>]}
[0m16:01:42.707342 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:01:42.708322 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:01:42.713056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcab890>]}
[0m16:01:42.713218 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m16:01:42.713319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11029b3f0>]}
[0m16:01:42.714083 [info ] [MainThread]: 
[0m16:01:42.714201 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:01:42.714290 [info ] [MainThread]: 
[0m16:01:42.714481 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:01:42.714574 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:01:42.717133 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m16:01:42.717289 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m16:01:42.721741 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m16:01:42.721873 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m16:01:42.721979 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:01:43.394479 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-afed-1f2c-8b5e-c7a7e6e5f055) - Created
[0m16:01:43.926545 [debug] [ThreadPool]: SQL status: OK in 1.200 seconds
[0m16:01:43.938045 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08997-afed-1f2c-8b5e-c7a7e6e5f055, command-id=01f08997-b00b-1ec0-83e1-1e1530201156) - Closing
[0m16:01:43.938804 [debug] [ThreadPool]: On list_flight_db: Close
[0m16:01:43.939099 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-afed-1f2c-8b5e-c7a7e6e5f055) - Closing
[0m16:01:44.150717 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m16:01:44.151345 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m16:01:44.163372 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m16:01:44.163991 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m16:01:44.164320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:01:44.856354 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-b0c9-1fa4-a42c-4df8be8a0351) - Created
[0m16:01:45.361022 [debug] [ThreadPool]: SQL status: OK in 1.200 seconds
[0m16:01:45.365273 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08997-b0c9-1fa4-a42c-4df8be8a0351, command-id=01f08997-b0e8-197a-8498-5d08b6dd9765) - Closing
[0m16:01:45.366107 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m16:01:45.366350 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-b0c9-1fa4-a42c-4df8be8a0351) - Closing
[0m16:01:45.575545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110363380>]}
[0m16:01:45.580627 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m16:01:45.581066 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m16:01:45.581388 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m16:01:45.582009 [info ] [Thread-1 (]: 1 of 3 START sql view model raw.bronze_airlines ................................ [RUN]
[0m16:01:45.582482 [info ] [Thread-2 (]: 2 of 3 START sql view model raw.bronze_airports ................................ [RUN]
[0m16:01:45.582923 [info ] [Thread-3 (]: 3 of 3 START sql view model raw.bronze_flights ................................. [RUN]
[0m16:01:45.583604 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m16:01:45.584115 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m16:01:45.584539 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m16:01:45.584882 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m16:01:45.585183 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m16:01:45.585426 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m16:01:45.585708 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m16:01:45.585963 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m16:01:45.586197 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m16:01:45.596447 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m16:01:45.598943 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m16:01:45.600967 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m16:01:45.601680 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m16:01:45.601933 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m16:01:45.602170 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m16:01:45.612667 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m16:01:45.613723 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m16:01:45.614896 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m16:01:45.616094 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:01:45.616375 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:01:45.616911 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110231490>]}
[0m16:01:45.616659 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:01:45.617111 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110365ff0>]}
[0m16:01:45.623636 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110421590>]}
[0m16:01:45.624810 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m16:01:45.625231 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m16:01:45.625587 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m16:01:45.630047 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m16:01:45.630433 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m16:01:45.630745 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m16:01:45.631040 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m16:01:45.631156 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m16:01:45.631279 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m16:01:45.631445 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m16:01:45.631597 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

[0m16:01:45.631747 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m16:01:45.631889 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:01:45.632006 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:01:45.632121 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:01:46.312229 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08997-b1aa-1526-8150-26536ca04237) - Created
[0m16:01:46.314167 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08997-b1aa-161d-9578-b1571092cd72) - Created
[0m16:01:46.362409 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08997-b1b2-1bd4-9236-fd84335a5501) - Created
[0m16:01:46.955967 [debug] [Thread-2 (]: SQL status: OK in 1.320 seconds
[0m16:01:46.957763 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08997-b1aa-1526-8150-26536ca04237, command-id=01f08997-b1c5-1bd4-8acb-a55c538eb367) - Closing
[0m16:01:46.972440 [debug] [Thread-2 (]: Applying tags to relation None
[0m16:01:46.975829 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m16:01:46.976156 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08997-b1aa-1526-8150-26536ca04237) - Closing
[0m16:01:47.005816 [debug] [Thread-3 (]: SQL status: OK in 1.370 seconds
[0m16:01:47.006786 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08997-b1aa-161d-9578-b1571092cd72, command-id=01f08997-b1c5-1e7c-beb0-89438265e7ca) - Closing
[0m16:01:47.007453 [debug] [Thread-3 (]: Applying tags to relation None
[0m16:01:47.046307 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08997-b1cf-1102-a1d1-499fec0e1d45
[0m16:01:47.172688 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m16:01:47.173817 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08997-b1aa-161d-9578-b1571092cd72) - Closing
[0m16:01:47.357678 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m16:01:47.358781 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08997-b1b2-1bd4-9236-fd84335a5501) - Closing
[0m16:01:47.573631 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1101817d0>]}
[0m16:01:47.574247 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fecd650>]}
[0m16:01:47.575115 [info ] [Thread-3 (]: 3 of 3 OK created sql view model raw.bronze_flights ............................ [[32mOK[0m in 1.99s]
[0m16:01:47.575566 [info ] [Thread-2 (]: 2 of 3 OK created sql view model raw.bronze_airports ........................... [[32mOK[0m in 1.98s]
[0m16:01:47.576192 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m16:01:47.576620 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m16:01:47.585376 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m16:01:47.585799 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63ebe742-37bd-47a1-b231-663f9d0c8ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11048a630>]}
[0m16:01:47.586293 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model raw.bronze_airlines ....................... [[31mERROR[0m in 2.00s]
[0m16:01:47.586710 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m16:01:47.587079 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m16:01:47.588710 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:01:47.588972 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:01:47.589305 [info ] [MainThread]: 
[0m16:01:47.589524 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 4.87 seconds (4.87s).
[0m16:01:47.590141 [debug] [MainThread]: Command end result
[0m16:01:47.609533 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:01:47.610411 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:01:47.613349 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m16:01:47.613483 [info ] [MainThread]: 
[0m16:01:47.613626 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m16:01:47.613741 [info ] [MainThread]: 
[0m16:01:47.613889 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m16:01:47.614033 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m16:01:47.614146 [info ] [MainThread]: 
[0m16:01:47.614261 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m16:01:47.614361 [info ] [MainThread]: 
[0m16:01:47.614482 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m16:01:47.617003 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 5.727992, "process_in_blocks": "0", "process_kernel_time": 0.198252, "process_mem_max_rss": "241483776", "process_out_blocks": "0", "process_user_time": 1.62389}
[0m16:01:47.617348 [debug] [MainThread]: Command `dbt run` failed at 16:01:47.617299 after 5.73 seconds
[0m16:01:47.617612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11048a630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092217f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1101cbbf0>]}
[0m16:01:47.617765 [debug] [MainThread]: Flushing usage events
[0m16:01:48.344303 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:02:47.659241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042a3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052af890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052afb10>]}


============================== 16:02:47.661535 | 118c045b-0920-49fd-9fda-9d0694887556 ==============================
[0m16:02:47.661535 [info ] [MainThread]: Running with dbt=1.10.9
[0m16:02:47.661781 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'printer_width': '80', 'profiles_dir': '/Users/artakerqeli/.dbt', 'indirect_selection': 'eager', 'quiet': 'False', 'introspect': 'True', 'fail_fast': 'False', 'log_cache_events': 'False', 'write_json': 'True', 'log_format': 'default', 'debug': 'False', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'empty': 'False', 'partial_parse': 'True', 'invocation_command': 'dbt run --select path:models/bronze', 'target_path': 'None', 'version_check': 'True', 'warn_error': 'None', 'no_print': 'None', 'static_parser': 'True', 'cache_selected_only': 'False'}
[0m16:02:47.961283 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:02:47.961479 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:02:47.961572 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:02:48.357647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045db360>]}
[0m16:02:48.377029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107506ad0>]}
[0m16:02:48.377266 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m16:02:48.430430 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m16:02:48.486729 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:02:48.486884 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:02:48.489746 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db
- models.flight_db.bronze
- models.flight_db.silver
- models.flight_db.gold
[0m16:02:48.504217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116606d50>]}
[0m16:02:48.540567 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:02:48.541531 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:02:48.546570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115d0b890>]}
[0m16:02:48.546737 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m16:02:48.546858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1167f73f0>]}
[0m16:02:48.547620 [info ] [MainThread]: 
[0m16:02:48.547744 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:02:48.547829 [info ] [MainThread]: 
[0m16:02:48.548017 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:02:48.548108 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:02:48.550453 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m16:02:48.550568 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m16:02:48.554592 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m16:02:48.554708 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m16:02:48.554798 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:49.342542 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-d73a-1db6-bc9a-3a2a5ba7d357) - Created
[0m16:02:49.707102 [debug] [ThreadPool]: SQL status: OK in 1.150 seconds
[0m16:02:49.714923 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08997-d73a-1db6-bc9a-3a2a5ba7d357, command-id=01f08997-d75a-12a9-9d9e-09a8cf7fd266) - Closing
[0m16:02:49.715430 [debug] [ThreadPool]: On list_flight_db: Close
[0m16:02:49.715688 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-d73a-1db6-bc9a-3a2a5ba7d357) - Closing
[0m16:02:49.906498 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m16:02:49.907049 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m16:02:49.920322 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m16:02:49.920718 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m16:02:49.920976 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:50.572808 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-d7f7-1390-90bf-40e7246f0b40) - Created
[0m16:02:51.136770 [debug] [ThreadPool]: SQL status: OK in 1.220 seconds
[0m16:02:51.140352 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08997-d7f7-1390-90bf-40e7246f0b40, command-id=01f08997-d812-1f16-98f9-82a74ef0df0a) - Closing
[0m16:02:51.141474 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m16:02:51.141921 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-d7f7-1390-90bf-40e7246f0b40) - Closing
[0m16:02:51.332477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1178bf380>]}
[0m16:02:51.337600 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m16:02:51.338061 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m16:02:51.338391 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m16:02:51.338993 [info ] [Thread-2 (]: 2 of 3 START sql view model raw.bronze_airports ................................ [RUN]
[0m16:02:51.339437 [info ] [Thread-1 (]: 1 of 3 START sql view model raw.bronze_airlines ................................ [RUN]
[0m16:02:51.339944 [info ] [Thread-3 (]: 3 of 3 START sql view model raw.bronze_flights ................................. [RUN]
[0m16:02:51.340627 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m16:02:51.341098 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m16:02:51.341503 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m16:02:51.341831 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m16:02:51.342112 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m16:02:51.342382 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m16:02:51.342655 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m16:02:51.342908 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m16:02:51.343132 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m16:02:51.353411 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m16:02:51.355926 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m16:02:51.357854 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m16:02:51.358774 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m16:02:51.359065 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m16:02:51.359282 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m16:02:51.369489 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m16:02:51.370555 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m16:02:51.371689 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m16:02:51.372824 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:02:51.373155 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:02:51.373441 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:02:51.373675 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11678d610>]}
[0m16:02:51.373847 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1178c1f40>]}
[0m16:02:51.374004 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1178c1de0>]}
[0m16:02:51.381921 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m16:02:51.382625 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m16:02:51.387113 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m16:02:51.387522 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m16:02:51.387882 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m16:02:51.388253 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m16:02:51.388643 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m16:02:51.388760 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m16:02:51.388888 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m16:02:51.389049 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m16:02:51.389205 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

[0m16:02:51.389365 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m16:02:51.389509 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:02:51.389635 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:02:51.389764 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:02:52.088075 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08997-d8df-16d0-9e67-4f36217addc3) - Created
[0m16:02:52.102359 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08997-d8e2-196d-98e9-9546b667cdd3) - Created
[0m16:02:52.134034 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08997-d8e3-175c-bc5c-04a3b8b90f99) - Created
[0m16:02:52.610193 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    select *
from `flight_db`.`raw`.`airlines`
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f08997-d900-1565-8c21-f6b626cd6aad
[0m16:02:52.612056 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m16:02:52.613063 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08997-d8e3-175c-bc5c-04a3b8b90f99) - Closing
[0m16:02:52.688179 [debug] [Thread-3 (]: SQL status: OK in 1.300 seconds
[0m16:02:52.689880 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08997-d8df-16d0-9e67-4f36217addc3, command-id=01f08997-d8f9-19d7-8dbf-efe05d6f51b5) - Closing
[0m16:02:52.704486 [debug] [Thread-3 (]: Applying tags to relation None
[0m16:02:52.748766 [debug] [Thread-2 (]: SQL status: OK in 1.360 seconds
[0m16:02:52.750211 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08997-d8e2-196d-98e9-9546b667cdd3, command-id=01f08997-d8fc-1ae4-9bef-e2e775e88aa9) - Closing
[0m16:02:52.751032 [debug] [Thread-2 (]: Applying tags to relation None
[0m16:02:52.817882 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m16:02:52.818757 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08997-d8df-16d0-9e67-4f36217addc3) - Closing
[0m16:02:52.830922 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m16:02:53.005098 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m16:02:53.006179 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08997-d8e2-196d-98e9-9546b667cdd3) - Closing
[0m16:02:53.202113 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10459ded0>]}
[0m16:02:53.202669 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1165fe950>]}
[0m16:02:53.202974 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '118c045b-0920-49fd-9fda-9d0694887556', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11640d710>]}
[0m16:02:53.203784 [info ] [Thread-3 (]: 3 of 3 OK created sql view model raw.bronze_flights ............................ [[32mOK[0m in 1.86s]
[0m16:02:53.204379 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model raw.bronze_airlines ....................... [[31mERROR[0m in 1.86s]
[0m16:02:53.205780 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m16:02:53.205225 [info ] [Thread-2 (]: 2 of 3 OK created sql view model raw.bronze_airports ........................... [[32mOK[0m in 1.86s]
[0m16:02:53.206356 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m16:02:53.207127 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m16:02:53.207645 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m16:02:53.210428 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:02:53.210797 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:02:53.211285 [info ] [MainThread]: 
[0m16:02:53.211609 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 4.66 seconds (4.66s).
[0m16:02:53.212621 [debug] [MainThread]: Command end result
[0m16:02:53.246295 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:02:53.247891 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:02:53.251929 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m16:02:53.252109 [info ] [MainThread]: 
[0m16:02:53.252312 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m16:02:53.252454 [info ] [MainThread]: 
[0m16:02:53.252637 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m16:02:53.252812 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m16:02:53.252945 [info ] [MainThread]: 
[0m16:02:53.253085 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m16:02:53.253207 [info ] [MainThread]: 
[0m16:02:53.253355 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m16:02:53.256302 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 5.6310368, "process_in_blocks": "0", "process_kernel_time": 0.242264, "process_mem_max_rss": "239878144", "process_out_blocks": "0", "process_user_time": 1.648936}
[0m16:02:53.256526 [debug] [MainThread]: Command `dbt run` failed at 16:02:53.256484 after 5.63 seconds
[0m16:02:53.256722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c856d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116393650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122243e30>]}
[0m16:02:53.256883 [debug] [MainThread]: Flushing usage events
[0m16:02:54.015878 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:03:54.174625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103aaf620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c87890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c87b10>]}


============================== 16:03:54.176979 | a1949d8c-f77b-4580-a3b1-02371abfd26e ==============================
[0m16:03:54.176979 [info ] [MainThread]: Running with dbt=1.10.9
[0m16:03:54.177204 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'target_path': 'None', 'warn_error': 'None', 'invocation_command': 'dbt run --select path:models/bronze', 'indirect_selection': 'eager', 'introspect': 'True', 'static_parser': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'no_print': 'None', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'empty': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'fail_fast': 'False', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'use_colors': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'write_json': 'True', 'debug': 'False', 'use_experimental_parser': 'False'}
[0m16:03:54.475119 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:03:54.475302 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:03:54.475413 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:03:54.901188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fb3360>]}
[0m16:03:54.920705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e8ead0>]}
[0m16:03:54.920980 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m16:03:54.969694 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m16:03:55.027694 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:03:55.027967 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m16:03:55.115460 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 4 unused configuration paths:
- models.flight_db.gold
- models.flight_db
- models.flight_db.silver
- models.flight_db.bronze
[0m16:03:55.120385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca06950>]}
[0m16:03:55.153652 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:03:55.154707 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:03:55.160112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c50b890>]}
[0m16:03:55.160302 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m16:03:55.160422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cd82270>]}
[0m16:03:55.161228 [info ] [MainThread]: 
[0m16:03:55.161346 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:03:55.161437 [info ] [MainThread]: 
[0m16:03:55.161626 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:03:55.161721 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:03:55.164137 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m16:03:55.164248 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m16:03:55.167602 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m16:03:55.167714 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m16:03:55.167798 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:03:55.811624 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-fedc-11cb-a88e-4c01ed40865f) - Created
[0m16:03:56.173594 [debug] [ThreadPool]: SQL status: OK in 1.010 seconds
[0m16:03:56.183669 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08997-fedc-11cb-a88e-4c01ed40865f, command-id=01f08997-fef9-1214-9d74-066d48601d29) - Closing
[0m16:03:56.184381 [debug] [ThreadPool]: On list_flight_db: Close
[0m16:03:56.184634 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-fedc-11cb-a88e-4c01ed40865f) - Closing
[0m16:03:56.373798 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m16:03:56.374528 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m16:03:56.389289 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m16:03:56.389798 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m16:03:56.390065 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:03:57.065468 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-ff99-1acc-81e6-93efc2e2da1d) - Created
[0m16:03:57.550887 [debug] [ThreadPool]: SQL status: OK in 1.160 seconds
[0m16:03:57.554077 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f08997-ff99-1acc-81e6-93efc2e2da1d, command-id=01f08997-ffb5-1227-bd0e-c35f62b11bcb) - Closing
[0m16:03:57.554933 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m16:03:57.555261 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f08997-ff99-1acc-81e6-93efc2e2da1d) - Closing
[0m16:03:57.755742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cd77450>]}
[0m16:03:57.760479 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m16:03:57.760911 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m16:03:57.761249 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m16:03:57.761847 [info ] [Thread-1 (]: 1 of 3 START sql view model raw.bronze_airlines ................................ [RUN]
[0m16:03:57.762284 [info ] [Thread-3 (]: 3 of 3 START sql view model raw.bronze_flights ................................. [RUN]
[0m16:03:57.762848 [info ] [Thread-2 (]: 2 of 3 START sql view model raw.bronze_airports ................................ [RUN]
[0m16:03:57.763522 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m16:03:57.764074 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m16:03:57.764627 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m16:03:57.765049 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m16:03:57.765373 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m16:03:57.765665 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m16:03:57.765935 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m16:03:57.766180 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m16:03:57.766413 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m16:03:57.774491 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m16:03:57.777824 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m16:03:57.781645 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m16:03:57.782657 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m16:03:57.782920 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m16:03:57.783118 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m16:03:57.795861 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m16:03:57.796920 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m16:03:57.797873 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m16:03:57.798976 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:03:57.799248 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:03:57.799470 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:03:57.799665 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cdc2390>]}
[0m16:03:57.799840 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb8f1d0>]}
[0m16:03:57.800020 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb8ed00>]}
[0m16:03:57.809061 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m16:03:57.809897 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m16:03:57.810276 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m16:03:57.814332 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m16:03:57.814653 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m16:03:57.814951 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m16:03:57.815578 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m16:03:57.815754 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    -- models/bronze/bronze_airlines.sql
select * from `flight_db`.`raw`.`flights` limit 0
  )

[0m16:03:57.815884 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m16:03:57.816046 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:03:57.816166 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m16:03:57.816318 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m16:03:57.816563 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m16:03:57.816740 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:03:57.816857 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:03:58.518109 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08998-0078-11e3-9cf5-c2c199e0a1a7) - Created
[0m16:03:58.524574 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08998-0078-189d-9fc0-44ed0e113547) - Created
[0m16:03:58.526827 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08998-0077-1737-9a7a-0d989ffaa29d) - Created
[0m16:03:59.197742 [debug] [Thread-1 (]: SQL status: OK in 1.380 seconds
[0m16:03:59.202683 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f08998-0078-189d-9fc0-44ed0e113547, command-id=01f08998-0094-1657-9e96-5c5aeb054d72) - Closing
[0m16:03:59.203163 [debug] [Thread-3 (]: SQL status: OK in 1.390 seconds
[0m16:03:59.203520 [debug] [Thread-2 (]: SQL status: OK in 1.390 seconds
[0m16:03:59.216662 [debug] [Thread-1 (]: Applying tags to relation None
[0m16:03:59.217602 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f08998-0077-1737-9a7a-0d989ffaa29d, command-id=01f08998-0093-1430-82d3-e0782ee502df) - Closing
[0m16:03:59.218270 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f08998-0078-11e3-9cf5-c2c199e0a1a7, command-id=01f08998-0092-1eec-b4a2-c60356f51e72) - Closing
[0m16:03:59.221319 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m16:03:59.221811 [debug] [Thread-3 (]: Applying tags to relation None
[0m16:03:59.222209 [debug] [Thread-2 (]: Applying tags to relation None
[0m16:03:59.222434 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f08998-0078-189d-9fc0-44ed0e113547) - Closing
[0m16:03:59.439379 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m16:03:59.440376 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f08998-0077-1737-9a7a-0d989ffaa29d) - Closing
[0m16:03:59.633692 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m16:03:59.634793 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f08998-0078-11e3-9cf5-c2c199e0a1a7) - Closing
[0m16:03:59.841401 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f75ed0>]}
[0m16:03:59.841982 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cea3ad0>]}
[0m16:03:59.842288 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a1949d8c-f77b-4580-a3b1-02371abfd26e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ceae200>]}
[0m16:03:59.843137 [info ] [Thread-3 (]: 3 of 3 OK created sql view model raw.bronze_flights ............................ [[32mOK[0m in 2.07s]
[0m16:03:59.843668 [info ] [Thread-1 (]: 1 of 3 OK created sql view model raw.bronze_airlines ........................... [[32mOK[0m in 2.07s]
[0m16:03:59.844331 [info ] [Thread-2 (]: 2 of 3 OK created sql view model raw.bronze_airports ........................... [[32mOK[0m in 2.08s]
[0m16:03:59.844990 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m16:03:59.845495 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m16:03:59.845947 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m16:03:59.847954 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:03:59.848296 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:03:59.848762 [info ] [MainThread]: 
[0m16:03:59.849095 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 4.69 seconds (4.69s).
[0m16:03:59.850020 [debug] [MainThread]: Command end result
[0m16:03:59.880377 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:03:59.881402 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:03:59.884505 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m16:03:59.884659 [info ] [MainThread]: 
[0m16:03:59.884828 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:03:59.884945 [info ] [MainThread]: 
[0m16:03:59.885074 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m16:03:59.888180 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.7448077, "process_in_blocks": "0", "process_kernel_time": 0.254696, "process_mem_max_rss": "231571456", "process_out_blocks": "0", "process_user_time": 1.683736}
[0m16:03:59.888415 [debug] [MainThread]: Command `dbt run` succeeded at 16:03:59.888367 after 5.75 seconds
[0m16:03:59.888607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b6e510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cecbf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cecba70>]}
[0m16:03:59.888770 [debug] [MainThread]: Flushing usage events
[0m16:04:00.608444 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:24:13.257903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071d3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086fbb10>]}


============================== 16:24:13.260203 | 0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756 ==============================
[0m16:24:13.260203 [info ] [MainThread]: Running with dbt=1.10.9
[0m16:24:13.260426 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'log_cache_events': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'indirect_selection': 'eager', 'partial_parse': 'True', 'static_parser': 'True', 'printer_width': '80', 'invocation_command': 'dbt run --select path:models/bronze', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'write_json': 'True', 'fail_fast': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'target_path': 'None', 'version_check': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'introspect': 'True', 'no_print': 'None', 'log_format': 'default'}
[0m16:24:13.554765 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:24:13.554973 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:24:13.555085 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:24:14.002628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107727360>]}
[0m16:24:14.022859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af06ad0>]}
[0m16:24:14.023138 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m16:24:14.075868 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m16:24:14.113288 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m16:24:14.113516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c0b450>]}
[0m16:24:14.783853 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m16:24:14.787537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b265f40>]}
[0m16:24:14.820422 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:24:14.821699 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:24:14.826907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c290f0>]}
[0m16:24:14.827067 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m16:24:14.827166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e32ea0>]}
[0m16:24:14.827901 [info ] [MainThread]: 
[0m16:24:14.828019 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:24:14.828105 [info ] [MainThread]: 
[0m16:24:14.828277 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:24:14.828370 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:24:14.830710 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m16:24:14.830826 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m16:24:14.834599 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m16:24:14.834726 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m16:24:14.834810 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:24:15.604580 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899a-d5e7-1629-b56e-ba1044fef95d) - Created
[0m16:24:16.099672 [debug] [ThreadPool]: SQL status: OK in 1.260 seconds
[0m16:24:16.109629 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0899a-d5e7-1629-b56e-ba1044fef95d, command-id=01f0899a-d601-12d1-b53b-b05524a83867) - Closing
[0m16:24:16.110185 [debug] [ThreadPool]: On list_flight_db: Close
[0m16:24:16.110421 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899a-d5e7-1629-b56e-ba1044fef95d) - Closing
[0m16:24:16.311910 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m16:24:16.312449 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m16:24:16.326242 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m16:24:16.326628 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m16:24:16.326887 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:24:17.043913 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899a-d6bf-1dc5-9a55-880314c9caa9) - Created
[0m16:24:18.281539 [debug] [ThreadPool]: SQL status: OK in 1.950 seconds
[0m16:24:18.285193 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0899a-d6bf-1dc5-9a55-880314c9caa9, command-id=01f0899a-d6de-1efa-84ca-39d72ebc331a) - Closing
[0m16:24:18.286221 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m16:24:18.286537 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899a-d6bf-1dc5-9a55-880314c9caa9) - Closing
[0m16:24:18.493425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ef2ed0>]}
[0m16:24:18.498913 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m16:24:18.499413 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m16:24:18.499697 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m16:24:18.501402 [info ] [Thread-1 (]: 1 of 3 START sql view model raw.bronze_airlines ................................ [RUN]
[0m16:24:18.501801 [info ] [Thread-3 (]: 3 of 3 START sql view model raw.bronze_flights ................................. [RUN]
[0m16:24:18.502304 [info ] [Thread-2 (]: 2 of 3 START sql view model raw.bronze_airports ................................ [RUN]
[0m16:24:18.502990 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m16:24:18.503405 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m16:24:18.503739 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m16:24:18.504018 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m16:24:18.504291 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m16:24:18.504545 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m16:24:18.504828 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m16:24:18.505076 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m16:24:18.505304 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m16:24:18.516828 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m16:24:18.520854 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m16:24:18.522844 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m16:24:18.523886 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m16:24:18.535003 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m16:24:18.535252 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m16:24:18.535500 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m16:24:18.536940 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:24:18.538220 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m16:24:18.539450 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m16:24:18.539678 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1390e4730>]}
[0m16:24:18.539940 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:24:18.540261 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:24:18.546863 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d43a70>]}
[0m16:24:18.548189 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m16:24:18.548350 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11db8c680>]}
[0m16:24:18.548781 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m16:24:18.553553 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m16:24:18.553981 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m16:24:18.554324 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m16:24:18.554747 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m16:24:18.555221 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m16:24:18.555358 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m16:24:18.555478 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m16:24:18.555640 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m16:24:18.555810 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m16:24:18.555970 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    -- models/bronze/bronze_airlines.sql
select * from `flight_db`.`raw`.`flights` limit 0
  )

[0m16:24:18.556111 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:24:18.556239 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:24:18.556364 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:24:19.232419 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0899a-d80e-1984-a9ec-d7afab45d79e) - Created
[0m16:24:19.253719 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0899a-d814-11ec-bbf4-48de39b7dbc7) - Created
[0m16:24:19.263028 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0899a-d814-1183-9088-43ca65f31939) - Created
[0m16:24:20.548898 [debug] [Thread-3 (]: SQL status: OK in 1.990 seconds
[0m16:24:20.552806 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0899a-d80e-1984-a9ec-d7afab45d79e, command-id=01f0899a-d82a-1960-9cdd-fa971aa61db4) - Closing
[0m16:24:20.553499 [debug] [Thread-2 (]: SQL status: OK in 2.000 seconds
[0m16:24:20.560348 [debug] [Thread-1 (]: SQL status: OK in 2.000 seconds
[0m16:24:20.566966 [debug] [Thread-3 (]: Applying tags to relation None
[0m16:24:20.567811 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0899a-d814-11ec-bbf4-48de39b7dbc7, command-id=01f0899a-d830-1454-9205-762cfb11d942) - Closing
[0m16:24:20.568508 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0899a-d814-1183-9088-43ca65f31939, command-id=01f0899a-d840-12e8-8a05-aedca27affc6) - Closing
[0m16:24:20.571172 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m16:24:20.571734 [debug] [Thread-2 (]: Applying tags to relation None
[0m16:24:20.572165 [debug] [Thread-1 (]: Applying tags to relation None
[0m16:24:20.572397 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0899a-d80e-1984-a9ec-d7afab45d79e) - Closing
[0m16:24:20.765691 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m16:24:20.766845 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0899a-d814-11ec-bbf4-48de39b7dbc7) - Closing
[0m16:24:20.966108 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m16:24:20.966943 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0899a-d814-1183-9088-43ca65f31939) - Closing
[0m16:24:21.176769 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dd17ee0>]}
[0m16:24:21.177544 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dbc9b70>]}
[0m16:24:21.177928 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ba3c8a4-8b9b-4f43-8273-22dfd0c0d756', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x139058e30>]}
[0m16:24:21.178865 [info ] [Thread-3 (]: 3 of 3 OK created sql view model raw.bronze_flights ............................ [[32mOK[0m in 2.67s]
[0m16:24:21.179591 [info ] [Thread-1 (]: 1 of 3 OK created sql view model raw.bronze_airlines ........................... [[32mOK[0m in 2.67s]
[0m16:24:21.180771 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m16:24:21.180115 [info ] [Thread-2 (]: 2 of 3 OK created sql view model raw.bronze_airports ........................... [[32mOK[0m in 2.67s]
[0m16:24:21.181385 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m16:24:21.182069 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m16:24:21.184046 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:24:21.184558 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:24:21.185077 [info ] [MainThread]: 
[0m16:24:21.185412 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 6.36 seconds (6.36s).
[0m16:24:21.186320 [debug] [MainThread]: Command end result
[0m16:24:21.211778 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:24:21.213022 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:24:21.216955 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m16:24:21.217122 [info ] [MainThread]: 
[0m16:24:21.217311 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:24:21.217435 [info ] [MainThread]: 
[0m16:24:21.217575 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m16:24:21.224621 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 7.9955683, "process_in_blocks": "0", "process_kernel_time": 0.260149, "process_mem_max_rss": "234848256", "process_out_blocks": "0", "process_user_time": 2.1873}
[0m16:24:21.224863 [debug] [MainThread]: Command `dbt run` succeeded at 16:24:21.224816 after 8.00 seconds
[0m16:24:21.225068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e2bd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085f1f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1390d7ef0>]}
[0m16:24:21.225243 [debug] [MainThread]: Flushing usage events
[0m16:24:21.944916 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:42:06.683972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a57620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c27750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c279d0>]}


============================== 16:42:06.686351 | c675da2b-c3f1-4274-8eed-62d8dda220d2 ==============================
[0m16:42:06.686351 [info ] [MainThread]: Running with dbt=1.10.9
[0m16:42:06.686587 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'write_json': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'log_format': 'default', 'no_print': 'None', 'partial_parse': 'True', 'target_path': 'None', 'static_parser': 'True', 'printer_width': '80', 'indirect_selection': 'eager', 'debug': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt docs serve', 'introspect': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs'}
[0m16:42:06.995324 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:42:06.995520 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:42:06.995620 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:42:07.457000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c675da2b-c3f1-4274-8eed-62d8dda220d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f5f360>]}
[0m16:42:07.476888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c675da2b-c3f1-4274-8eed-62d8dda220d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e36be0>]}
[0m16:42:21.724274 [error] [MainThread]: Encountered an error:

[0m16:42:21.731886 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ~~~~^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 272, in wrapper
    return func(*args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 303, in wrapper
    return func(*args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 350, in wrapper
    return func(*args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/main.py", line 307, in docs_serve
    results = task.run()
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/task/docs/serve.py", line 29, in run
    httpd.serve_forever()
    ~~~~~~~~~~~~~~~~~~~^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socketserver.py", line 235, in serve_forever
    ready = selector.select(poll_interval)
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py", line 398, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt

[0m16:42:21.737988 [debug] [MainThread]: Resource report: {"command_name": "serve", "command_success": false, "command_wall_clock_time": 15.084355, "process_in_blocks": "0", "process_kernel_time": 0.19265, "process_mem_max_rss": "217661440", "process_out_blocks": "0", "process_user_time": 0.957158}
[0m16:42:21.740052 [debug] [MainThread]: Command `dbt docs serve` failed at 16:42:21.739730 after 15.09 seconds
[0m16:42:21.740939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f870550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f870450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f8b8230>]}
[0m16:42:21.741481 [debug] [MainThread]: Flushing usage events
[0m16:42:27.839456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059e3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fff750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fff9d0>]}


============================== 16:42:27.841522 | 37dfb5a1-5c2f-4d79-b462-0e1e7ac88acb ==============================
[0m16:42:27.841522 [info ] [MainThread]: Running with dbt=1.10.9
[0m16:42:27.841746 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'cache_selected_only': 'False', 'version_check': 'True', 'quiet': 'False', 'use_experimental_parser': 'False', 'empty': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'use_colors': 'True', 'introspect': 'True', 'write_json': 'True', 'invocation_command': 'dbt docs generate', 'log_format': 'default', 'debug': 'False', 'static_parser': 'True', 'printer_width': '80', 'partial_parse': 'True'}
[0m16:42:28.138880 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:42:28.139076 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:42:28.139176 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:42:28.465894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '37dfb5a1-5c2f-4d79-b462-0e1e7ac88acb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f27360>]}
[0m16:42:28.485307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '37dfb5a1-5c2f-4d79-b462-0e1e7ac88acb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11100aad0>]}
[0m16:42:28.485534 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m16:42:28.542535 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m16:42:28.601107 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:42:28.601279 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:42:28.604323 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m16:42:28.618939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '37dfb5a1-5c2f-4d79-b462-0e1e7ac88acb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121813550>]}
[0m16:42:28.623869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '37dfb5a1-5c2f-4d79-b462-0e1e7ac88acb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1217f27b0>]}
[0m16:42:28.624021 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m16:42:28.624129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37dfb5a1-5c2f-4d79-b462-0e1e7ac88acb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121af7cb0>]}
[0m16:42:28.624891 [info ] [MainThread]: 
[0m16:42:28.625011 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:42:28.625092 [info ] [MainThread]: 
[0m16:42:28.625273 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:42:28.625362 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:42:28.628111 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m16:42:28.628283 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m16:42:28.634009 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m16:42:28.634149 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m16:42:28.634260 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:42:29.414603 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899d-61da-1ffe-b989-e8ac64f76556) - Created
[0m16:42:30.738313 [debug] [ThreadPool]: SQL status: OK in 2.100 seconds
[0m16:42:30.748822 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0899d-61da-1ffe-b989-e8ac64f76556, command-id=01f0899d-61f7-1aca-80f4-6e8dbeee3f6e) - Closing
[0m16:42:30.749445 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m16:42:30.749636 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899d-61da-1ffe-b989-e8ac64f76556) - Closing
[0m16:42:30.967738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37dfb5a1-5c2f-4d79-b462-0e1e7ac88acb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1160060d0>]}
[0m16:42:30.973056 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m16:42:30.973499 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m16:42:30.973820 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m16:42:30.974099 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m16:42:30.974668 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m16:42:30.975143 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m16:42:30.975546 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m16:42:30.976873 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m16:42:30.979623 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m16:42:30.979916 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m16:42:30.980204 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m16:42:30.980492 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m16:42:30.980749 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m16:42:30.980986 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m16:42:30.981233 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m16:42:30.988802 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m16:42:30.989141 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m16:42:30.991427 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m16:42:30.993674 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m16:42:30.995650 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m16:42:30.996419 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m16:42:30.996678 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m16:42:30.997258 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m16:42:30.997475 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m16:42:30.997841 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m16:42:30.998016 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m16:42:30.998505 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m16:42:30.998753 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_flights
[0m16:42:30.999200 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m16:42:30.999403 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_airlines
[0m16:42:30.999770 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m16:42:30.999996 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airports
[0m16:42:31.000323 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airlines) - Creating connection
[0m16:42:31.000503 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m16:42:31.000691 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m16:42:31.000947 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m16:42:31.001124 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airlines'
[0m16:42:31.001351 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m16:42:31.001524 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_flights
[0m16:42:31.001699 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m16:42:31.001863 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_airlines
[0m16:42:31.002024 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m16:42:31.007233 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m16:42:31.007437 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.silver_airports
[0m16:42:31.008839 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_airlines"
[0m16:42:31.009004 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m16:42:31.010415 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m16:42:31.011663 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m16:42:31.011993 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_flights
[0m16:42:31.012170 [debug] [Thread-2 (]: Began executing node model.flights_dbt.silver_airports
[0m16:42:31.012493 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_flights
[0m16:42:31.012651 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_airlines
[0m16:42:31.012800 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m16:42:31.013078 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airports
[0m16:42:31.013256 [debug] [Thread-3 (]: Began running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:42:31.013547 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_airlines
[0m16:42:31.013791 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m16:42:31.013941 [debug] [Thread-2 (]: Began running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m16:42:31.014173 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m16:42:31.014389 [debug] [Thread-1 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m16:42:31.014651 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m16:42:31.014796 [debug] [Thread-4 (]: Began running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m16:42:31.014941 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710'
[0m16:42:31.015133 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.gold_flight_metrics) - Creating connection
[0m16:42:31.015272 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_first_dbt_model_id.16e066b321'
[0m16:42:31.015460 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m16:42:31.015601 [debug] [Thread-3 (]: Began compiling node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:42:31.015737 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.gold_flight_metrics'
[0m16:42:31.015863 [debug] [Thread-2 (]: Began compiling node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m16:42:31.015996 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778'
[0m16:42:31.021447 [debug] [Thread-3 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:42:31.021594 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.gold_flight_metrics
[0m16:42:31.024210 [debug] [Thread-2 (]: Writing injected SQL for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m16:42:31.024339 [debug] [Thread-4 (]: Began compiling node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m16:42:31.025870 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.gold_flight_metrics"
[0m16:42:31.027779 [debug] [Thread-4 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m16:42:31.028024 [debug] [Thread-2 (]: Began executing node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m16:42:31.028275 [debug] [Thread-2 (]: Finished running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m16:42:31.028408 [debug] [Thread-2 (]: Began running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m16:42:31.028576 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m16:42:31.028706 [debug] [Thread-3 (]: Began executing node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:42:31.028830 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493'
[0m16:42:31.029069 [debug] [Thread-3 (]: Finished running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:42:31.029205 [debug] [Thread-4 (]: Began executing node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m16:42:31.029308 [debug] [Thread-2 (]: Began compiling node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m16:42:31.029418 [debug] [Thread-1 (]: Began executing node model.flights_dbt.gold_flight_metrics
[0m16:42:31.029645 [debug] [Thread-4 (]: Finished running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m16:42:31.031264 [debug] [Thread-2 (]: Writing injected SQL for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:42:31.031478 [debug] [Thread-1 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m16:42:31.031702 [debug] [Thread-2 (]: Began executing node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m16:42:31.031899 [debug] [Thread-2 (]: Finished running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m16:42:31.032956 [debug] [MainThread]: Command end result
[0m16:42:31.068101 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:42:31.069283 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:42:31.071454 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m16:42:31.072979 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=generate_catalog) - Creating connection
[0m16:42:31.073072 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m16:42:31.073139 [info ] [MainThread]: Building catalog
[0m16:42:31.073858 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=('flight_db', 'raw')) - Creating connection
[0m16:42:31.073954 [debug] [ThreadPool]: Acquiring new databricks connection '('flight_db', 'raw')'
[0m16:42:31.075575 [debug] [ThreadPool]: Using databricks connection "('flight_db', 'raw')"
[0m16:42:31.075669 [debug] [ThreadPool]: On ('flight_db', 'raw'): /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "('flight_db', 'raw')"} */

    
SELECT current_catalog()

  
[0m16:42:31.075746 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:42:31.724780 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899d-633a-1100-ae80-a9a9ac82401b) - Created
[0m16:42:32.138265 [debug] [ThreadPool]: SQL status: OK in 1.060 seconds
[0m16:42:32.141137 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0899d-633a-1100-ae80-a9a9ac82401b, command-id=01f0899d-6358-17d8-a4ff-3d6c22c10808) - Closing
[0m16:42:32.149347 [debug] [ThreadPool]: Using databricks connection "('flight_db', 'raw')"
[0m16:42:32.149753 [debug] [ThreadPool]: On ('flight_db', 'raw'): /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "('flight_db', 'raw')"} */

    
SHOW TABLE EXTENDED IN `flight_db`.`raw` LIKE 'my_second_dbt_model|airports|flights|bronze_flights|bronze_airlines|silver_airports|weather|gold_flight_metrics|bronze_airports|silver_airlines|my_first_dbt_model|silver_flights|airlines'

  
[0m16:42:33.071112 [debug] [ThreadPool]: SQL status: OK in 0.920 seconds
[0m16:42:33.078036 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0899d-633a-1100-ae80-a9a9ac82401b, command-id=01f0899d-6398-138d-87da-8c208377ee98) - Closing
[0m16:42:33.085215 [debug] [ThreadPool]: On ('flight_db', 'raw'): Close
[0m16:42:33.085779 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899d-633a-1100-ae80-a9a9ac82401b) - Closing
[0m16:42:33.291202 [debug] [MainThread]: Wrote artifact CatalogArtifact to /Users/artakerqeli/flights_dbt/flights_dbt/target/catalog.json
[0m16:42:33.320180 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:42:33.321515 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:42:33.321700 [info ] [MainThread]: Catalog written to /Users/artakerqeli/flights_dbt/flights_dbt/target/catalog.json
[0m16:42:33.324799 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 5.5135355, "process_in_blocks": "0", "process_kernel_time": 0.208257, "process_mem_max_rss": "234618880", "process_out_blocks": "0", "process_user_time": 1.302122}
[0m16:42:33.325095 [debug] [MainThread]: Command `dbt docs generate` succeeded at 16:42:33.325046 after 5.51 seconds
[0m16:42:33.325299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106cb2e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e3b1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e3b120>]}
[0m16:42:33.325478 [debug] [MainThread]: Flushing usage events
[0m16:42:33.940172 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:42:37.835500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c83620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109cf3750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109cf39d0>]}


============================== 16:42:37.837649 | ebd5ac1d-6c81-46dc-93eb-b53165a1a3b6 ==============================
[0m16:42:37.837649 [info ] [MainThread]: Running with dbt=1.10.9
[0m16:42:37.837871 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'warn_error': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'empty': 'None', 'no_print': 'None', 'write_json': 'True', 'printer_width': '80', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'partial_parse': 'True', 'log_format': 'default', 'fail_fast': 'False', 'quiet': 'False', 'use_colors': 'True', 'invocation_command': 'dbt docs serve', 'debug': 'False', 'indirect_selection': 'eager', 'log_cache_events': 'False'}
[0m16:42:38.130742 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:42:38.130941 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:42:38.131043 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:42:38.438118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ebd5ac1d-6c81-46dc-93eb-b53165a1a3b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108427360>]}
[0m16:42:38.458158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ebd5ac1d-6c81-46dc-93eb-b53165a1a3b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd02be0>]}
[0m16:53:53.135304 [error] [MainThread]: Encountered an error:

[0m16:53:53.151755 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ~~~~^^^^^^^^^^^^^^^^^
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 272, in wrapper
    return func(*args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 303, in wrapper
    return func(*args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 350, in wrapper
    return func(*args, **kwargs)
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/cli/main.py", line 307, in docs_serve
    results = task.run()
  File "/Users/artakerqeli/flights_dbt/.venv/lib/python3.13/site-packages/dbt/task/docs/serve.py", line 29, in run
    httpd.serve_forever()
    ~~~~~~~~~~~~~~~~~~~^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socketserver.py", line 235, in serve_forever
    ready = selector.select(poll_interval)
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py", line 398, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt

[0m16:53:53.161186 [debug] [MainThread]: Resource report: {"command_name": "serve", "command_success": false, "command_wall_clock_time": 675.3469, "process_in_blocks": "0", "process_kernel_time": 0.219847, "process_mem_max_rss": "217513984", "process_out_blocks": "0", "process_user_time": 1.013717}
[0m16:53:53.163244 [debug] [MainThread]: Command `dbt docs serve` failed at 16:53:53.163100 after 675.35 seconds
[0m16:53:53.163811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b0cc550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b0cc450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b218230>]}
[0m16:53:53.164125 [debug] [MainThread]: Flushing usage events
[0m16:53:56.064147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10876b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109787890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109787b10>]}


============================== 16:53:56.066413 | 41712194-35ce-42fd-92f6-7112744ec6c7 ==============================
[0m16:53:56.066413 [info ] [MainThread]: Running with dbt=1.10.9
[0m16:53:56.066658 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'log_cache_events': 'False', 'invocation_command': 'dbt build --select path:models/bronze', 'indirect_selection': 'eager', 'no_print': 'None', 'write_json': 'True', 'empty': 'False', 'partial_parse': 'True', 'debug': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'use_experimental_parser': 'False', 'printer_width': '80', 'static_parser': 'True', 'warn_error': 'None', 'target_path': 'None', 'version_check': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'fail_fast': 'False', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'cache_selected_only': 'False', 'quiet': 'False'}
[0m16:53:56.378128 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:53:56.378344 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:53:56.378447 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:53:56.712350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108aaf360>]}
[0m16:53:56.731672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a992ad0>]}
[0m16:53:56.731904 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m16:53:56.783329 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m16:53:56.840417 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:53:56.840551 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:53:56.843545 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m16:53:56.858090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110636d50>]}
[0m16:53:56.894029 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:53:56.894948 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:53:56.905791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1108c5400>]}
[0m16:53:56.905980 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m16:53:56.907145 [info ] [MainThread]: 
[0m16:53:56.907280 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:53:56.907370 [info ] [MainThread]: 
[0m16:53:56.907571 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:53:56.907666 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:53:56.910340 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m16:53:56.910485 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m16:53:56.914581 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m16:53:56.914697 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m16:53:56.914788 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:53:57.777893 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899e-fc24-1009-859f-1169ecc08087) - Created
[0m16:53:58.552521 [debug] [ThreadPool]: SQL status: OK in 1.640 seconds
[0m16:53:58.559439 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0899e-fc24-1009-859f-1169ecc08087, command-id=01f0899e-fc4b-11f7-8d39-63f19e9259af) - Closing
[0m16:53:58.559990 [debug] [ThreadPool]: On list_flight_db: Close
[0m16:53:58.560223 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899e-fc24-1009-859f-1169ecc08087) - Closing
[0m16:53:58.775545 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m16:53:58.776114 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m16:53:58.788423 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m16:53:58.788800 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m16:53:58.789056 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:53:59.489404 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899e-fd2a-142f-95ab-f2cb8062d6ec) - Created
[0m16:54:00.186789 [debug] [ThreadPool]: SQL status: OK in 1.400 seconds
[0m16:54:00.190382 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0899e-fd2a-142f-95ab-f2cb8062d6ec, command-id=01f0899e-fd47-1a55-a08d-bf6f0a128766) - Closing
[0m16:54:00.191393 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m16:54:00.191690 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0899e-fd2a-142f-95ab-f2cb8062d6ec) - Closing
[0m16:54:00.399535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1108bcf30>]}
[0m16:54:00.403603 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m16:54:00.404011 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m16:54:00.404344 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m16:54:00.405053 [info ] [Thread-3 (]: 3 of 3 START sql view model raw.bronze_flights ................................. [RUN]
[0m16:54:00.405536 [info ] [Thread-2 (]: 2 of 3 START sql view model raw.bronze_airports ................................ [RUN]
[0m16:54:00.405976 [info ] [Thread-1 (]: 1 of 3 START sql view model raw.bronze_airlines ................................ [RUN]
[0m16:54:00.406588 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m16:54:00.407052 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m16:54:00.407499 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m16:54:00.407818 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m16:54:00.408078 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m16:54:00.408319 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m16:54:00.408580 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m16:54:00.408826 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m16:54:00.409064 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m16:54:00.416800 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m16:54:00.420066 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m16:54:00.423669 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m16:54:00.425090 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m16:54:00.425365 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m16:54:00.425644 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m16:54:00.436000 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m16:54:00.437450 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m16:54:00.438474 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m16:54:00.439675 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:54:00.439980 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:54:00.440294 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:54:00.440557 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a09640>]}
[0m16:54:00.440725 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1108de450>]}
[0m16:54:00.440877 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1108df590>]}
[0m16:54:00.450396 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m16:54:00.450657 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m16:54:00.451086 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m16:54:00.455717 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m16:54:00.456060 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m16:54:00.456357 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m16:54:00.456882 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m16:54:00.457028 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m16:54:00.457163 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m16:54:00.457316 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m16:54:00.457500 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m16:54:00.457661 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    -- models/bronze/bronze_airlines.sql
select * from `flight_db`.`raw`.`flights` limit 0
  )

[0m16:54:00.457801 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:54:00.457931 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:54:00.458052 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:54:01.139293 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0899e-fe26-10d4-b410-a126e954425c) - Created
[0m16:54:01.158783 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0899e-fe29-100b-9aae-9e92a50f71ec) - Created
[0m16:54:01.186976 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0899e-fe2e-1226-b124-43c49fc907a5) - Created
[0m16:54:02.339542 [debug] [Thread-2 (]: SQL status: OK in 1.880 seconds
[0m16:54:02.344688 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0899e-fe2e-1226-b124-43c49fc907a5, command-id=01f0899e-fe4a-1d3d-b884-dca65b77ea76) - Closing
[0m16:54:02.345205 [debug] [Thread-1 (]: SQL status: OK in 1.890 seconds
[0m16:54:02.345554 [debug] [Thread-3 (]: SQL status: OK in 1.890 seconds
[0m16:54:02.358755 [debug] [Thread-2 (]: Applying tags to relation None
[0m16:54:02.359744 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0899e-fe29-100b-9aae-9e92a50f71ec, command-id=01f0899e-fe46-18e7-9d00-20b239eb4325) - Closing
[0m16:54:02.360404 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0899e-fe26-10d4-b410-a126e954425c, command-id=01f0899e-fe42-190a-a2d4-1c024808d068) - Closing
[0m16:54:02.363295 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m16:54:02.363818 [debug] [Thread-1 (]: Applying tags to relation None
[0m16:54:02.364299 [debug] [Thread-3 (]: Applying tags to relation None
[0m16:54:02.364579 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0899e-fe2e-1226-b124-43c49fc907a5) - Closing
[0m16:54:02.562773 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m16:54:02.563611 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0899e-fe29-100b-9aae-9e92a50f71ec) - Closing
[0m16:54:02.820452 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m16:54:02.821559 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0899e-fe26-10d4-b410-a126e954425c) - Closing
[0m16:54:03.019825 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a8e2b0>]}
[0m16:54:03.020469 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a1ac30>]}
[0m16:54:03.020808 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41712194-35ce-42fd-92f6-7112744ec6c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11071d0d0>]}
[0m16:54:03.021653 [info ] [Thread-2 (]: 2 of 3 OK created sql view model raw.bronze_airports ........................... [[32mOK[0m in 2.61s]
[0m16:54:03.022201 [info ] [Thread-3 (]: 3 of 3 OK created sql view model raw.bronze_flights ............................ [[32mOK[0m in 2.61s]
[0m16:54:03.023514 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m16:54:03.022967 [info ] [Thread-1 (]: 1 of 3 OK created sql view model raw.bronze_airlines ........................... [[32mOK[0m in 2.61s]
[0m16:54:03.024224 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m16:54:03.025039 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m16:54:03.028137 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:54:03.030465 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:54:03.031406 [info ] [MainThread]: 
[0m16:54:03.031993 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 6.12 seconds (6.12s).
[0m16:54:03.033745 [debug] [MainThread]: Command end result
[0m16:54:03.066595 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m16:54:03.067696 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m16:54:03.070837 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m16:54:03.070974 [info ] [MainThread]: 
[0m16:54:03.071123 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:54:03.071234 [info ] [MainThread]: 
[0m16:54:03.071352 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m16:54:03.073646 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 7.041098, "process_in_blocks": "0", "process_kernel_time": 0.215811, "process_mem_max_rss": "238534656", "process_out_blocks": "0", "process_user_time": 1.585417}
[0m16:54:03.073849 [debug] [MainThread]: Command `dbt build` succeeded at 16:54:03.073805 after 7.04 seconds
[0m16:54:03.074021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107676750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ac4710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ac4e30>]}
[0m16:54:03.074165 [debug] [MainThread]: Flushing usage events
[0m16:54:03.767914 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:11:11.556624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10464b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105663890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105663b10>]}


============================== 19:11:11.559051 | 726446cc-64c8-4c77-92cc-bc7ae307fd44 ==============================
[0m19:11:11.559051 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:11:11.559293 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'quiet': 'False', 'empty': 'False', 'fail_fast': 'False', 'static_parser': 'True', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'use_experimental_parser': 'False', 'introspect': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run', 'debug': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'version_check': 'True', 'target_path': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'use_colors': 'True', 'log_format': 'default', 'no_print': 'None'}
[0m19:11:11.855597 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:11:11.855779 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:11:11.855874 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:11:12.547580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104993360>]}
[0m19:11:12.566010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10686ead0>]}
[0m19:11:12.566226 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:11:12.616754 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:11:12.672660 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:11:12.672914 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m19:11:12.759628 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:11:12.764343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c52e950>]}
[0m19:11:12.797301 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:11:12.799033 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:11:12.804719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c12b890>]}
[0m19:11:12.804886 [info ] [MainThread]: Found 9 models, 4 data tests, 4 sources, 686 macros
[0m19:11:12.804984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c926350>]}
[0m19:11:12.805732 [info ] [MainThread]: 
[0m19:11:12.805849 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:11:12.805932 [info ] [MainThread]: 
[0m19:11:12.806123 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:11:12.806222 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:11:12.808643 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m19:11:12.808760 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m19:11:12.812122 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m19:11:12.812230 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m19:11:12.812312 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:11:13.726145 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b2-291d-1ab7-868a-ef3fd5145d90) - Created
[0m19:11:26.924888 [debug] [ThreadPool]: SQL status: OK in 14.110 seconds
[0m19:11:26.937297 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b2-291d-1ab7-868a-ef3fd5145d90, command-id=01f089b2-294e-1699-9f66-5d3740511ed0) - Closing
[0m19:11:27.215792 [debug] [ThreadPool]: On list_flight_db: Close
[0m19:11:27.216689 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b2-291d-1ab7-868a-ef3fd5145d90) - Closing
[0m19:11:27.405063 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:11:27.405693 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:11:27.419695 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:11:27.420195 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:11:27.420495 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:11:28.121257 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b2-31c3-1d1d-b3ac-2e124c76f8a1) - Created
[0m19:11:32.344214 [debug] [ThreadPool]: SQL status: OK in 4.920 seconds
[0m19:11:32.347050 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b2-31c3-1d1d-b3ac-2e124c76f8a1, command-id=01f089b2-31e2-197e-bb43-715f3f0650b9) - Closing
[0m19:11:32.347766 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:11:32.347972 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b2-31c3-1d1d-b3ac-2e124c76f8a1) - Closing
[0m19:11:32.549592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8971e0>]}
[0m19:11:32.555376 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m19:11:32.556066 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m19:11:32.556497 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m19:11:32.556836 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m19:11:32.557535 [info ] [Thread-2 (]: 2 of 9 START sql view model raw.bronze_airports ................................ [RUN]
[0m19:11:32.558079 [info ] [Thread-4 (]: 4 of 9 START sql table model raw.my_first_dbt_model ............................ [RUN]
[0m19:11:32.558766 [info ] [Thread-1 (]: 1 of 9 START sql view model raw.bronze_airlines ................................ [RUN]
[0m19:11:32.559489 [info ] [Thread-3 (]: 3 of 9 START sql view model raw.bronze_flights ................................. [RUN]
[0m19:11:32.560267 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m19:11:32.560806 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m19:11:32.561242 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m19:11:32.561603 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m19:11:32.561916 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m19:11:32.562329 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m19:11:32.562623 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m19:11:32.562870 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m19:11:32.563156 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m19:11:32.563384 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m19:11:32.563601 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m19:11:32.563811 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m19:11:32.575850 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m19:11:32.587887 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m19:11:32.588875 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m19:11:32.599539 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m19:11:32.600951 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m19:11:32.601184 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m19:11:32.601341 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m19:11:32.601487 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m19:11:32.614417 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:11:32.613015 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:11:32.615575 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:11:32.624292 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m19:11:32.625246 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:11:32.625531 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:11:32.625772 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:11:32.625974 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:11:32.626206 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8ea210>]}
[0m19:11:32.626394 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c63f070>]}
[0m19:11:32.626535 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c63f540>]}
[0m19:11:32.626675 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c9620d0>]}
[0m19:11:32.632941 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m19:11:32.633330 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m19:11:32.633655 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m19:11:32.649681 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m19:11:32.650719 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m19:11:32.651002 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m19:11:32.651254 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m19:11:32.651780 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m19:11:32.651941 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`raw`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:11:32.652070 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:11:32.652172 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m19:11:32.652273 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m19:11:32.652458 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m19:11:32.652607 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    -- Bronze layer: just expose the raw airlines table
SELECT *
FROM flight_db.raw.airlines
  )

[0m19:11:32.652783 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m19:11:32.652915 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m19:11:32.653025 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:11:32.653131 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:11:32.653260 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:11:33.418791 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b2-34ef-1e86-b409-28958db2ab1d) - Created
[0m19:11:33.421888 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b2-34ec-1eb8-96d5-aca6a335d71a) - Created
[0m19:11:33.424748 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b2-34ef-1481-96c4-19fbb9958591) - Created
[0m19:11:33.435387 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b2-34f0-10fb-9eb6-89ae86daadb0) - Created
[0m19:11:34.773938 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    -- Bronze layer: just expose the raw airlines table
SELECT *
FROM flight_db.raw.airlines
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f089b2-350f-12d7-838f-a8febbe00c45
[0m19:11:34.775564 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m19:11:34.775969 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b2-34f0-10fb-9eb6-89ae86daadb0) - Closing
[0m19:11:34.981823 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:11:34.985027 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cab8280>]}
[0m19:11:34.986025 [error] [Thread-1 (]: 1 of 9 ERROR creating sql view model raw.bronze_airlines ....................... [[31mERROR[0m in 2.42s]
[0m19:11:34.986573 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m19:11:34.987177 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m19:11:34.988538 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_airlines
[0m19:11:34.988838 [info ] [Thread-1 (]: 5 of 9 SKIP relation raw.silver_airlines ....................................... [[33mSKIP[0m]
[0m19:11:34.989121 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_airlines
[0m19:11:37.694508 [debug] [Thread-2 (]: SQL status: OK in 5.040 seconds
[0m19:11:37.696417 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b2-34ec-1eb8-96d5-aca6a335d71a, command-id=01f089b2-350b-108e-bc40-198d762c66a3) - Closing
[0m19:11:37.709847 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:11:37.713729 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m19:11:37.714061 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b2-34ec-1eb8-96d5-aca6a335d71a) - Closing
[0m19:11:37.790530 [debug] [Thread-3 (]: SQL status: OK in 5.140 seconds
[0m19:11:37.792873 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b2-34ef-1e86-b409-28958db2ab1d, command-id=01f089b2-350c-1bf9-af91-ac0ec4c5bfe3) - Closing
[0m19:11:37.794429 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:11:37.903516 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m19:11:37.904411 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b2-34ef-1e86-b409-28958db2ab1d) - Closing
[0m19:11:38.116805 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca8ad50>]}
[0m19:11:38.117825 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca32630>]}
[0m19:11:38.119030 [info ] [Thread-2 (]: 2 of 9 OK created sql view model raw.bronze_airports ........................... [[32mOK[0m in 5.56s]
[0m19:11:38.119612 [info ] [Thread-3 (]: 3 of 9 OK created sql view model raw.bronze_flights ............................ [[32mOK[0m in 5.56s]
[0m19:11:38.120476 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m19:11:38.121078 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m19:11:38.122002 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_airports
[0m19:11:38.122444 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_flights
[0m19:11:38.122976 [info ] [Thread-1 (]: 6 of 9 START sql view model raw.silver_airports ................................ [RUN]
[0m19:11:38.123469 [info ] [Thread-2 (]: 7 of 9 START sql view model raw.silver_flights ................................. [RUN]
[0m19:11:38.124218 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m19:11:38.124740 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m19:11:38.125090 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m19:11:38.125448 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m19:11:38.125893 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_airports
[0m19:11:38.126250 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.silver_flights
[0m19:11:38.134013 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m19:11:38.141004 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m19:11:38.142531 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_airports
[0m19:11:38.142778 [debug] [Thread-2 (]: Began executing node model.flights_dbt.silver_flights
[0m19:11:38.144247 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:11:38.146000 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:11:38.146809 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`silver_airports`
[0m19:11:38.147397 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`silver_flights`
[0m19:11:38.147954 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m19:11:38.148465 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m19:11:38.148999 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m19:11:38.149238 [debug] [Thread-2 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    select
    f.flight_id,
    f.airline,
    f.origin,
    f.dep_time,
    f.arr_time,
    f.arr_delay
from `flight_db`.`raw`.`bronze_flights` as f
  )

[0m19:11:38.149427 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:11:38.149576 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m19:11:38.150052 [debug] [Thread-1 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`bronze_airports`
  )

[0m19:11:38.150220 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:11:38.793449 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b2-3825-10c2-a20a-ab970af3f7cb) - Created
[0m19:11:38.849014 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b2-382c-141d-b1fd-45e81b846a3f) - Created
[0m19:11:39.594823 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    select
    f.flight_id,
    f.airline,
    f.origin,
    f.dep_time,
    f.arr_time,
    f.arr_delay
from `flight_db`.`raw`.`bronze_flights` as f
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f089b2-3849-1022-bef8-653143c706d8
[0m19:11:39.596068 [debug] [Thread-2 (]: On model.flights_dbt.silver_flights: Close
[0m19:11:39.596406 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b2-382c-141d-b1fd-45e81b846a3f) - Closing
[0m19:11:39.813510 [debug] [Thread-2 (]: Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:11:39.814475 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca9bef0>]}
[0m19:11:39.815349 [error] [Thread-2 (]: 7 of 9 ERROR creating sql view model raw.silver_flights ........................ [[31mERROR[0m in 1.69s]
[0m19:11:39.815920 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_flights
[0m19:11:39.816448 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_flights' to be skipped because of status 'error'.  Reason: Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql.
[0m19:11:39.887257 [debug] [Thread-1 (]: SQL status: OK in 1.740 seconds
[0m19:11:39.889029 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b2-3825-10c2-a20a-ab970af3f7cb, command-id=01f089b2-383e-1e90-9375-799b24705320) - Closing
[0m19:11:39.889902 [debug] [Thread-1 (]: Applying tags to relation None
[0m19:11:39.890710 [debug] [Thread-1 (]: On model.flights_dbt.silver_airports: Close
[0m19:11:39.890964 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b2-3825-10c2-a20a-ab970af3f7cb) - Closing
[0m19:11:40.113082 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10caaf2f0>]}
[0m19:11:40.114379 [info ] [Thread-1 (]: 6 of 9 OK created sql view model raw.silver_airports ........................... [[32mOK[0m in 1.99s]
[0m19:11:40.115040 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_airports
[0m19:11:40.115725 [debug] [Thread-3 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m19:11:40.116090 [info ] [Thread-3 (]: 8 of 9 SKIP relation raw.gold_flight_metrics ................................... [[33mSKIP[0m]
[0m19:11:40.116435 [debug] [Thread-3 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m19:11:45.721183 [debug] [Thread-4 (]: SQL status: OK in 13.070 seconds
[0m19:11:45.723605 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b2-34ef-1481-96c4-19fbb9958591, command-id=01f089b2-350d-1c33-8a8d-8ebf17687ac6) - Closing
[0m19:11:45.961804 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:11:45.980824 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m19:11:45.981229 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b2-34ef-1481-96c4-19fbb9958591) - Closing
[0m19:11:46.200851 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c89f5f0>]}
[0m19:11:46.202706 [info ] [Thread-4 (]: 4 of 9 OK created sql table model raw.my_first_dbt_model ....................... [[32mOK[0m in 13.64s]
[0m19:11:46.203556 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m19:11:46.204587 [debug] [Thread-2 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m19:11:46.205216 [info ] [Thread-2 (]: 9 of 9 START sql view model raw.my_second_dbt_model ............................ [RUN]
[0m19:11:46.205966 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m19:11:46.206354 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m19:11:46.206670 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m19:11:46.214686 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m19:11:46.215773 [debug] [Thread-2 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m19:11:46.217786 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:11:46.218759 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`my_second_dbt_model`
[0m19:11:46.219705 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m19:11:46.220613 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m19:11:46.220975 [debug] [Thread-2 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`raw`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`raw`.`my_first_dbt_model`
where id = 1
  )

[0m19:11:46.221224 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:11:46.968891 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b2-3cfe-1606-8247-72613ecd27a5) - Created
[0m19:11:47.644244 [debug] [Thread-2 (]: SQL status: OK in 1.420 seconds
[0m19:11:47.646784 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b2-3cfe-1606-8247-72613ecd27a5, command-id=01f089b2-3d1f-10b2-9717-5da42c069375) - Closing
[0m19:11:47.648319 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:11:47.649888 [debug] [Thread-2 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m19:11:47.650167 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b2-3cfe-1606-8247-72613ecd27a5) - Closing
[0m19:11:47.864297 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726446cc-64c8-4c77-92cc-bc7ae307fd44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca31a90>]}
[0m19:11:47.865914 [info ] [Thread-2 (]: 9 of 9 OK created sql view model raw.my_second_dbt_model ....................... [[32mOK[0m in 1.66s]
[0m19:11:47.866724 [debug] [Thread-2 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m19:11:47.868651 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:11:47.869037 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:11:47.869605 [info ] [MainThread]: 
[0m19:11:47.869970 [info ] [MainThread]: Finished running 2 table models, 7 view models in 0 hours 0 minutes and 35.06 seconds (35.06s).
[0m19:11:47.871475 [debug] [MainThread]: Command end result
[0m19:11:47.903418 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:11:47.905607 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:11:47.910000 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:11:47.910188 [info ] [MainThread]: 
[0m19:11:47.910356 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m19:11:47.910483 [info ] [MainThread]: 
[0m19:11:47.910648 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m19:11:47.910812 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:11:47.910929 [info ] [MainThread]: 
[0m19:11:47.911062 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:11:47.911172 [info ] [MainThread]: 
[0m19:11:47.911299 [error] [MainThread]: [31mFailure in model silver_flights (models/silver/silver_flights.sql)[0m
[0m19:11:47.911439 [error] [MainThread]:   Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:11:47.911546 [info ] [MainThread]: 
[0m19:11:47.911670 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_flights.sql
[0m19:11:47.911780 [info ] [MainThread]: 
[0m19:11:47.911919 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=9
[0m19:11:47.914775 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 36.39053, "process_in_blocks": "0", "process_kernel_time": 0.296179, "process_mem_max_rss": "233160704", "process_out_blocks": "0", "process_user_time": 2.319842}
[0m19:11:47.915066 [debug] [MainThread]: Command `dbt run` failed at 19:11:47.915022 after 36.39 seconds
[0m19:11:47.915300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054a84d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cae3e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cae39b0>]}
[0m19:11:47.915474 [debug] [MainThread]: Flushing usage events
[0m19:11:48.661465 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:24:06.831074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060d3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075ff890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075ffb10>]}


============================== 19:24:06.833417 | 67b78a2b-9100-48a0-813c-dde9e3af3744 ==============================
[0m19:24:06.833417 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:24:06.833642 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'debug': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'target_path': 'None', 'fail_fast': 'False', 'log_cache_events': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'use_colors': 'True', 'introspect': 'True', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'version_check': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'no_print': 'None', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False'}
[0m19:24:07.142225 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:24:07.142436 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:24:07.142535 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:24:07.611279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '67b78a2b-9100-48a0-813c-dde9e3af3744', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106627360>]}
[0m19:24:07.630448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '67b78a2b-9100-48a0-813c-dde9e3af3744', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x131606ad0>]}
[0m19:24:07.630687 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:24:07.684124 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:24:07.741070 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m19:24:07.741316 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/stg_airlines.sql
[0m19:24:07.830537 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:24:07.835751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67b78a2b-9100-48a0-813c-dde9e3af3744', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x140b0f550>]}
[0m19:24:07.869699 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:24:07.870844 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:24:07.880562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67b78a2b-9100-48a0-813c-dde9e3af3744', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x140f8d220>]}
[0m19:24:07.880721 [info ] [MainThread]: Found 10 models, 4 data tests, 4 sources, 686 macros
[0m19:24:07.880825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67b78a2b-9100-48a0-813c-dde9e3af3744', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x140f9ea50>]}
[0m19:24:07.881548 [info ] [MainThread]: 
[0m19:24:07.881666 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:24:07.881752 [info ] [MainThread]: 
[0m19:24:07.881934 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:24:07.882028 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:24:07.884920 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:24:07.885061 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:24:07.889257 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:24:07.889401 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:24:07.889504 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:24:08.900819 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b3-f729-152b-bee7-c65bb04c6263) - Created
[0m19:24:26.427409 [debug] [ThreadPool]: SQL status: OK in 18.540 seconds
[0m19:24:26.438798 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b3-f729-152b-bee7-c65bb04c6263, command-id=01f089b3-f75a-18b0-8721-0223655edbb2) - Closing
[0m19:24:26.707936 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:24:26.708639 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b3-f729-152b-bee7-c65bb04c6263) - Closing
[0m19:24:26.928333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67b78a2b-9100-48a0-813c-dde9e3af3744', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x140f3ed00>]}
[0m19:24:26.932896 [debug] [Thread-1 (]: Began running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:24:26.933269 [debug] [Thread-4 (]: Began running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:24:26.933519 [debug] [Thread-2 (]: Began running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:24:26.933878 [debug] [Thread-3 (]: Began running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:24:26.934370 [info ] [Thread-1 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m19:24:26.934916 [info ] [Thread-4 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m19:24:26.935358 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m19:24:26.935723 [info ] [Thread-3 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m19:24:26.936246 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m19:24:26.936615 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m19:24:26.936939 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m19:24:26.937251 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m19:24:26.937494 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710'
[0m19:24:26.937708 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493'
[0m19:24:26.937908 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778'
[0m19:24:26.938110 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_first_dbt_model_id.16e066b321'
[0m19:24:26.938331 [debug] [Thread-1 (]: Began compiling node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:24:26.938554 [debug] [Thread-4 (]: Began compiling node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:24:26.938766 [debug] [Thread-2 (]: Began compiling node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:24:26.938962 [debug] [Thread-3 (]: Began compiling node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:24:26.959765 [debug] [Thread-4 (]: Writing injected SQL for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:24:26.962681 [debug] [Thread-1 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:24:26.964915 [debug] [Thread-2 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:24:26.966676 [debug] [Thread-3 (]: Writing injected SQL for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:24:26.967493 [debug] [Thread-3 (]: Began executing node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:24:26.967677 [debug] [Thread-4 (]: Began executing node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:24:26.967853 [debug] [Thread-1 (]: Began executing node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:24:26.968043 [debug] [Thread-2 (]: Began executing node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:24:26.976823 [debug] [Thread-3 (]: Writing runtime sql for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:24:26.977872 [debug] [Thread-4 (]: Writing runtime sql for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:24:26.978969 [debug] [Thread-1 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:24:26.979892 [debug] [Thread-2 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:24:26.980413 [debug] [Thread-1 (]: Using databricks connection "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:24:26.980582 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:24:26.980719 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:24:26.980825 [debug] [Thread-2 (]: Using databricks connection "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:24:26.980941 [debug] [Thread-3 (]: Using databricks connection "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:24:26.981164 [debug] [Thread-4 (]: Using databricks connection "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:24:26.981323 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:24:26.981524 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:24:26.981689 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:24:26.981830 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:24:26.981949 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:24:26.982064 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:24:27.720711 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b4-0273-1677-af1b-1e107bb462b2) - Created
[0m19:24:27.740403 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b4-0275-1082-ae87-a7dce7f654d3) - Created
[0m19:24:27.762952 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b4-0279-1b99-acb9-95f97af4b0f0) - Created
[0m19:24:27.789908 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b4-027d-1e23-b1b3-2e3c6a74df32) - Created
[0m19:24:34.607331 [debug] [Thread-2 (]: SQL status: OK in 7.630 seconds
[0m19:24:34.613130 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b4-027d-1e23-b1b3-2e3c6a74df32, command-id=01f089b4-029b-17c8-8352-01a20f95e244) - Closing
[0m19:24:34.935357 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: Close
[0m19:24:34.936404 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b4-027d-1e23-b1b3-2e3c6a74df32) - Closing
[0m19:24:35.182116 [info ] [Thread-2 (]: 2 of 4 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 8.24s]
[0m19:24:35.183610 [debug] [Thread-2 (]: Finished running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:24:36.617202 [debug] [Thread-1 (]: SQL status: OK in 9.640 seconds
[0m19:24:36.620738 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b4-0273-1677-af1b-1e107bb462b2, command-id=01f089b4-0291-1917-b40e-51f8f1e1834e) - Closing
[0m19:24:36.658243 [debug] [Thread-3 (]: SQL status: OK in 9.680 seconds
[0m19:24:36.661693 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b4-0275-1082-ae87-a7dce7f654d3, command-id=01f089b4-0293-127d-b685-2b175f116912) - Closing
[0m19:24:36.877308 [debug] [Thread-4 (]: SQL status: OK in 9.890 seconds
[0m19:24:36.880457 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b4-0279-1b99-acb9-95f97af4b0f0, command-id=01f089b4-0295-1e48-8e9c-453b606a5670) - Closing
[0m19:24:36.886945 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m19:24:36.887317 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b4-0273-1677-af1b-1e107bb462b2) - Closing
[0m19:24:37.089541 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: Close
[0m19:24:37.090491 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b4-0275-1082-ae87-a7dce7f654d3) - Closing
[0m19:24:37.304214 [error] [Thread-1 (]: 1 of 4 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 10.37s]
[0m19:24:37.304993 [debug] [Thread-1 (]: Finished running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:24:37.305328 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m19:24:37.305782 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b4-0279-1b99-acb9-95f97af4b0f0) - Closing
[0m19:24:37.504616 [info ] [Thread-3 (]: 3 of 4 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 10.57s]
[0m19:24:37.505126 [info ] [Thread-4 (]: 4 of 4 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 10.57s]
[0m19:24:37.505886 [debug] [Thread-3 (]: Finished running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:24:37.506404 [debug] [Thread-4 (]: Finished running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:24:37.507811 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:24:37.508123 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:24:37.508488 [info ] [MainThread]: 
[0m19:24:37.508760 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 29.63 seconds (29.63s).
[0m19:24:37.509717 [debug] [MainThread]: Command end result
[0m19:24:37.530151 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:24:37.531294 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:24:37.534141 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:24:37.534262 [info ] [MainThread]: 
[0m19:24:37.534392 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:24:37.534491 [info ] [MainThread]: 
[0m19:24:37.534621 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m19:24:37.534741 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m19:24:37.534824 [info ] [MainThread]: 
[0m19:24:37.534927 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m19:24:37.535014 [info ] [MainThread]: 
[0m19:24:37.535116 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m19:24:37.538241 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 30.738081, "process_in_blocks": "0", "process_kernel_time": 0.255372, "process_mem_max_rss": "233390080", "process_out_blocks": "0", "process_user_time": 1.763774}
[0m19:24:37.538455 [debug] [MainThread]: Command `dbt test` failed at 19:24:37.538414 after 30.74 seconds
[0m19:24:37.538637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072b2e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x140eca2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x140eca4c0>]}
[0m19:24:37.538794 [debug] [MainThread]: Flushing usage events
[0m19:24:38.157467 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:27:27.886953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104883620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060ff890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060ffb10>]}


============================== 19:27:27.889171 | 2fa1555c-a57a-4a1a-a154-13be9da8ead4 ==============================
[0m19:27:27.889171 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:27:27.889390 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'debug': 'False', 'warn_error': 'None', 'version_check': 'True', 'printer_width': '80', 'use_colors': 'True', 'target_path': 'None', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'partial_parse': 'True', 'no_print': 'None', 'fail_fast': 'False', 'log_cache_events': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'invocation_command': 'dbt test', 'quiet': 'False', 'introspect': 'True', 'static_parser': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'indirect_selection': 'eager'}
[0m19:27:28.204693 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:27:28.204978 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:27:28.205078 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:27:28.667104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2fa1555c-a57a-4a1a-a154-13be9da8ead4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f27360>]}
[0m19:27:28.686826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2fa1555c-a57a-4a1a-a154-13be9da8ead4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108002ad0>]}
[0m19:27:28.687104 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:27:28.737487 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:27:28.796356 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:27:28.796550 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:27:28.799747 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:27:28.814674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2fa1555c-a57a-4a1a-a154-13be9da8ead4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11870bd50>]}
[0m19:27:28.851370 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:27:28.852352 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:27:28.862826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2fa1555c-a57a-4a1a-a154-13be9da8ead4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1189f2a80>]}
[0m19:27:28.863027 [info ] [MainThread]: Found 10 models, 4 data tests, 4 sources, 686 macros
[0m19:27:28.863139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2fa1555c-a57a-4a1a-a154-13be9da8ead4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11899fbd0>]}
[0m19:27:28.863978 [info ] [MainThread]: 
[0m19:27:28.864129 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:27:28.864225 [info ] [MainThread]: 
[0m19:27:28.864439 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:27:28.864537 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:27:28.867129 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:27:28.867235 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:27:28.872210 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:27:28.872362 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:27:28.872468 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:27:29.692192 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b4-6ee7-1a30-8081-a7e6eac4f1da) - Created
[0m19:27:30.474978 [debug] [ThreadPool]: SQL status: OK in 1.600 seconds
[0m19:27:30.485453 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b4-6ee7-1a30-8081-a7e6eac4f1da, command-id=01f089b4-6f09-1b39-9ce6-3cbba8baea77) - Closing
[0m19:27:30.486199 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:27:30.486420 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b4-6ee7-1a30-8081-a7e6eac4f1da) - Closing
[0m19:27:30.687824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2fa1555c-a57a-4a1a-a154-13be9da8ead4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e24e0d0>]}
[0m19:27:30.692698 [debug] [Thread-4 (]: Began running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:27:30.693181 [debug] [Thread-3 (]: Began running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:27:30.693498 [debug] [Thread-1 (]: Began running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:27:30.693792 [debug] [Thread-2 (]: Began running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:27:30.694149 [info ] [Thread-4 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m19:27:30.694548 [info ] [Thread-3 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m19:27:30.694952 [info ] [Thread-1 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m19:27:30.695332 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m19:27:30.695980 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m19:27:30.696412 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m19:27:30.696805 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m19:27:30.697172 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m19:27:30.697487 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493'
[0m19:27:30.697732 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_first_dbt_model_id.16e066b321'
[0m19:27:30.697965 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710'
[0m19:27:30.698193 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778'
[0m19:27:30.698470 [debug] [Thread-4 (]: Began compiling node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:27:30.698713 [debug] [Thread-3 (]: Began compiling node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:27:30.698945 [debug] [Thread-1 (]: Began compiling node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:27:30.699169 [debug] [Thread-2 (]: Began compiling node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:27:30.714150 [debug] [Thread-3 (]: Writing injected SQL for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:27:30.718467 [debug] [Thread-4 (]: Writing injected SQL for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:27:30.723943 [debug] [Thread-1 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:27:30.726117 [debug] [Thread-2 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:27:30.726928 [debug] [Thread-4 (]: Began executing node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:27:30.727129 [debug] [Thread-1 (]: Began executing node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:27:30.727309 [debug] [Thread-2 (]: Began executing node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:27:30.727475 [debug] [Thread-3 (]: Began executing node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:27:30.736826 [debug] [Thread-4 (]: Writing runtime sql for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:27:30.737929 [debug] [Thread-1 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:27:30.739106 [debug] [Thread-2 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:27:30.740088 [debug] [Thread-3 (]: Writing runtime sql for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:27:30.740533 [debug] [Thread-1 (]: Using databricks connection "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:27:30.740681 [debug] [Thread-2 (]: Using databricks connection "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:27:30.740855 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:27:30.741046 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:27:30.741177 [debug] [Thread-4 (]: Using databricks connection "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:27:30.741301 [debug] [Thread-3 (]: Using databricks connection "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:27:30.741438 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:27:30.741572 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:27:30.741859 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:27:30.742051 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:27:30.742368 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:27:30.742572 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:27:31.740952 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b4-7022-1997-aa0e-1779aa0219f2) - Created
[0m19:27:31.743315 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b4-7023-1b8c-87df-50149cb4663e) - Created
[0m19:27:31.745107 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b4-7024-1244-bd72-f9d364343945) - Created
[0m19:27:31.752292 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b4-7024-11c3-85ef-ea9aa5775e58) - Created
[0m19:27:32.199440 [debug] [Thread-2 (]: SQL status: OK in 1.460 seconds
[0m19:27:32.200014 [debug] [Thread-4 (]: SQL status: OK in 1.460 seconds
[0m19:27:32.205820 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b4-7024-1244-bd72-f9d364343945, command-id=01f089b4-7043-1035-9227-d8ccae663aa8) - Closing
[0m19:27:32.207117 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b4-7022-1997-aa0e-1779aa0219f2, command-id=01f089b4-7041-1249-bb80-41542f562aeb) - Closing
[0m19:27:32.210744 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: Close
[0m19:27:32.211337 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b4-7024-1244-bd72-f9d364343945) - Closing
[0m19:27:32.245428 [debug] [Thread-3 (]: SQL status: OK in 1.500 seconds
[0m19:27:32.248466 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b4-7024-11c3-85ef-ea9aa5775e58, command-id=01f089b4-7042-1c63-a986-622dce3a8c97) - Closing
[0m19:27:32.271745 [debug] [Thread-1 (]: SQL status: OK in 1.530 seconds
[0m19:27:32.274439 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b4-7023-1b8c-87df-50149cb4663e, command-id=01f089b4-7042-1e08-9850-13789a7777a0) - Closing
[0m19:27:32.431313 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m19:27:32.432387 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b4-7022-1997-aa0e-1779aa0219f2) - Closing
[0m19:27:32.638451 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: Close
[0m19:27:32.639459 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b4-7024-11c3-85ef-ea9aa5775e58) - Closing
[0m19:27:32.872625 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m19:27:32.873662 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b4-7023-1b8c-87df-50149cb4663e) - Closing
[0m19:27:33.097547 [info ] [Thread-2 (]: 2 of 4 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 2.40s]
[0m19:27:33.098244 [info ] [Thread-4 (]: 4 of 4 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 2.40s]
[0m19:27:33.099729 [debug] [Thread-2 (]: Finished running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:27:33.098808 [info ] [Thread-3 (]: 3 of 4 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 2.40s]
[0m19:27:33.099240 [error] [Thread-1 (]: 1 of 4 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 2.40s]
[0m19:27:33.100429 [debug] [Thread-4 (]: Finished running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:27:33.101123 [debug] [Thread-3 (]: Finished running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:27:33.101531 [debug] [Thread-1 (]: Finished running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:27:33.103272 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:27:33.103557 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:27:33.103940 [info ] [MainThread]: 
[0m19:27:33.104201 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 4.24 seconds (4.24s).
[0m19:27:33.105116 [debug] [MainThread]: Command end result
[0m19:27:33.136686 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:27:33.137736 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:27:33.141486 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:27:33.141645 [info ] [MainThread]: 
[0m19:27:33.141810 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:27:33.141925 [info ] [MainThread]: 
[0m19:27:33.142075 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m19:27:33.142209 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m19:27:33.142308 [info ] [MainThread]: 
[0m19:27:33.142432 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m19:27:33.142532 [info ] [MainThread]: 
[0m19:27:33.142652 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m19:27:33.145122 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 5.288659, "process_in_blocks": "0", "process_kernel_time": 0.243223, "process_mem_max_rss": "239140864", "process_out_blocks": "0", "process_user_time": 1.826276}
[0m19:27:33.145322 [debug] [MainThread]: Command `dbt test` failed at 19:27:33.145284 after 5.29 seconds
[0m19:27:33.145486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105db2e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a454f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a455a0>]}
[0m19:27:33.145630 [debug] [MainThread]: Flushing usage events
[0m19:27:34.054507 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:30:09.326288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c67620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070fbb10>]}


============================== 19:30:09.328656 | c40a5449-366b-474a-9277-755384315cfd ==============================
[0m19:30:09.328656 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:30:09.328884 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'introspect': 'True', 'debug': 'False', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'write_json': 'True', 'no_print': 'None', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'printer_width': '80', 'empty': 'False', 'invocation_command': 'dbt run', 'log_cache_events': 'False', 'quiet': 'False', 'cache_selected_only': 'False'}
[0m19:30:09.632937 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:30:09.633156 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:30:09.633251 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:30:10.097816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105faf360>]}
[0m19:30:10.117182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108806ad0>]}
[0m19:30:10.117418 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:30:10.171083 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:30:10.229768 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:30:10.229947 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:30:10.232907 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:30:10.247538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118d0ae50>]}
[0m19:30:10.284274 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:30:10.285253 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:30:10.290408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118807890>]}
[0m19:30:10.290581 [info ] [MainThread]: Found 10 models, 4 data tests, 4 sources, 686 macros
[0m19:30:10.290683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118ef1c50>]}
[0m19:30:10.291457 [info ] [MainThread]: 
[0m19:30:10.291577 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:30:10.291659 [info ] [MainThread]: 
[0m19:30:10.291845 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:30:10.291938 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:30:10.294460 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m19:30:10.294567 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m19:30:10.298931 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m19:30:10.299130 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m19:30:10.299241 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:30:11.068444 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b4-cf1c-17be-9e70-50c1d7472f65) - Created
[0m19:30:11.546722 [debug] [ThreadPool]: SQL status: OK in 1.250 seconds
[0m19:30:11.556671 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b4-cf1c-17be-9e70-50c1d7472f65, command-id=01f089b4-cf37-133a-9162-4357706aab1b) - Closing
[0m19:30:11.557238 [debug] [ThreadPool]: On list_flight_db: Close
[0m19:30:11.557473 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b4-cf1c-17be-9e70-50c1d7472f65) - Closing
[0m19:30:11.771015 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:30:11.771601 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:30:11.783237 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:30:11.783615 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:30:11.783841 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:30:12.542450 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b4-cff9-1e3c-b500-ed299f06d152) - Created
[0m19:30:13.247384 [debug] [ThreadPool]: SQL status: OK in 1.460 seconds
[0m19:30:13.250680 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b4-cff9-1e3c-b500-ed299f06d152, command-id=01f089b4-d01a-1214-9c52-2e15293745b0) - Closing
[0m19:30:13.251391 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:30:13.251640 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b4-cff9-1e3c-b500-ed299f06d152) - Closing
[0m19:30:13.466076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118fcbba0>]}
[0m19:30:13.470740 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m19:30:13.471207 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m19:30:13.471566 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m19:30:13.471830 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m19:30:13.472345 [info ] [Thread-2 (]: 2 of 10 START sql view model raw.bronze_airports ............................... [RUN]
[0m19:30:13.472747 [info ] [Thread-1 (]: 1 of 10 START sql view model raw.bronze_airlines ............................... [RUN]
[0m19:30:13.473139 [info ] [Thread-4 (]: 4 of 10 START sql table model raw.my_first_dbt_model ........................... [RUN]
[0m19:30:13.473554 [info ] [Thread-3 (]: 3 of 10 START sql view model raw.bronze_flights ................................ [RUN]
[0m19:30:13.474164 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m19:30:13.474578 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m19:30:13.474932 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m19:30:13.475278 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m19:30:13.475560 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m19:30:13.475781 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m19:30:13.475999 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m19:30:13.476199 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m19:30:13.476433 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m19:30:13.476650 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m19:30:13.476851 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m19:30:13.477051 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m19:30:13.485956 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m19:30:13.489143 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m19:30:13.490972 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m19:30:13.493058 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m19:30:13.494083 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m19:30:13.494359 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m19:30:13.494593 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m19:30:13.494784 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m19:30:13.504237 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:30:13.513049 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m19:30:13.513944 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:30:13.514916 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:30:13.515874 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:30:13.516115 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:30:13.516333 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:30:13.516540 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:30:13.516732 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118fc5910>]}
[0m19:30:13.516861 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e8a360>]}
[0m19:30:13.516976 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e8a8e0>]}
[0m19:30:13.517091 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1191bf070>]}
[0m19:30:13.523112 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m19:30:13.539465 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m19:30:13.539799 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m19:30:13.540105 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m19:30:13.540387 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m19:30:13.540706 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m19:30:13.540975 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m19:30:13.541457 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m19:30:13.541565 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m19:30:13.541673 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m19:30:13.541817 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`raw`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:30:13.541918 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m19:30:13.542036 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m19:30:13.542165 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m19:30:13.542276 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:30:13.542389 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    -- Bronze layer: just expose the raw airlines table
SELECT *
FROM flight_db.raw.airlines
  )

[0m19:30:13.542499 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:30:13.542605 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:30:13.542769 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:30:14.273972 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b4-d106-1856-8c46-3e252fdd867c) - Created
[0m19:30:14.304597 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b4-d107-1541-be3f-78dcdcdc0c62) - Created
[0m19:30:14.313167 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b4-d10b-16bb-a570-9c2cec675ddc) - Created
[0m19:30:14.329285 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b4-d10d-19bf-a889-e9801f8226e6) - Created
[0m19:30:14.799790 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    -- Bronze layer: just expose the raw airlines table
SELECT *
FROM flight_db.raw.airlines
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f089b4-d126-1cdc-bae8-e2e29eb021b7
[0m19:30:14.801923 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m19:30:14.802523 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b4-d10b-16bb-a570-9c2cec675ddc) - Closing
[0m19:30:15.067258 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:30:15.069309 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1196752b0>]}
[0m19:30:15.069947 [error] [Thread-1 (]: 1 of 10 ERROR creating sql view model raw.bronze_airlines ...................... [[31mERROR[0m in 1.59s]
[0m19:30:15.070425 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m19:30:15.070726 [debug] [Thread-1 (]: Began running node model.flights_dbt.stg_airlines
[0m19:30:15.071174 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m19:30:15.071618 [info ] [Thread-1 (]: 5 of 10 START sql table model raw.stg_airlines ................................. [RUN]
[0m19:30:15.072919 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.stg_airlines) - Creating connection
[0m19:30:15.073171 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.stg_airlines'
[0m19:30:15.073379 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.stg_airlines
[0m19:30:15.076448 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.stg_airlines"
[0m19:30:15.076974 [debug] [Thread-1 (]: Began executing node model.flights_dbt.stg_airlines
[0m19:30:15.078335 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m19:30:15.079509 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.stg_airlines"
[0m19:30:15.079860 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.stg_airlines"
[0m19:30:15.080054 [debug] [Thread-1 (]: On model.flights_dbt.stg_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.stg_airlines"} */

  
    
        create or replace table `flight_db`.`raw`.`stg_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
  1            as airline_id,
  'Test Air'   as airline_name
  
[0m19:30:15.080237 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:30:15.258091 [debug] [Thread-3 (]: SQL status: OK in 1.720 seconds
[0m19:30:15.259105 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b4-d107-1541-be3f-78dcdcdc0c62, command-id=01f089b4-d124-163b-90d9-297551c12b26) - Closing
[0m19:30:15.268226 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:30:15.272088 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m19:30:15.272364 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b4-d107-1541-be3f-78dcdcdc0c62) - Closing
[0m19:30:15.273740 [debug] [Thread-2 (]: SQL status: OK in 1.730 seconds
[0m19:30:15.274451 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b4-d10d-19bf-a889-e9801f8226e6, command-id=01f089b4-d12a-10a1-bb94-5606d3f14618) - Closing
[0m19:30:15.274880 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:30:15.474075 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m19:30:15.474758 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b4-d10d-19bf-a889-e9801f8226e6) - Closing
[0m19:30:15.681853 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e1e870>]}
[0m19:30:15.682884 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118fd9e50>]}
[0m19:30:15.684019 [info ] [Thread-3 (]: 3 of 10 OK created sql view model raw.bronze_flights ........................... [[32mOK[0m in 2.21s]
[0m19:30:15.684608 [info ] [Thread-2 (]: 2 of 10 OK created sql view model raw.bronze_airports .......................... [[32mOK[0m in 2.21s]
[0m19:30:15.685344 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m19:30:15.685794 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m19:30:15.686195 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airlines
[0m19:30:15.687191 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airports
[0m19:30:15.686882 [info ] [Thread-3 (]: 6 of 10 SKIP relation raw.silver_airlines ...................................... [[33mSKIP[0m]
[0m19:30:15.687728 [info ] [Thread-2 (]: 7 of 10 START sql view model raw.silver_airports ............................... [RUN]
[0m19:30:15.688112 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airlines
[0m19:30:15.688691 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m19:30:15.689070 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_flights
[0m19:30:15.689466 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m19:30:15.689937 [info ] [Thread-3 (]: 8 of 10 START sql view model raw.silver_flights ................................ [RUN]
[0m19:30:15.690374 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.silver_airports
[0m19:30:15.690939 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m19:30:15.695123 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m19:30:15.695530 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m19:30:15.695877 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_flights
[0m19:30:15.699465 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m19:30:15.700044 [debug] [Thread-2 (]: Began executing node model.flights_dbt.silver_airports
[0m19:30:15.701987 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:30:15.703211 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`silver_airports`
[0m19:30:15.703860 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m19:30:15.704106 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_flights
[0m19:30:15.709358 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m19:30:15.710198 [debug] [Thread-2 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`bronze_airports`
  )

[0m19:30:15.714142 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:30:15.714358 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:30:15.714969 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`silver_flights`
[0m19:30:15.715646 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m19:30:15.716556 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m19:30:15.716793 [debug] [Thread-3 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    select
    f.flight_id,
    f.airline,
    f.origin,
    f.dep_time,
    f.arr_time,
    f.arr_delay
from `flight_db`.`raw`.`bronze_flights` as f
  )

[0m19:30:15.716969 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:30:15.741020 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b4-d1e5-1cd4-a23f-570767d036a6) - Created
[0m19:30:16.362459 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b4-d244-1431-a14c-481f620a95b5) - Created
[0m19:30:16.388256 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b4-d248-1061-ab6f-ae0fccbfeeb5) - Created
[0m19:30:16.694178 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    select
    f.flight_id,
    f.airline,
    f.origin,
    f.dep_time,
    f.arr_time,
    f.arr_delay
from `flight_db`.`raw`.`bronze_flights` as f
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f089b4-d25e-19b8-a90c-228be9400c5c
[0m19:30:16.695485 [debug] [Thread-3 (]: On model.flights_dbt.silver_flights: Close
[0m19:30:16.695848 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b4-d244-1431-a14c-481f620a95b5) - Closing
[0m19:30:16.891767 [debug] [Thread-3 (]: Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:30:16.892410 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e614f0>]}
[0m19:30:16.892996 [error] [Thread-3 (]: 8 of 10 ERROR creating sql view model raw.silver_flights ....................... [[31mERROR[0m in 1.20s]
[0m19:30:16.893423 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_flights
[0m19:30:16.893741 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_flights' to be skipped because of status 'error'.  Reason: Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql.
[0m19:30:17.166309 [debug] [Thread-2 (]: SQL status: OK in 1.450 seconds
[0m19:30:17.168794 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b4-d248-1061-ab6f-ae0fccbfeeb5, command-id=01f089b4-d263-1996-9aac-4c11587cfb47) - Closing
[0m19:30:17.170379 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:30:17.171738 [debug] [Thread-2 (]: On model.flights_dbt.silver_airports: Close
[0m19:30:17.172187 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b4-d248-1061-ab6f-ae0fccbfeeb5) - Closing
[0m19:30:17.379632 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a05f70>]}
[0m19:30:17.381471 [info ] [Thread-2 (]: 7 of 10 OK created sql view model raw.silver_airports .......................... [[32mOK[0m in 1.69s]
[0m19:30:17.382072 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airports
[0m19:30:17.382739 [debug] [Thread-3 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m19:30:17.383166 [info ] [Thread-3 (]: 9 of 10 SKIP relation raw.gold_flight_metrics .................................. [[33mSKIP[0m]
[0m19:30:17.383535 [debug] [Thread-3 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m19:30:21.012007 [debug] [Thread-1 (]: SQL status: OK in 5.930 seconds
[0m19:30:21.014191 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b4-d1e5-1cd4-a23f-570767d036a6, command-id=01f089b4-d1ff-1e51-b393-e5f1a00fb1ce) - Closing
[0m19:30:21.015156 [debug] [Thread-1 (]: Applying tags to relation None
[0m19:30:21.034594 [debug] [Thread-1 (]: On model.flights_dbt.stg_airlines: Close
[0m19:30:21.035020 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b4-d1e5-1cd4-a23f-570767d036a6) - Closing
[0m19:30:21.233675 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118fd9af0>]}
[0m19:30:21.235364 [info ] [Thread-1 (]: 5 of 10 OK created sql table model raw.stg_airlines ............................ [[32mOK[0m in 6.16s]
[0m19:30:21.236478 [debug] [Thread-1 (]: Finished running node model.flights_dbt.stg_airlines
[0m19:30:21.696385 [debug] [Thread-4 (]: SQL status: OK in 8.150 seconds
[0m19:30:21.699006 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b4-d106-1856-8c46-3e252fdd867c, command-id=01f089b4-d120-1fc2-a888-d456909fd065) - Closing
[0m19:30:21.913792 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:30:21.915552 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m19:30:21.915933 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b4-d106-1856-8c46-3e252fdd867c) - Closing
[0m19:30:22.123085 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118ae6870>]}
[0m19:30:22.124878 [info ] [Thread-4 (]: 4 of 10 OK created sql table model raw.my_first_dbt_model ...................... [[32mOK[0m in 8.65s]
[0m19:30:22.125993 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m19:30:22.127204 [debug] [Thread-2 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m19:30:22.127801 [info ] [Thread-2 (]: 10 of 10 START sql view model raw.my_second_dbt_model .......................... [RUN]
[0m19:30:22.128525 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m19:30:22.128881 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m19:30:22.129178 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m19:30:22.138691 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m19:30:22.140009 [debug] [Thread-2 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m19:30:22.142250 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:30:22.143313 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`my_second_dbt_model`
[0m19:30:22.144052 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m19:30:22.144708 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m19:30:22.145014 [debug] [Thread-2 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`raw`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`raw`.`my_first_dbt_model`
where id = 1
  )

[0m19:30:22.145270 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:30:23.043833 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b4-d636-1f4b-9d1c-377a856507c1) - Created
[0m19:30:23.678856 [debug] [Thread-2 (]: SQL status: OK in 1.530 seconds
[0m19:30:23.680820 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b4-d636-1f4b-9d1c-377a856507c1, command-id=01f089b4-d65b-11dd-98ec-7bb729b7bfbc) - Closing
[0m19:30:23.681582 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:30:23.682763 [debug] [Thread-2 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m19:30:23.683352 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b4-d636-1f4b-9d1c-377a856507c1) - Closing
[0m19:30:23.873120 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c40a5449-366b-474a-9277-755384315cfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118ae69f0>]}
[0m19:30:23.874857 [info ] [Thread-2 (]: 10 of 10 OK created sql view model raw.my_second_dbt_model ..................... [[32mOK[0m in 1.74s]
[0m19:30:23.875792 [debug] [Thread-2 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m19:30:23.877745 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:30:23.878117 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:30:23.878636 [info ] [MainThread]: 
[0m19:30:23.878958 [info ] [MainThread]: Finished running 3 table models, 7 view models in 0 hours 0 minutes and 13.59 seconds (13.59s).
[0m19:30:23.880584 [debug] [MainThread]: Command end result
[0m19:30:23.911510 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:30:23.913084 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:30:23.917394 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:30:23.917647 [info ] [MainThread]: 
[0m19:30:23.917876 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m19:30:23.918021 [info ] [MainThread]: 
[0m19:30:23.918186 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m19:30:23.918350 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:30:23.918476 [info ] [MainThread]: 
[0m19:30:23.918606 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:30:23.918712 [info ] [MainThread]: 
[0m19:30:23.918845 [error] [MainThread]: [31mFailure in model silver_flights (models/silver/silver_flights.sql)[0m
[0m19:30:23.918985 [error] [MainThread]:   Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:30:23.919089 [info ] [MainThread]: 
[0m19:30:23.919215 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_flights.sql
[0m19:30:23.919317 [info ] [MainThread]: 
[0m19:30:23.919446 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=10
[0m19:30:23.922621 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 14.6282, "process_in_blocks": "0", "process_kernel_time": 0.275563, "process_mem_max_rss": "227229696", "process_out_blocks": "0", "process_user_time": 2.286094}
[0m19:30:23.922860 [debug] [MainThread]: Command `dbt run` failed at 19:30:23.922814 after 14.63 seconds
[0m19:30:23.923051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e92ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e76bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118fd9c10>]}
[0m19:30:23.923209 [debug] [MainThread]: Flushing usage events
[0m19:30:24.689050 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:34:01.031274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104113620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053fbb10>]}


============================== 19:34:01.033648 | e10031c1-6117-4aab-80fe-e79f6c4a0890 ==============================
[0m19:34:01.033648 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:34:01.033903 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'fail_fast': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'printer_width': '80', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'use_colors': 'True', 'version_check': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'debug': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'warn_error': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'static_parser': 'True', 'quiet': 'False', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run', 'no_print': 'None', 'send_anonymous_usage_stats': 'True'}
[0m19:34:01.336982 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:34:01.337167 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:34:01.337260 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:34:01.779965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10445b360>]}
[0m19:34:01.799295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107206ad0>]}
[0m19:34:01.799536 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:34:01.851571 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:34:01.909844 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:34:01.910019 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:34:01.913072 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:34:01.927712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11650ae50>]}
[0m19:34:01.965512 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:34:01.966503 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:34:01.971885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11600b890>]}
[0m19:34:01.972069 [info ] [MainThread]: Found 10 models, 4 data tests, 4 sources, 686 macros
[0m19:34:01.972180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166f1c50>]}
[0m19:34:01.972930 [info ] [MainThread]: 
[0m19:34:01.973048 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:34:01.973134 [info ] [MainThread]: 
[0m19:34:01.973323 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:34:01.973419 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:34:01.975904 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m19:34:01.976018 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m19:34:01.980205 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m19:34:01.980364 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m19:34:01.980477 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:34:02.723937 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-5930-14f7-80d2-c15ea07a479e) - Created
[0m19:34:03.171397 [debug] [ThreadPool]: SQL status: OK in 1.190 seconds
[0m19:34:03.183191 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b5-5930-14f7-80d2-c15ea07a479e, command-id=01f089b5-594a-1b2c-94c0-2fb6ed871c9b) - Closing
[0m19:34:03.183742 [debug] [ThreadPool]: On list_flight_db: Close
[0m19:34:03.183974 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-5930-14f7-80d2-c15ea07a479e) - Closing
[0m19:34:03.386645 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:34:03.387201 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:34:03.399981 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:34:03.400321 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:34:03.400537 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:34:04.094804 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-59ff-1914-ac24-1ba4102a0ca6) - Created
[0m19:34:04.701520 [debug] [ThreadPool]: SQL status: OK in 1.300 seconds
[0m19:34:04.704799 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b5-59ff-1914-ac24-1ba4102a0ca6, command-id=01f089b5-5a1c-19e8-a1ee-0cf9af219600) - Closing
[0m19:34:04.705747 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:34:04.706083 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-59ff-1914-ac24-1ba4102a0ca6) - Closing
[0m19:34:04.943452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1167cb790>]}
[0m19:34:04.948048 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m19:34:04.948498 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m19:34:04.948822 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m19:34:04.949108 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m19:34:04.949689 [info ] [Thread-4 (]: 4 of 10 START sql table model raw.my_first_dbt_model ........................... [RUN]
[0m19:34:04.950178 [info ] [Thread-1 (]: 1 of 10 START sql view model raw.bronze_airlines ............................... [RUN]
[0m19:34:04.950599 [info ] [Thread-3 (]: 3 of 10 START sql view model raw.bronze_flights ................................ [RUN]
[0m19:34:04.951068 [info ] [Thread-2 (]: 2 of 10 START sql view model raw.bronze_airports ............................... [RUN]
[0m19:34:04.951729 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m19:34:04.952183 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m19:34:04.952582 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m19:34:04.952958 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m19:34:04.953287 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m19:34:04.953568 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m19:34:04.953852 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m19:34:04.954116 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m19:34:04.954429 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m19:34:04.954707 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m19:34:04.954996 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m19:34:04.955248 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m19:34:04.964175 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m19:34:04.967471 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m19:34:04.969614 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m19:34:04.972073 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m19:34:04.973495 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m19:34:04.973818 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m19:34:04.974028 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m19:34:04.974198 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m19:34:04.985638 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m19:34:04.993256 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:34:04.994122 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:34:04.995046 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:34:04.995291 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:34:04.996199 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:34:04.996470 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:34:04.996912 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1167c5850>]}
[0m19:34:04.996695 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:34:04.997103 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11668a620>]}
[0m19:34:04.997235 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11668a780>]}
[0m19:34:05.003671 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1170bd3b0>]}
[0m19:34:05.016712 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m19:34:05.020564 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m19:34:05.020950 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m19:34:05.021279 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m19:34:05.021569 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m19:34:05.021879 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m19:34:05.022172 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m19:34:05.022359 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m19:34:05.022577 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`raw`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:34:05.022699 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:34:05.022789 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m19:34:05.022918 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m19:34:05.023157 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m19:34:05.023302 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m19:34:05.023430 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m19:34:05.023573 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:34:05.023704 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    -- Bronze layer: just expose the raw airlines table
SELECT *
FROM flight_db.raw.airlines
  )

[0m19:34:05.023816 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:34:05.023977 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:34:05.855770 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-5b0d-1fea-bc45-d1fe45c68f96) - Created
[0m19:34:05.879221 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-5b10-1d4e-aa79-bea2c1a39360) - Created
[0m19:34:06.031083 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-5b24-1885-a9d1-f7725fcf0fef) - Created
[0m19:34:06.033386 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-5b26-1250-a130-23af36498d26) - Created
[0m19:34:06.546718 [debug] [Thread-3 (]: SQL status: OK in 1.520 seconds
[0m19:34:06.548909 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b5-5b0d-1fea-bc45-d1fe45c68f96, command-id=01f089b5-5b31-1700-8d25-589373ffaeb6) - Closing
[0m19:34:06.558969 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:34:06.560933 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m19:34:06.561162 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-5b0d-1fea-bc45-d1fe45c68f96) - Closing
[0m19:34:06.566242 [debug] [Thread-2 (]: SQL status: OK in 1.540 seconds
[0m19:34:06.566900 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b5-5b10-1d4e-aa79-bea2c1a39360, command-id=01f089b5-5b31-16a3-be42-7fb849273cda) - Closing
[0m19:34:06.567314 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:34:06.642112 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    -- Bronze layer: just expose the raw airlines table
SELECT *
FROM flight_db.raw.airlines
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f089b5-5b45-19b6-8cd3-9ab875e89158
[0m19:34:06.787563 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m19:34:06.788593 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-5b10-1d4e-aa79-bea2c1a39360) - Closing
[0m19:34:07.234206 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m19:34:07.235276 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-5b26-1250-a130-23af36498d26) - Closing
[0m19:34:07.478664 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1170c2a50>]}
[0m19:34:07.479456 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1162e72f0>]}
[0m19:34:07.480233 [info ] [Thread-2 (]: 2 of 10 OK created sql view model raw.bronze_airports .......................... [[32mOK[0m in 2.52s]
[0m19:34:07.481470 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m19:34:07.480951 [info ] [Thread-3 (]: 3 of 10 OK created sql view model raw.bronze_flights ........................... [[32mOK[0m in 2.52s]
[0m19:34:07.482031 [debug] [Thread-2 (]: Began running node model.flights_dbt.stg_airlines
[0m19:34:07.482655 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m19:34:07.483204 [info ] [Thread-2 (]: 5 of 10 START sql table model raw.stg_airlines ................................. [RUN]
[0m19:34:07.483675 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airports
[0m19:34:07.484269 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.stg_airlines) - Creating connection
[0m19:34:07.484705 [info ] [Thread-3 (]: 6 of 10 START sql view model raw.silver_airports ............................... [RUN]
[0m19:34:07.485074 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.stg_airlines'
[0m19:34:07.485499 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m19:34:07.485818 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.stg_airlines
[0m19:34:07.486084 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m19:34:07.489503 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.stg_airlines"
[0m19:34:07.489921 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_airports
[0m19:34:07.493340 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m19:34:07.493874 [debug] [Thread-2 (]: Began executing node model.flights_dbt.stg_airlines
[0m19:34:07.495572 [debug] [Thread-2 (]: MATERIALIZING TABLE
[0m19:34:07.496960 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.stg_airlines"
[0m19:34:07.497356 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_airports
[0m19:34:07.498955 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:34:07.499639 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`silver_airports`
[0m19:34:07.499845 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.stg_airlines"
[0m19:34:07.500405 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m19:34:07.500779 [debug] [Thread-2 (]: On model.flights_dbt.stg_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.stg_airlines"} */

  
    
        create or replace table `flight_db`.`raw`.`stg_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
  1            as airline_id,
  'Test Air'   as airline_name
  
[0m19:34:07.501088 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:34:07.501545 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m19:34:07.501874 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`bronze_airports`
  )

[0m19:34:07.502081 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:34:07.507636 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:34:07.507949 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1167cfdd0>]}
[0m19:34:07.508294 [error] [Thread-1 (]: 1 of 10 ERROR creating sql view model raw.bronze_airlines ...................... [[31mERROR[0m in 2.56s]
[0m19:34:07.508595 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m19:34:07.508773 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_flights
[0m19:34:07.509022 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m19:34:07.509268 [info ] [Thread-1 (]: 7 of 10 START sql view model raw.silver_flights ................................ [RUN]
[0m19:34:07.509953 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m19:34:07.510126 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m19:34:07.510274 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_flights
[0m19:34:07.511797 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m19:34:07.512099 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_flights
[0m19:34:07.518223 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:34:07.518771 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`silver_flights`
[0m19:34:07.519136 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m19:34:07.519406 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m19:34:07.519570 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    select
    f.flight_id,
    f.airline,
    f.origin,
    f.dep_time,
    f.arr_time,
    f.arr_delay
from `flight_db`.`raw`.`bronze_flights` as f
  )

[0m19:34:07.519712 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:34:08.136459 [debug] [Thread-4 (]: SQL status: OK in 3.110 seconds
[0m19:34:08.138261 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b5-5b24-1885-a9d1-f7725fcf0fef, command-id=01f089b5-5b45-111d-bad4-dc1de20d414b) - Closing
[0m19:34:08.143214 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:34:08.159355 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m19:34:08.159702 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-5b24-1885-a9d1-f7725fcf0fef) - Closing
[0m19:34:08.335876 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-5c89-1b33-b34e-012802e865bb) - Created
[0m19:34:08.340376 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-5c88-1476-b61c-ca067fc055ac) - Created
[0m19:34:08.342229 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-5c89-110f-b799-6a5375cf220f) - Created
[0m19:34:08.358720 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11661fd10>]}
[0m19:34:08.360366 [info ] [Thread-4 (]: 4 of 10 OK created sql table model raw.my_first_dbt_model ...................... [[32mOK[0m in 3.41s]
[0m19:34:08.361422 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m19:34:08.362113 [debug] [Thread-4 (]: Began running node model.flights_dbt.silver_airlines
[0m19:34:08.363027 [info ] [Thread-4 (]: 8 of 10 SKIP relation raw.silver_airlines ...................................... [[33mSKIP[0m]
[0m19:34:08.363738 [debug] [Thread-4 (]: Finished running node model.flights_dbt.silver_airlines
[0m19:34:08.364050 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m19:34:08.364533 [info ] [Thread-4 (]: 9 of 10 START sql view model raw.my_second_dbt_model ........................... [RUN]
[0m19:34:08.365174 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m19:34:08.365521 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m19:34:08.365825 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m19:34:08.371035 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m19:34:08.372094 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m19:34:08.375010 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m19:34:08.376246 [debug] [Thread-4 (]: Creating view `flight_db`.`raw`.`my_second_dbt_model`
[0m19:34:08.376978 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m19:34:08.377906 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m19:34:08.378335 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`raw`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`raw`.`my_first_dbt_model`
where id = 1
  )

[0m19:34:08.378634 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:34:08.663299 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    select
    f.flight_id,
    f.airline,
    f.origin,
    f.dep_time,
    f.arr_time,
    f.arr_delay
from `flight_db`.`raw`.`bronze_flights` as f
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f089b5-5ca3-19a1-820f-24629b930dc0
[0m19:34:08.664638 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: Close
[0m19:34:08.664948 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-5c88-1476-b61c-ca067fc055ac) - Closing
[0m19:34:08.872465 [debug] [Thread-1 (]: Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:34:08.872947 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116325c10>]}
[0m19:34:08.873554 [error] [Thread-1 (]: 7 of 10 ERROR creating sql view model raw.silver_flights ....................... [[31mERROR[0m in 1.36s]
[0m19:34:08.874009 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_flights
[0m19:34:08.874363 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_flights' to be skipped because of status 'error'.  Reason: Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql.
[0m19:34:09.013556 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-5cf0-1d23-a129-cfaa7758a23a) - Created
[0m19:34:09.018345 [debug] [Thread-3 (]: SQL status: OK in 1.520 seconds
[0m19:34:09.019414 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b5-5c89-110f-b799-6a5375cf220f, command-id=01f089b5-5ca5-14e6-ac10-bfe6767c18ed) - Closing
[0m19:34:09.020081 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:34:09.020781 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: Close
[0m19:34:09.021070 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-5c89-110f-b799-6a5375cf220f) - Closing
[0m19:34:09.214464 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1218ab710>]}
[0m19:34:09.216154 [info ] [Thread-3 (]: 6 of 10 OK created sql view model raw.silver_airports .......................... [[32mOK[0m in 1.73s]
[0m19:34:09.216903 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airports
[0m19:34:09.217465 [debug] [Thread-1 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m19:34:09.217767 [info ] [Thread-1 (]: 10 of 10 SKIP relation raw.gold_flight_metrics ................................. [[33mSKIP[0m]
[0m19:34:09.218058 [debug] [Thread-1 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m19:34:09.591628 [debug] [Thread-4 (]: SQL status: OK in 1.210 seconds
[0m19:34:09.593698 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b5-5cf0-1d23-a129-cfaa7758a23a, command-id=01f089b5-5d0a-1a0e-bd4f-7f0f2902bcd9) - Closing
[0m19:34:09.594357 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:34:09.595007 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m19:34:09.595280 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-5cf0-1d23-a129-cfaa7758a23a) - Closing
[0m19:34:09.792406 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1218b6cf0>]}
[0m19:34:09.794142 [info ] [Thread-4 (]: 9 of 10 OK created sql view model raw.my_second_dbt_model ...................... [[32mOK[0m in 1.43s]
[0m19:34:09.794960 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m19:34:10.165811 [debug] [Thread-2 (]: SQL status: OK in 2.660 seconds
[0m19:34:10.168333 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b5-5c89-1b33-b34e-012802e865bb, command-id=01f089b5-5ca3-1975-891b-d8d6c29d7a42) - Closing
[0m19:34:10.168941 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:34:10.169765 [debug] [Thread-2 (]: On model.flights_dbt.stg_airlines: Close
[0m19:34:10.169975 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-5c89-1b33-b34e-012802e865bb) - Closing
[0m19:34:10.387390 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e10031c1-6117-4aab-80fe-e79f6c4a0890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116773b30>]}
[0m19:34:10.389126 [info ] [Thread-2 (]: 5 of 10 OK created sql table model raw.stg_airlines ............................ [[32mOK[0m in 2.90s]
[0m19:34:10.389941 [debug] [Thread-2 (]: Finished running node model.flights_dbt.stg_airlines
[0m19:34:10.391721 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:34:10.392084 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:34:10.392639 [info ] [MainThread]: 
[0m19:34:10.392980 [info ] [MainThread]: Finished running 3 table models, 7 view models in 0 hours 0 minutes and 8.42 seconds (8.42s).
[0m19:34:10.394662 [debug] [MainThread]: Command end result
[0m19:34:10.425587 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:34:10.427000 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:34:10.431669 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:34:10.431914 [info ] [MainThread]: 
[0m19:34:10.432146 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m19:34:10.432299 [info ] [MainThread]: 
[0m19:34:10.432489 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m19:34:10.432654 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `flight_db`.`raw`.`airlines` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 10 pos 5
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:34:10.432788 [info ] [MainThread]: 
[0m19:34:10.432920 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:34:10.433042 [info ] [MainThread]: 
[0m19:34:10.433173 [error] [MainThread]: [31mFailure in model silver_flights (models/silver/silver_flights.sql)[0m
[0m19:34:10.433315 [error] [MainThread]:   Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:34:10.433421 [info ] [MainThread]: 
[0m19:34:10.433547 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_flights.sql
[0m19:34:10.433653 [info ] [MainThread]: 
[0m19:34:10.433782 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=10
[0m19:34:10.436441 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.4364195, "process_in_blocks": "0", "process_kernel_time": 0.264217, "process_mem_max_rss": "239026176", "process_out_blocks": "0", "process_user_time": 2.270354}
[0m19:34:10.436656 [debug] [MainThread]: Command `dbt run` failed at 19:34:10.436612 after 9.44 seconds
[0m19:34:10.436843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10443b230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11652e3f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1167cd850>]}
[0m19:34:10.436998 [debug] [MainThread]: Flushing usage events
[0m19:34:11.240308 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:35:26.881937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1037d3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047eb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047ebb10>]}


============================== 19:35:26.884232 | 77fffbbc-4221-4da1-9ff4-b68adf8a31a4 ==============================
[0m19:35:26.884232 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:35:26.884480 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run', 'write_json': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'cache_selected_only': 'False', 'partial_parse': 'True', 'target_path': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'log_cache_events': 'False', 'static_parser': 'True', 'use_colors': 'True', 'version_check': 'True', 'debug': 'False', 'introspect': 'True', 'no_print': 'None', 'log_format': 'default', 'printer_width': '80'}
[0m19:35:27.185751 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:35:27.185943 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:35:27.186047 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:35:27.621364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b17360>]}
[0m19:35:27.640825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059f2ad0>]}
[0m19:35:27.641063 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:35:27.692390 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:35:27.752089 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:35:27.752335 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m19:35:27.840793 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:35:27.845622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba0ea50>]}
[0m19:35:27.878862 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:35:27.880256 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:35:27.885731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b50b890>]}
[0m19:35:27.885888 [info ] [MainThread]: Found 10 models, 4 data tests, 4 sources, 686 macros
[0m19:35:27.885992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be19fd0>]}
[0m19:35:27.886753 [info ] [MainThread]: 
[0m19:35:27.886865 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:35:27.886947 [info ] [MainThread]: 
[0m19:35:27.887125 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:35:27.887218 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:35:27.889792 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m19:35:27.889917 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m19:35:27.893501 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m19:35:27.893620 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m19:35:27.893709 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:35:28.658468 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-8c68-11cf-bc43-82edd62d9461) - Created
[0m19:35:29.083567 [debug] [ThreadPool]: SQL status: OK in 1.190 seconds
[0m19:35:29.090916 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b5-8c68-11cf-bc43-82edd62d9461, command-id=01f089b5-8c8e-1320-addf-04e4c925da82) - Closing
[0m19:35:29.091380 [debug] [ThreadPool]: On list_flight_db: Close
[0m19:35:29.091591 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-8c68-11cf-bc43-82edd62d9461) - Closing
[0m19:35:29.446079 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:35:29.446696 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:35:29.457099 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:35:29.457426 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:35:29.457648 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:35:30.099133 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-8d43-11e2-a507-a6e34d298893) - Created
[0m19:35:30.618251 [debug] [ThreadPool]: SQL status: OK in 1.160 seconds
[0m19:35:30.622769 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b5-8d43-11e2-a507-a6e34d298893, command-id=01f089b5-8d5f-162a-97c9-170fb08ac400) - Closing
[0m19:35:30.623855 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:35:30.624163 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-8d43-11e2-a507-a6e34d298893) - Closing
[0m19:35:30.829077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd87450>]}
[0m19:35:30.833939 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m19:35:30.834448 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m19:35:30.834809 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m19:35:30.835128 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m19:35:30.835727 [info ] [Thread-2 (]: 2 of 10 START sql view model raw.bronze_airports ............................... [RUN]
[0m19:35:30.836265 [info ] [Thread-4 (]: 4 of 10 START sql table model raw.my_first_dbt_model ........................... [RUN]
[0m19:35:30.836742 [info ] [Thread-3 (]: 3 of 10 START sql view model raw.bronze_flights ................................ [RUN]
[0m19:35:30.837171 [info ] [Thread-1 (]: 1 of 10 START sql view model raw.bronze_airlines ............................... [RUN]
[0m19:35:30.837828 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m19:35:30.838316 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m19:35:30.838710 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m19:35:30.839086 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m19:35:30.839391 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m19:35:30.839693 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m19:35:30.839960 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m19:35:30.840220 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m19:35:30.840562 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m19:35:30.840896 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m19:35:30.841189 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m19:35:30.841473 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m19:35:30.850504 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m19:35:30.853431 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m19:35:30.855148 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m19:35:30.859111 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m19:35:30.860131 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m19:35:30.860381 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m19:35:30.860622 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m19:35:30.860820 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m19:35:30.870025 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:35:30.877230 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:35:30.879905 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m19:35:30.880743 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:35:30.881674 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:35:30.881919 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:35:30.882117 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:35:30.882332 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:35:30.882523 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdc6390>]}
[0m19:35:30.882658 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb0f1d0>]}
[0m19:35:30.882777 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb0f120>]}
[0m19:35:30.882900 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be21f90>]}
[0m19:35:30.889578 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m19:35:30.890504 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m19:35:30.903768 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m19:35:30.906543 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m19:35:30.906836 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m19:35:30.907097 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m19:35:30.907343 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m19:35:30.907758 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m19:35:30.907864 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m19:35:30.907968 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m19:35:30.908126 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`raw`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:35:30.908232 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m19:35:30.908354 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m19:35:30.908478 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    models:
  flights_dbt:
    bronze:
      materialized: view
    silver:
      materialized: view
    gold:
      materialized: table
    example:
      materialized: table
  )

[0m19:35:30.908586 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:35:30.908698 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m19:35:30.908797 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:35:30.908886 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:35:30.909059 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:35:31.672294 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-8e33-12e6-8a76-861fec8facd0) - Created
[0m19:35:31.690423 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-8e36-181b-b397-9d90b0063e9e) - Created
[0m19:35:31.723489 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-8e3a-17a9-9084-4c12ece97896) - Created
[0m19:35:31.873160 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-8e53-1a2c-b043-9bcc5355b0f6) - Created
[0m19:35:32.102371 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    models:
  flights_dbt:
    bronze:
      materialized: view
    silver:
      materialized: view
    gold:
      materialized: table
    example:
      materialized: table
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'models'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    models:
----^^^
  flights_dbt:
    bronze:
      materialized: view
    silver:
      materialized: view
    gold:
      materialized: table
    example:
      materialized: table
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'models'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    models:
----^^^
  flights_dbt:
    bronze:
      materialized: view
    silver:
      materialized: view
    gold:
      materialized: table
    example:
      materialized: table
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'models'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    models:
----^^^
  flights_dbt:
    bronze:
      materialized: view
    silver:
      materialized: view
    gold:
      materialized: table
    example:
      materialized: table
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b5-8e52-1c5c-b582-b090140f98ab
[0m19:35:32.105002 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m19:35:32.105697 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-8e36-181b-b397-9d90b0063e9e) - Closing
[0m19:35:32.314031 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'models'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      models:
  ----^^^
    flights_dbt:
      bronze:
        materialized: view
      silver:
        materialized: view
      gold:
        materialized: table
      example:
        materialized: table
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:35:32.316730 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c19c2f0>]}
[0m19:35:32.317619 [error] [Thread-1 (]: 1 of 10 ERROR creating sql view model raw.bronze_airlines ...................... [[31mERROR[0m in 1.48s]
[0m19:35:32.318251 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m19:35:32.318618 [debug] [Thread-1 (]: Began running node model.flights_dbt.stg_airlines
[0m19:35:32.319139 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'models'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      models:
  ----^^^
    flights_dbt:
      bronze:
        materialized: view
      silver:
        materialized: view
      gold:
        materialized: table
      example:
        materialized: table
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m19:35:32.319596 [info ] [Thread-1 (]: 5 of 10 START sql table model raw.stg_airlines ................................. [RUN]
[0m19:35:32.321150 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.stg_airlines) - Creating connection
[0m19:35:32.321468 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.stg_airlines'
[0m19:35:32.321720 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.stg_airlines
[0m19:35:32.325194 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.stg_airlines"
[0m19:35:32.325809 [debug] [Thread-1 (]: Began executing node model.flights_dbt.stg_airlines
[0m19:35:32.327177 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m19:35:32.328712 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.stg_airlines"
[0m19:35:32.329120 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.stg_airlines"
[0m19:35:32.329353 [debug] [Thread-1 (]: On model.flights_dbt.stg_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.stg_airlines"} */

  
    
        create or replace table `flight_db`.`raw`.`stg_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
  1            as airline_id,
  'Test Air'   as airline_name
  
[0m19:35:32.329560 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:35:32.528011 [debug] [Thread-2 (]: SQL status: OK in 1.620 seconds
[0m19:35:32.529534 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b5-8e3a-17a9-9084-4c12ece97896, command-id=01f089b5-8e56-1edd-96f5-6a4fc0b52f40) - Closing
[0m19:35:32.539200 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:35:32.541806 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m19:35:32.542050 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-8e3a-17a9-9084-4c12ece97896) - Closing
[0m19:35:32.572676 [debug] [Thread-3 (]: SQL status: OK in 1.660 seconds
[0m19:35:32.573422 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b5-8e53-1a2c-b043-9bcc5355b0f6, command-id=01f089b5-8e6e-142f-887b-2488ca6764e4) - Closing
[0m19:35:32.573941 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:35:32.737176 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m19:35:32.738171 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-8e53-1a2c-b043-9bcc5355b0f6) - Closing
[0m19:35:32.935049 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd7e870>]}
[0m19:35:32.936041 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd7fb90>]}
[0m19:35:32.937208 [info ] [Thread-2 (]: 2 of 10 OK created sql view model raw.bronze_airports .......................... [[32mOK[0m in 2.10s]
[0m19:35:32.937798 [info ] [Thread-3 (]: 3 of 10 OK created sql view model raw.bronze_flights ........................... [[32mOK[0m in 2.10s]
[0m19:35:32.938611 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m19:35:32.939153 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m19:35:32.939574 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airlines
[0m19:35:32.940566 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airports
[0m19:35:32.940280 [info ] [Thread-2 (]: 6 of 10 SKIP relation raw.silver_airlines ...................................... [[33mSKIP[0m]
[0m19:35:32.941067 [info ] [Thread-3 (]: 7 of 10 START sql view model raw.silver_airports ............................... [RUN]
[0m19:35:32.941442 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airlines
[0m19:35:32.942025 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m19:35:32.942398 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_flights
[0m19:35:32.942751 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m19:35:32.943163 [info ] [Thread-2 (]: 8 of 10 START sql view model raw.silver_flights ................................ [RUN]
[0m19:35:32.943505 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_airports
[0m19:35:32.943913 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m19:35:32.947766 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m19:35:32.948098 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m19:35:32.948438 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.silver_flights
[0m19:35:32.951714 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m19:35:32.952265 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_airports
[0m19:35:32.952700 [debug] [Thread-2 (]: Began executing node model.flights_dbt.silver_flights
[0m19:35:32.954741 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:35:32.956501 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:35:32.957683 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`silver_airports`
[0m19:35:32.958425 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`silver_flights`
[0m19:35:32.959039 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m19:35:32.959570 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m19:35:32.960068 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m19:35:32.960278 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m19:35:32.960534 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`bronze_airports`
  )

[0m19:35:32.960782 [debug] [Thread-2 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    select
    f.flight_id,
    f.airline,
    f.origin,
    f.dep_time,
    f.arr_time,
    f.arr_delay
from `flight_db`.`raw`.`bronze_flights` as f
  )

[0m19:35:32.960993 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:35:32.961178 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:35:32.974905 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-8efb-16e6-acb4-9bdcaf4d5181) - Created
[0m19:35:33.651902 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-8f62-1914-8e62-221788dfd129) - Created
[0m19:35:33.661210 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-8f64-17d1-b2db-4c4dd67eb182) - Created
[0m19:35:34.091599 [debug] [Thread-4 (]: SQL status: OK in 3.180 seconds
[0m19:35:34.093371 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b5-8e33-12e6-8a76-861fec8facd0, command-id=01f089b5-8e51-127f-9f4d-668c41b8bd69) - Closing
[0m19:35:34.098315 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:35:34.114443 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m19:35:34.114790 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-8e33-12e6-8a76-861fec8facd0) - Closing
[0m19:35:34.176907 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    select
    f.flight_id,
    f.airline,
    f.origin,
    f.dep_time,
    f.arr_time,
    f.arr_delay
from `flight_db`.`raw`.`bronze_flights` as f
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more
, operation-id=01f089b5-8f7d-1d5b-9cb6-705e1db445d0
[0m19:35:34.305628 [debug] [Thread-2 (]: On model.flights_dbt.silver_flights: Close
[0m19:35:34.306465 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-8f62-1914-8e62-221788dfd129) - Closing
[0m19:35:34.443296 [debug] [Thread-3 (]: SQL status: OK in 1.480 seconds
[0m19:35:34.444951 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b5-8f64-17d1-b2db-4c4dd67eb182, command-id=01f089b5-8f7f-1ddd-a1cd-4453bd520707) - Closing
[0m19:35:34.445909 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:35:34.517361 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd7ee70>]}
[0m19:35:34.518170 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: Close
[0m19:35:34.519175 [info ] [Thread-4 (]: 4 of 10 OK created sql table model raw.my_first_dbt_model ...................... [[32mOK[0m in 3.68s]
[0m19:35:34.519627 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-8f64-17d1-b2db-4c4dd67eb182) - Closing
[0m19:35:34.520248 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m19:35:34.526046 [debug] [Thread-2 (]: Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:35:34.526615 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m19:35:34.527326 [info ] [Thread-4 (]: 9 of 10 START sql view model raw.my_second_dbt_model ........................... [RUN]
[0m19:35:34.720482 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c163650>]}
[0m19:35:34.721336 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m19:35:34.721918 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c455010>]}
[0m19:35:34.723211 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m19:35:34.722807 [error] [Thread-2 (]: 8 of 10 ERROR creating sql view model raw.silver_flights ....................... [[31mERROR[0m in 1.78s]
[0m19:35:34.724437 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m19:35:34.724073 [info ] [Thread-3 (]: 7 of 10 OK created sql view model raw.silver_airports .......................... [[32mOK[0m in 1.78s]
[0m19:35:34.725106 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_flights
[0m19:35:34.727983 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airports
[0m19:35:34.734105 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_flights' to be skipped because of status 'error'.  Reason: Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql.
[0m19:35:34.735478 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m19:35:34.736389 [debug] [Thread-2 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m19:35:34.736759 [info ] [Thread-2 (]: 10 of 10 SKIP relation raw.gold_flight_metrics ................................. [[33mSKIP[0m]
[0m19:35:34.737242 [debug] [Thread-2 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m19:35:34.737711 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m19:35:34.740204 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m19:35:34.741238 [debug] [Thread-4 (]: Creating view `flight_db`.`raw`.`my_second_dbt_model`
[0m19:35:34.741850 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m19:35:34.742661 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m19:35:34.742952 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`raw`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`raw`.`my_first_dbt_model`
where id = 1
  )

[0m19:35:34.743188 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:35:34.975693 [debug] [Thread-1 (]: SQL status: OK in 2.650 seconds
[0m19:35:34.977258 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b5-8efb-16e6-acb4-9bdcaf4d5181, command-id=01f089b5-8f15-1796-8025-4dd149f11883) - Closing
[0m19:35:34.978367 [debug] [Thread-1 (]: Applying tags to relation None
[0m19:35:34.979841 [debug] [Thread-1 (]: On model.flights_dbt.stg_airlines: Close
[0m19:35:34.980166 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-8efb-16e6-acb4-9bdcaf4d5181) - Closing
[0m19:35:35.173468 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd7c170>]}
[0m19:35:35.174881 [info ] [Thread-1 (]: 5 of 10 OK created sql table model raw.stg_airlines ............................ [[32mOK[0m in 2.85s]
[0m19:35:35.175627 [debug] [Thread-1 (]: Finished running node model.flights_dbt.stg_airlines
[0m19:35:35.366696 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-9068-1a19-9585-eb2c7af62bb9) - Created
[0m19:35:35.961662 [debug] [Thread-4 (]: SQL status: OK in 1.220 seconds
[0m19:35:35.964174 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b5-9068-1a19-9585-eb2c7af62bb9, command-id=01f089b5-9082-1e0b-b09b-86ad0176d4a3) - Closing
[0m19:35:35.965769 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:35:35.967476 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m19:35:35.968111 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-9068-1a19-9585-eb2c7af62bb9) - Closing
[0m19:35:36.151858 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '77fffbbc-4221-4da1-9ff4-b68adf8a31a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c188710>]}
[0m19:35:36.153672 [info ] [Thread-4 (]: 9 of 10 OK created sql view model raw.my_second_dbt_model ...................... [[32mOK[0m in 1.62s]
[0m19:35:36.154500 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m19:35:36.156346 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:35:36.156710 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:35:36.157293 [info ] [MainThread]: 
[0m19:35:36.157623 [info ] [MainThread]: Finished running 3 table models, 7 view models in 0 hours 0 minutes and 8.27 seconds (8.27s).
[0m19:35:36.159189 [debug] [MainThread]: Command end result
[0m19:35:36.196129 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:35:36.197098 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:35:36.200415 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:35:36.200546 [info ] [MainThread]: 
[0m19:35:36.200695 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m19:35:36.200808 [info ] [MainThread]: 
[0m19:35:36.200952 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m19:35:36.201100 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'models'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      models:
  ----^^^
    flights_dbt:
      bronze:
        materialized: view
      silver:
        materialized: view
      gold:
        materialized: table
      example:
        materialized: table
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:35:36.201217 [info ] [MainThread]: 
[0m19:35:36.201334 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:35:36.201432 [info ] [MainThread]: 
[0m19:35:36.201555 [error] [MainThread]: [31mFailure in model silver_flights (models/silver/silver_flights.sql)[0m
[0m19:35:36.201680 [error] [MainThread]:   Database Error in model silver_flights (models/silver/silver_flights.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `f`.`dep_time` cannot be resolved. Did you mean one of the following? [`f`.`arr_time`, `f`.`dest`, `f`.`airline`, `f`.`origin`, `f`.`flight_id`]. SQLSTATE: 42703; line 12 pos 4
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:35:36.201776 [info ] [MainThread]: 
[0m19:35:36.201891 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_flights.sql
[0m19:35:36.201984 [info ] [MainThread]: 
[0m19:35:36.202102 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=10
[0m19:35:36.204538 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.353447, "process_in_blocks": "0", "process_kernel_time": 0.270596, "process_mem_max_rss": "248152064", "process_out_blocks": "0", "process_user_time": 2.431752}
[0m19:35:36.204730 [debug] [MainThread]: Command `dbt run` failed at 19:35:36.204693 after 9.35 seconds
[0m19:35:36.204894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104677050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c18bcb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6eb050>]}
[0m19:35:36.205024 [debug] [MainThread]: Flushing usage events
[0m19:35:37.016445 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:37:20.155608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086a3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10987b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10987bb10>]}


============================== 19:37:20.157909 | 0c2319f1-e671-45ad-ae2f-adc2b5d9b55e ==============================
[0m19:37:20.157909 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:37:20.158128 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'write_json': 'True', 'fail_fast': 'False', 'profiles_dir': '/Users/artakerqeli/.dbt', 'log_cache_events': 'False', 'target_path': 'None', 'introspect': 'True', 'printer_width': '80', 'invocation_command': 'dbt run', 'warn_error': 'None', 'static_parser': 'True', 'no_print': 'None', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'empty': 'False', 'debug': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'version_check': 'True'}
[0m19:37:20.463543 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:37:20.463814 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:37:20.463911 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:37:20.923716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ba7360>]}
[0m19:37:20.943223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c206ad0>]}
[0m19:37:20.943458 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:37:20.992739 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:37:21.050712 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 2 files changed.
[0m19:37:21.050951 [debug] [MainThread]: Partial parsing: added file: flights_dbt://models/sources.yml
[0m19:37:21.051093 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/silver/silver_flights.sql
[0m19:37:21.051215 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/bronze/bronze_airlines.sql
[0m19:37:21.174316 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:37:21.177848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c10b650>]}
[0m19:37:21.211370 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:37:21.212334 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:37:21.217498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c665f40>]}
[0m19:37:21.217678 [info ] [MainThread]: Found 10 models, 4 data tests, 7 sources, 686 macros
[0m19:37:21.217784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b77dd30>]}
[0m19:37:21.218558 [info ] [MainThread]: 
[0m19:37:21.218674 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:37:21.218756 [info ] [MainThread]: 
[0m19:37:21.218938 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:37:21.219029 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:37:21.221620 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m19:37:21.221736 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m19:37:21.225219 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m19:37:21.225326 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m19:37:21.225417 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:37:22.001697 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-cff3-184a-87d6-235d292dc148) - Created
[0m19:37:22.545983 [debug] [ThreadPool]: SQL status: OK in 1.320 seconds
[0m19:37:22.555817 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b5-cff3-184a-87d6-235d292dc148, command-id=01f089b5-d016-155a-b837-5880542bec61) - Closing
[0m19:37:22.556326 [debug] [ThreadPool]: On list_flight_db: Close
[0m19:37:22.556542 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-cff3-184a-87d6-235d292dc148) - Closing
[0m19:37:22.793418 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:37:22.793985 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:37:22.807136 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:37:22.807697 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:37:22.807977 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:37:23.467332 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-d0d7-1331-8eff-0b2ef0f5a925) - Created
[0m19:37:23.982341 [debug] [ThreadPool]: SQL status: OK in 1.170 seconds
[0m19:37:23.985198 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b5-d0d7-1331-8eff-0b2ef0f5a925, command-id=01f089b5-d0f1-1c0f-b85d-9c6c77cbcedf) - Closing
[0m19:37:23.985917 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:37:23.986112 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-d0d7-1331-8eff-0b2ef0f5a925) - Closing
[0m19:37:24.185569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c67a000>]}
[0m19:37:24.189966 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m19:37:24.190441 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m19:37:24.190786 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m19:37:24.191106 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m19:37:24.191724 [info ] [Thread-2 (]: 2 of 10 START sql view model raw.bronze_airports ............................... [RUN]
[0m19:37:24.192203 [info ] [Thread-1 (]: 1 of 10 START sql view model raw.bronze_airlines ............................... [RUN]
[0m19:37:24.192646 [info ] [Thread-4 (]: 4 of 10 START sql table model raw.my_first_dbt_model ........................... [RUN]
[0m19:37:24.193071 [info ] [Thread-3 (]: 3 of 10 START sql view model raw.bronze_flights ................................ [RUN]
[0m19:37:24.193683 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m19:37:24.194102 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m19:37:24.194494 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m19:37:24.194851 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m19:37:24.195129 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m19:37:24.195385 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m19:37:24.195634 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m19:37:24.195867 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m19:37:24.196127 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m19:37:24.196374 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m19:37:24.196599 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m19:37:24.196798 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m19:37:24.205080 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m19:37:24.207833 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m19:37:24.210469 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m19:37:24.212220 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m19:37:24.213231 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m19:37:24.213486 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m19:37:24.213673 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m19:37:24.219404 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m19:37:24.223379 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:37:24.224416 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:37:24.225539 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:37:24.234050 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m19:37:24.234966 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:37:24.235220 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:37:24.235435 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:37:24.235631 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:37:24.235817 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c18ea50>]}
[0m19:37:24.235946 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c68c5d0>]}
[0m19:37:24.236067 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c68c520>]}
[0m19:37:24.236180 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e404d70>]}
[0m19:37:24.242251 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m19:37:24.242622 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m19:37:24.242938 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m19:37:24.258888 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m19:37:24.259743 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m19:37:24.260041 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m19:37:24.260307 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m19:37:24.260801 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m19:37:24.260920 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m19:37:24.261058 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`raw`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:37:24.261169 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m19:37:24.261268 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m19:37:24.261382 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m19:37:24.261503 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:37:24.261644 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m19:37:24.261773 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

[0m19:37:24.261879 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:37:24.262052 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:37:24.262150 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:37:24.946586 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-d1b8-1df7-9389-63aafed8a474) - Created
[0m19:37:24.950148 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-d1b9-1c77-aad6-61c410352523) - Created
[0m19:37:25.082435 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-d1c8-1d77-b01e-386c62956396) - Created
[0m19:37:25.125073 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-d1cf-1e75-bc8f-0c8f51e9ca77) - Created
[0m19:37:25.322467 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b5-d1d3-107e-9d58-eeea789f2b92
[0m19:37:25.324888 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m19:37:25.325241 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-d1b8-1df7-9389-63aafed8a474) - Closing
[0m19:37:25.528751 [debug] [Thread-2 (]: SQL status: OK in 1.270 seconds
[0m19:37:25.530333 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b5-d1b9-1c77-aad6-61c410352523, command-id=01f089b5-d1d3-1cf2-b96d-5675d06117ec) - Closing
[0m19:37:25.544297 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:37:25.546929 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m19:37:25.547251 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-d1b9-1c77-aad6-61c410352523) - Closing
[0m19:37:25.555379 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:37:25.731584 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c545e80>]}
[0m19:37:25.732173 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4934d0>]}
[0m19:37:25.733041 [error] [Thread-1 (]: 1 of 10 ERROR creating sql view model raw.bronze_airlines ...................... [[31mERROR[0m in 1.53s]
[0m19:37:25.734277 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m19:37:25.733749 [info ] [Thread-2 (]: 2 of 10 OK created sql view model raw.bronze_airports .......................... [[32mOK[0m in 1.54s]
[0m19:37:25.734742 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_flights
[0m19:37:25.735222 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m19:37:25.735669 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m19:37:25.736088 [info ] [Thread-1 (]: 5 of 10 START sql view model raw.silver_flights ................................ [RUN]
[0m19:37:25.737505 [debug] [Thread-2 (]: Began running node model.flights_dbt.stg_airlines
[0m19:37:25.738264 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m19:37:25.738787 [info ] [Thread-2 (]: 6 of 10 START sql table model raw.stg_airlines ................................. [RUN]
[0m19:37:25.739154 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m19:37:25.739620 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.stg_airlines) - Creating connection
[0m19:37:25.739945 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_flights
[0m19:37:25.740220 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.stg_airlines'
[0m19:37:25.744212 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.stg_airlines
[0m19:37:25.747050 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m19:37:25.749467 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.stg_airlines"
[0m19:37:25.750046 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_flights
[0m19:37:25.751600 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:37:25.752761 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`silver_flights`
[0m19:37:25.753003 [debug] [Thread-2 (]: Began executing node model.flights_dbt.stg_airlines
[0m19:37:25.753601 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m19:37:25.755020 [debug] [Thread-2 (]: MATERIALIZING TABLE
[0m19:37:25.756193 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.stg_airlines"
[0m19:37:25.757577 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m19:37:25.757909 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

[0m19:37:25.758218 [debug] [Thread-3 (]: SQL status: OK in 1.500 seconds
[0m19:37:25.758421 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:37:25.759152 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b5-d1c8-1d77-b01e-386c62956396, command-id=01f089b5-d1e8-1083-940c-6b3851e8a992) - Closing
[0m19:37:25.759474 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.stg_airlines"
[0m19:37:25.759935 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:37:25.760222 [debug] [Thread-2 (]: On model.flights_dbt.stg_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.stg_airlines"} */

  
    
        create or replace table `flight_db`.`raw`.`stg_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
  1            as airline_id,
  'Test Air'   as airline_name
  
[0m19:37:25.760698 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m19:37:25.760877 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:37:25.761077 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-d1c8-1d77-b01e-386c62956396) - Closing
[0m19:37:25.967372 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c5e4fb0>]}
[0m19:37:25.967953 [info ] [Thread-3 (]: 3 of 10 OK created sql view model raw.bronze_flights ........................... [[32mOK[0m in 1.77s]
[0m19:37:25.968288 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m19:37:25.968511 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airlines
[0m19:37:25.968775 [info ] [Thread-3 (]: 7 of 10 SKIP relation raw.silver_airlines ...................................... [[33mSKIP[0m]
[0m19:37:25.969014 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airlines
[0m19:37:25.969210 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airports
[0m19:37:25.969502 [info ] [Thread-3 (]: 8 of 10 START sql view model raw.silver_airports ............................... [RUN]
[0m19:37:25.969845 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m19:37:25.970041 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m19:37:25.970219 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_airports
[0m19:37:25.977946 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m19:37:25.978565 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_airports
[0m19:37:25.979649 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:37:25.980203 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`silver_airports`
[0m19:37:25.980571 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m19:37:25.980857 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m19:37:25.981019 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`bronze_airports`
  )

[0m19:37:25.981171 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:37:26.448821 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-d29b-16ca-b17f-340590c4f87d) - Created
[0m19:37:26.476383 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-d2a2-1b72-98d1-33e26872eb1b) - Created
[0m19:37:26.645153 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-d2bb-1926-ac66-225608f78973) - Created
[0m19:37:26.895129 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b5-d2bd-1c7b-863f-ada88a4a7c67
[0m19:37:26.896799 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: Close
[0m19:37:26.897187 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-d2a2-1b72-98d1-33e26872eb1b) - Closing
[0m19:37:27.126762 [debug] [Thread-1 (]: Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:37:27.128191 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c372570>]}
[0m19:37:27.129684 [error] [Thread-1 (]: 5 of 10 ERROR creating sql view model raw.silver_flights ....................... [[31mERROR[0m in 1.39s]
[0m19:37:27.130510 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_flights
[0m19:37:27.131392 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_flights' to be skipped because of status 'error'.  Reason: Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql.
[0m19:37:27.247672 [debug] [Thread-4 (]: SQL status: OK in 2.990 seconds
[0m19:37:27.250144 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b5-d1cf-1e75-bc8f-0c8f51e9ca77, command-id=01f089b5-d1ef-1bf3-90d6-decf11d27da7) - Closing
[0m19:37:27.253767 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:37:27.266067 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m19:37:27.266303 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-d1cf-1e75-bc8f-0c8f51e9ca77) - Closing
[0m19:37:27.380646 [debug] [Thread-3 (]: SQL status: OK in 1.400 seconds
[0m19:37:27.382229 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b5-d2bb-1926-ac66-225608f78973, command-id=01f089b5-d2d7-11cd-9609-c287c5c48aae) - Closing
[0m19:37:27.383280 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:37:27.467316 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: Close
[0m19:37:27.468426 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-d2bb-1926-ac66-225608f78973) - Closing
[0m19:37:27.680086 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c3ce990>]}
[0m19:37:27.681053 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e49b830>]}
[0m19:37:27.682089 [info ] [Thread-4 (]: 4 of 10 OK created sql table model raw.my_first_dbt_model ...................... [[32mOK[0m in 3.49s]
[0m19:37:27.682697 [info ] [Thread-3 (]: 8 of 10 OK created sql view model raw.silver_airports .......................... [[32mOK[0m in 1.71s]
[0m19:37:27.683376 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m19:37:27.683825 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airports
[0m19:37:27.684542 [debug] [Thread-1 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m19:37:27.684941 [debug] [Thread-4 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m19:37:27.685472 [info ] [Thread-1 (]: 9 of 10 START sql view model raw.my_second_dbt_model ........................... [RUN]
[0m19:37:27.686038 [info ] [Thread-4 (]: 10 of 10 SKIP relation raw.gold_flight_metrics ................................. [[33mSKIP[0m]
[0m19:37:27.686789 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m19:37:27.687259 [debug] [Thread-4 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m19:37:27.687703 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m19:37:27.688263 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m19:37:27.692770 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m19:37:27.693932 [debug] [Thread-1 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m19:37:27.696793 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:37:27.698058 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`my_second_dbt_model`
[0m19:37:27.698775 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m19:37:27.699626 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m19:37:27.699963 [debug] [Thread-1 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`raw`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`raw`.`my_first_dbt_model`
where id = 1
  )

[0m19:37:27.700249 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:37:28.357787 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-d3c1-1458-b93e-2ed717c7031b) - Created
[0m19:37:28.371250 [debug] [Thread-2 (]: SQL status: OK in 2.610 seconds
[0m19:37:28.372597 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b5-d29b-16ca-b17f-340590c4f87d, command-id=01f089b5-d2b8-1491-b789-b11508735d4a) - Closing
[0m19:37:28.373645 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:37:28.375079 [debug] [Thread-2 (]: On model.flights_dbt.stg_airlines: Close
[0m19:37:28.375354 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-d29b-16ca-b17f-340590c4f87d) - Closing
[0m19:37:28.579840 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1366f0>]}
[0m19:37:28.581188 [info ] [Thread-2 (]: 6 of 10 OK created sql table model raw.stg_airlines ............................ [[32mOK[0m in 2.84s]
[0m19:37:28.581804 [debug] [Thread-2 (]: Finished running node model.flights_dbt.stg_airlines
[0m19:37:29.098704 [debug] [Thread-1 (]: SQL status: OK in 1.400 seconds
[0m19:37:29.101118 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b5-d3c1-1458-b93e-2ed717c7031b, command-id=01f089b5-d3db-1a5e-b883-d20f3126ef51) - Closing
[0m19:37:29.102010 [debug] [Thread-1 (]: Applying tags to relation None
[0m19:37:29.102944 [debug] [Thread-1 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m19:37:29.103214 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-d3c1-1458-b93e-2ed717c7031b) - Closing
[0m19:37:29.346379 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2319f1-e671-45ad-ae2f-adc2b5d9b55e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c3cdc10>]}
[0m19:37:29.348182 [info ] [Thread-1 (]: 9 of 10 OK created sql view model raw.my_second_dbt_model ...................... [[32mOK[0m in 1.66s]
[0m19:37:29.349027 [debug] [Thread-1 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m19:37:29.350924 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:37:29.351301 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:37:29.351867 [info ] [MainThread]: 
[0m19:37:29.352186 [info ] [MainThread]: Finished running 3 table models, 7 view models in 0 hours 0 minutes and 8.13 seconds (8.13s).
[0m19:37:29.353858 [debug] [MainThread]: Command end result
[0m19:37:29.384909 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:37:29.386259 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:37:29.390143 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:37:29.390290 [info ] [MainThread]: 
[0m19:37:29.390473 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m19:37:29.390597 [info ] [MainThread]: 
[0m19:37:29.390760 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m19:37:29.390925 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:37:29.391059 [info ] [MainThread]: 
[0m19:37:29.391196 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:37:29.391310 [info ] [MainThread]: 
[0m19:37:29.391450 [error] [MainThread]: [31mFailure in model silver_flights (models/silver/silver_flights.sql)[0m
[0m19:37:29.391601 [error] [MainThread]:   Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:37:29.391724 [info ] [MainThread]: 
[0m19:37:29.391858 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_flights.sql
[0m19:37:29.391972 [info ] [MainThread]: 
[0m19:37:29.392104 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=10
[0m19:37:29.395145 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.27195, "process_in_blocks": "0", "process_kernel_time": 0.265295, "process_mem_max_rss": "250462208", "process_out_blocks": "0", "process_user_time": 2.194264}
[0m19:37:29.395352 [debug] [MainThread]: Command `dbt run` failed at 19:37:29.395310 after 9.27 seconds
[0m19:37:29.395524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108633b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c119fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1366f0>]}
[0m19:37:29.395672 [debug] [MainThread]: Flushing usage events
[0m19:37:30.125098 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:38:16.489231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e37620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076fbb10>]}


============================== 19:38:16.491591 | 69c7d61f-2b2c-4dfe-a571-46bbf465a89d ==============================
[0m19:38:16.491591 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:38:16.491821 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'profiles_dir': '/Users/artakerqeli/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'log_format': 'default', 'write_json': 'True', 'fail_fast': 'False', 'invocation_command': 'dbt run', 'empty': 'False', 'partial_parse': 'True', 'warn_error': 'None', 'no_print': 'None', 'cache_selected_only': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'quiet': 'False', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'debug': 'False', 'version_check': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False'}
[0m19:38:16.794656 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:38:16.794884 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:38:16.794994 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:38:17.266198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106623360>]}
[0m19:38:17.286166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111702ad0>]}
[0m19:38:17.286418 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:38:17.341559 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:38:17.398619 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:38:17.398780 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:38:17.401794 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:38:17.416597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c0ae50>]}
[0m19:38:17.451719 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:38:17.453755 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:38:17.459172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12160b890>]}
[0m19:38:17.459332 [info ] [MainThread]: Found 10 models, 4 data tests, 7 sources, 686 macros
[0m19:38:17.459437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121df3e70>]}
[0m19:38:17.460255 [info ] [MainThread]: 
[0m19:38:17.460380 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:38:17.460465 [info ] [MainThread]: 
[0m19:38:17.460650 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:38:17.460741 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:38:17.463303 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m19:38:17.463428 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m19:38:17.467949 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m19:38:17.468105 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m19:38:17.468197 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:38:18.188176 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-f174-1a16-9626-f9885f5d3776) - Created
[0m19:38:18.550923 [debug] [ThreadPool]: SQL status: OK in 1.080 seconds
[0m19:38:18.558261 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b5-f174-1a16-9626-f9885f5d3776, command-id=01f089b5-f18f-11ac-8819-dd12ace05236) - Closing
[0m19:38:18.558731 [debug] [ThreadPool]: On list_flight_db: Close
[0m19:38:18.558938 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-f174-1a16-9626-f9885f5d3776) - Closing
[0m19:38:18.756057 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:38:18.756621 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:38:18.767977 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:38:18.768637 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:38:18.768911 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:38:19.483915 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-f23a-108b-b679-76fcee0bb397) - Created
[0m19:38:19.944600 [debug] [ThreadPool]: SQL status: OK in 1.180 seconds
[0m19:38:19.949175 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b5-f23a-108b-b679-76fcee0bb397, command-id=01f089b5-f256-1be0-ae71-f01de6595196) - Closing
[0m19:38:19.950254 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:38:19.950560 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b5-f23a-108b-b679-76fcee0bb397) - Closing
[0m19:38:20.150582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121ecbc70>]}
[0m19:38:20.154489 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m19:38:20.155076 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m19:38:20.155369 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m19:38:20.155637 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m19:38:20.156183 [info ] [Thread-1 (]: 1 of 10 START sql view model raw.bronze_airlines ............................... [RUN]
[0m19:38:20.156658 [info ] [Thread-3 (]: 3 of 10 START sql view model raw.bronze_flights ................................ [RUN]
[0m19:38:20.157124 [info ] [Thread-4 (]: 4 of 10 START sql table model raw.my_first_dbt_model ........................... [RUN]
[0m19:38:20.157565 [info ] [Thread-2 (]: 2 of 10 START sql view model raw.bronze_airports ............................... [RUN]
[0m19:38:20.158242 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m19:38:20.158661 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m19:38:20.159063 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m19:38:20.159441 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m19:38:20.159718 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m19:38:20.159957 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m19:38:20.160186 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m19:38:20.160416 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m19:38:20.160666 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m19:38:20.160902 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m19:38:20.161127 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m19:38:20.161349 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m19:38:20.168199 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m19:38:20.169726 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m19:38:20.171003 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m19:38:20.172396 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m19:38:20.172987 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m19:38:20.173145 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m19:38:20.179150 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m19:38:20.179379 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m19:38:20.183514 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:38:20.193416 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m19:38:20.194392 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:38:20.195439 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:38:20.196467 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:38:20.196727 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:38:20.196969 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:38:20.197243 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:38:20.197496 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121ed5910>]}
[0m19:38:20.197651 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121d9a8e0>]}
[0m19:38:20.197838 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121d9aa40>]}
[0m19:38:20.198022 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121fbb110>]}
[0m19:38:20.204861 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m19:38:20.221573 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m19:38:20.221959 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m19:38:20.222263 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m19:38:20.222558 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m19:38:20.222899 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m19:38:20.223173 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m19:38:20.223470 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m19:38:20.223636 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`raw`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:38:20.223773 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:38:20.224003 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m19:38:20.224104 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m19:38:20.224251 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m19:38:20.224349 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m19:38:20.224512 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

[0m19:38:20.224625 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:38:20.224737 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m19:38:20.224842 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:38:20.224999 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:38:20.977041 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-f31e-1ff7-8a70-7a2bbc4abef0) - Created
[0m19:38:21.034178 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-f327-11dc-9db8-c51734eba33e) - Created
[0m19:38:21.037748 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-f327-10a2-90f1-a34f3f76bc29) - Created
[0m19:38:21.039802 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-f328-1603-8690-28b262751714) - Created
[0m19:38:21.500010 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b5-f342-1b85-8fec-c3ad8a914e2f
[0m19:38:21.502587 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m19:38:21.503296 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-f327-10a2-90f1-a34f3f76bc29) - Closing
[0m19:38:21.648770 [debug] [Thread-2 (]: SQL status: OK in 1.420 seconds
[0m19:38:21.651271 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b5-f327-11dc-9db8-c51734eba33e, command-id=01f089b5-f342-1972-a60e-21b0ba40bacc) - Closing
[0m19:38:21.661262 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:38:21.664889 [debug] [Thread-3 (]: SQL status: OK in 1.440 seconds
[0m19:38:21.665543 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b5-f328-1603-8690-28b262751714, command-id=01f089b5-f345-10cf-b57f-b1a8829b0f18) - Closing
[0m19:38:21.665944 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:38:21.713238 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m19:38:21.713536 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-f327-11dc-9db8-c51734eba33e) - Closing
[0m19:38:21.723007 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:38:21.936929 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m19:38:21.937956 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-f328-1603-8690-28b262751714) - Closing
[0m19:38:22.137785 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121f2c130>]}
[0m19:38:22.138350 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12217d080>]}
[0m19:38:22.138663 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122116150>]}
[0m19:38:22.139463 [error] [Thread-1 (]: 1 of 10 ERROR creating sql view model raw.bronze_airlines ...................... [[31mERROR[0m in 1.97s]
[0m19:38:22.140076 [info ] [Thread-2 (]: 2 of 10 OK created sql view model raw.bronze_airports .......................... [[32mOK[0m in 1.97s]
[0m19:38:22.140645 [info ] [Thread-3 (]: 3 of 10 OK created sql view model raw.bronze_flights ........................... [[32mOK[0m in 1.98s]
[0m19:38:22.141275 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m19:38:22.141737 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m19:38:22.142152 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m19:38:22.142554 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_flights
[0m19:38:22.143062 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m19:38:22.143427 [debug] [Thread-2 (]: Began running node model.flights_dbt.stg_airlines
[0m19:38:22.143879 [info ] [Thread-1 (]: 5 of 10 START sql view model raw.silver_flights ................................ [RUN]
[0m19:38:22.145604 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airlines
[0m19:38:22.145326 [info ] [Thread-2 (]: 6 of 10 START sql table model raw.stg_airlines ................................. [RUN]
[0m19:38:22.146247 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m19:38:22.146615 [info ] [Thread-3 (]: 7 of 10 SKIP relation raw.silver_airlines ...................................... [[33mSKIP[0m]
[0m19:38:22.147032 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.stg_airlines) - Creating connection
[0m19:38:22.147312 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m19:38:22.147604 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airlines
[0m19:38:22.147840 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.stg_airlines'
[0m19:38:22.148089 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_flights
[0m19:38:22.148333 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airports
[0m19:38:22.148565 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.stg_airlines
[0m19:38:22.151152 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m19:38:22.151655 [info ] [Thread-3 (]: 8 of 10 START sql view model raw.silver_airports ............................... [RUN]
[0m19:38:22.155015 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.stg_airlines"
[0m19:38:22.155571 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m19:38:22.155980 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m19:38:22.156312 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_flights
[0m19:38:22.156576 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_airports
[0m19:38:22.158507 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:38:22.158735 [debug] [Thread-2 (]: Began executing node model.flights_dbt.stg_airlines
[0m19:38:22.160670 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m19:38:22.161756 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`silver_flights`
[0m19:38:22.163021 [debug] [Thread-2 (]: MATERIALIZING TABLE
[0m19:38:22.163634 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m19:38:22.164733 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.stg_airlines"
[0m19:38:22.165001 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_airports
[0m19:38:22.168667 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m19:38:22.169524 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.stg_airlines"
[0m19:38:22.169748 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

[0m19:38:22.170532 [debug] [Thread-2 (]: On model.flights_dbt.stg_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.stg_airlines"} */

  
    
        create or replace table `flight_db`.`raw`.`stg_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
  1            as airline_id,
  'Test Air'   as airline_name
  
[0m19:38:22.173869 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:38:22.174061 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:38:22.174229 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:38:22.174815 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`silver_airports`
[0m19:38:22.175441 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m19:38:22.175844 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m19:38:22.176005 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`bronze_airports`
  )

[0m19:38:22.176189 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:38:22.655965 [debug] [Thread-4 (]: SQL status: OK in 2.430 seconds
[0m19:38:22.658337 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b5-f31e-1ff7-8a70-7a2bbc4abef0, command-id=01f089b5-f339-11fc-838f-a1d800d19358) - Closing
[0m19:38:22.662974 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:38:22.677261 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m19:38:22.677575 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-f31e-1ff7-8a70-7a2bbc4abef0) - Closing
[0m19:38:22.863621 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-f43f-18c3-b016-6f2677081acd) - Created
[0m19:38:22.865934 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12182a5d0>]}
[0m19:38:22.867667 [info ] [Thread-4 (]: 4 of 10 OK created sql table model raw.my_first_dbt_model ...................... [[32mOK[0m in 2.71s]
[0m19:38:22.868320 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m19:38:22.869021 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m19:38:22.869477 [info ] [Thread-4 (]: 9 of 10 START sql view model raw.my_second_dbt_model ........................... [RUN]
[0m19:38:22.870060 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m19:38:22.870379 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m19:38:22.870676 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m19:38:22.875575 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m19:38:22.876621 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m19:38:22.879477 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m19:38:22.881033 [debug] [Thread-4 (]: Creating view `flight_db`.`raw`.`my_second_dbt_model`
[0m19:38:22.881787 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m19:38:22.887180 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m19:38:22.887770 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`raw`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`raw`.`my_first_dbt_model`
where id = 1
  )

[0m19:38:22.888094 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:38:22.928903 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-f447-1dea-a890-59652eaa750a) - Created
[0m19:38:22.933492 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-f447-1ff5-b4a4-2b8918e6576a) - Created
[0m19:38:23.390367 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b5-f464-129e-b9b6-6b2a2ceb6ba1
[0m19:38:23.392292 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: Close
[0m19:38:23.392681 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b5-f447-1ff5-b4a4-2b8918e6576a) - Closing
[0m19:38:23.546591 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-f4a7-15f1-b4c6-3b110a25569c) - Created
[0m19:38:23.607527 [debug] [Thread-1 (]: Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:38:23.608378 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1221162d0>]}
[0m19:38:23.609112 [error] [Thread-1 (]: 5 of 10 ERROR creating sql view model raw.silver_flights ....................... [[31mERROR[0m in 1.46s]
[0m19:38:23.609592 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_flights
[0m19:38:23.610050 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_flights' to be skipped because of status 'error'.  Reason: Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql.
[0m19:38:23.621345 [debug] [Thread-3 (]: SQL status: OK in 1.450 seconds
[0m19:38:23.622559 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b5-f43f-18c3-b016-6f2677081acd, command-id=01f089b5-f459-1757-b6fe-1c44c5f91b5c) - Closing
[0m19:38:23.623201 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:38:23.623797 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: Close
[0m19:38:23.624048 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b5-f43f-18c3-b016-6f2677081acd) - Closing
[0m19:38:23.819930 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122172c90>]}
[0m19:38:23.821582 [info ] [Thread-3 (]: 8 of 10 OK created sql view model raw.silver_airports .......................... [[32mOK[0m in 1.66s]
[0m19:38:23.822456 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airports
[0m19:38:23.823216 [debug] [Thread-1 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m19:38:23.823721 [info ] [Thread-1 (]: 10 of 10 SKIP relation raw.gold_flight_metrics ................................. [[33mSKIP[0m]
[0m19:38:23.824164 [debug] [Thread-1 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m19:38:24.188858 [debug] [Thread-4 (]: SQL status: OK in 1.300 seconds
[0m19:38:24.191434 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b5-f4a7-15f1-b4c6-3b110a25569c, command-id=01f089b5-f4c1-1821-bcc3-446707f2dc83) - Closing
[0m19:38:24.192964 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:38:24.193650 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m19:38:24.193880 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b5-f4a7-15f1-b4c6-3b110a25569c) - Closing
[0m19:38:24.393099 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1217e8e30>]}
[0m19:38:24.394574 [info ] [Thread-4 (]: 9 of 10 OK created sql view model raw.my_second_dbt_model ...................... [[32mOK[0m in 1.52s]
[0m19:38:24.395325 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m19:38:24.905204 [debug] [Thread-2 (]: SQL status: OK in 2.730 seconds
[0m19:38:24.907042 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b5-f447-1dea-a890-59652eaa750a, command-id=01f089b5-f465-122c-84fe-82b95d04edaf) - Closing
[0m19:38:24.908050 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:38:24.909371 [debug] [Thread-2 (]: On model.flights_dbt.stg_airlines: Close
[0m19:38:24.909638 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b5-f447-1dea-a890-59652eaa750a) - Closing
[0m19:38:25.120637 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69c7d61f-2b2c-4dfe-a571-46bbf465a89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122172c30>]}
[0m19:38:25.121972 [info ] [Thread-2 (]: 6 of 10 OK created sql table model raw.stg_airlines ............................ [[32mOK[0m in 2.97s]
[0m19:38:25.122671 [debug] [Thread-2 (]: Finished running node model.flights_dbt.stg_airlines
[0m19:38:25.124587 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:38:25.124929 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:38:25.125475 [info ] [MainThread]: 
[0m19:38:25.125784 [info ] [MainThread]: Finished running 3 table models, 7 view models in 0 hours 0 minutes and 7.66 seconds (7.66s).
[0m19:38:25.127439 [debug] [MainThread]: Command end result
[0m19:38:25.159484 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:38:25.161090 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:38:25.165859 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:38:25.166041 [info ] [MainThread]: 
[0m19:38:25.166248 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m19:38:25.166390 [info ] [MainThread]: 
[0m19:38:25.166588 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m19:38:25.166775 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:38:25.166920 [info ] [MainThread]: 
[0m19:38:25.167062 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:38:25.167180 [info ] [MainThread]: 
[0m19:38:25.167329 [error] [MainThread]: [31mFailure in model silver_flights (models/silver/silver_flights.sql)[0m
[0m19:38:25.167478 [error] [MainThread]:   Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:38:25.167598 [info ] [MainThread]: 
[0m19:38:25.167722 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_flights.sql
[0m19:38:25.167827 [info ] [MainThread]: 
[0m19:38:25.167970 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=10
[0m19:38:25.171017 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.712818, "process_in_blocks": "0", "process_kernel_time": 0.27603, "process_mem_max_rss": "237649920", "process_out_blocks": "0", "process_user_time": 2.295661}
[0m19:38:25.171281 [debug] [MainThread]: Command `dbt run` failed at 19:38:25.171231 after 8.71 seconds
[0m19:38:25.171488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10640d010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122115f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e73ef0>]}
[0m19:38:25.171655 [debug] [MainThread]: Flushing usage events
[0m19:38:25.955514 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:39:15.052608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ce3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083fb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083fbb10>]}


============================== 19:39:15.054849 | 1ae76515-01c0-4f3f-875e-efd963d4647d ==============================
[0m19:39:15.054849 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:39:15.055077 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'static_parser': 'True', 'version_check': 'True', 'debug': 'False', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'printer_width': '80', 'partial_parse': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run', 'warn_error': 'None', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'write_json': 'True', 'quiet': 'False', 'indirect_selection': 'eager'}
[0m19:39:15.367356 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:39:15.367564 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:39:15.367661 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:39:15.828183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10732b360>]}
[0m19:39:15.847638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a402ad0>]}
[0m19:39:15.847891 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:39:15.897938 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:39:15.955068 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:39:15.955239 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:39:15.958244 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:39:15.973070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12970ae50>]}
[0m19:39:16.008112 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:39:16.011211 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:39:16.016481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1290e3890>]}
[0m19:39:16.016638 [info ] [MainThread]: Found 10 models, 4 data tests, 7 sources, 686 macros
[0m19:39:16.016740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1298efe70>]}
[0m19:39:16.017642 [info ] [MainThread]: 
[0m19:39:16.017800 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:39:16.017905 [info ] [MainThread]: 
[0m19:39:16.018109 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:39:16.018205 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:39:16.020912 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m19:39:16.021030 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m19:39:16.025205 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m19:39:16.025324 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m19:39:16.025412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:39:16.751582 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-145d-1e4b-8773-cbeb799a2a23) - Created
[0m19:39:17.088050 [debug] [ThreadPool]: SQL status: OK in 1.060 seconds
[0m19:39:17.097402 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b6-145d-1e4b-8773-cbeb799a2a23, command-id=01f089b6-1478-1739-b0dc-a167bf4715ad) - Closing
[0m19:39:17.097982 [debug] [ThreadPool]: On list_flight_db: Close
[0m19:39:17.098225 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-145d-1e4b-8773-cbeb799a2a23) - Closing
[0m19:39:17.282713 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:39:17.283420 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:39:17.298324 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:39:17.298751 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:39:17.299016 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:39:18.004844 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-151a-1a8c-b907-6fde49e7f461) - Created
[0m19:39:18.659866 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m19:39:18.663612 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b6-151a-1a8c-b907-6fde49e7f461, command-id=01f089b6-1538-1a6e-8026-cb8ed03cc879) - Closing
[0m19:39:18.664742 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:39:18.665043 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-151a-1a8c-b907-6fde49e7f461) - Closing
[0m19:39:18.870560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1299cbc70>]}
[0m19:39:18.875616 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m19:39:18.876081 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m19:39:18.876423 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m19:39:18.876724 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m19:39:18.877332 [info ] [Thread-2 (]: 2 of 10 START sql view model raw.bronze_airports ............................... [RUN]
[0m19:39:18.877771 [info ] [Thread-1 (]: 1 of 10 START sql view model raw.bronze_airlines ............................... [RUN]
[0m19:39:18.878203 [info ] [Thread-4 (]: 4 of 10 START sql table model raw.my_first_dbt_model ........................... [RUN]
[0m19:39:18.878605 [info ] [Thread-3 (]: 3 of 10 START sql view model raw.bronze_flights ................................ [RUN]
[0m19:39:18.879249 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m19:39:18.879660 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m19:39:18.880001 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m19:39:18.880327 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m19:39:18.880592 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m19:39:18.880840 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m19:39:18.881070 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m19:39:18.881307 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m19:39:18.881563 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m19:39:18.881798 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m19:39:18.882026 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m19:39:18.882249 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m19:39:18.890272 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m19:39:18.894484 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m19:39:18.896410 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m19:39:18.898595 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m19:39:18.899605 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m19:39:18.899853 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m19:39:18.900087 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m19:39:18.906594 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m19:39:18.910171 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:39:18.911248 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:39:18.918473 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:39:18.921142 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m19:39:18.922078 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:39:18.922377 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:39:18.922628 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:39:18.922826 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:39:18.923032 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1299d9610>]}
[0m19:39:18.923169 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129896a40>]}
[0m19:39:18.923290 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129896af0>]}
[0m19:39:18.923429 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129aba710>]}
[0m19:39:18.929681 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m19:39:18.930047 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m19:39:18.930493 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m19:39:18.939505 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m19:39:18.947642 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m19:39:18.947974 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m19:39:18.948258 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m19:39:18.948906 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m19:39:18.949024 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m19:39:18.949166 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m19:39:18.949293 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m19:39:18.949376 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m19:39:18.949490 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

[0m19:39:18.949601 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:39:18.949730 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`raw`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:39:18.949855 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m19:39:18.949958 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:39:18.950143 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:39:18.950305 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:39:19.690585 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-161d-1e28-835a-baf7d5f47b96) - Created
[0m19:39:19.739696 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-1624-18d2-b4c5-4049964301c7) - Created
[0m19:39:19.743542 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-1625-1ca8-b211-8d0f829904ad) - Created
[0m19:39:19.754223 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-1626-167b-8d4c-172cb0b96631) - Created
[0m19:39:20.118557 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b6-163f-15e3-8583-cc4f6691ac07
[0m19:39:20.120053 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m19:39:20.120411 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-1624-18d2-b4c5-4049964301c7) - Closing
[0m19:39:20.315178 [debug] [Thread-2 (]: SQL status: OK in 1.370 seconds
[0m19:39:20.317755 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b6-161d-1e28-835a-baf7d5f47b96, command-id=01f089b6-1638-104f-bf52-30a56d15fb68) - Closing
[0m19:39:20.327819 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:39:20.331680 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m19:39:20.331921 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-161d-1e28-835a-baf7d5f47b96) - Closing
[0m19:39:20.339523 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:39:20.436340 [debug] [Thread-3 (]: SQL status: OK in 1.490 seconds
[0m19:39:20.437704 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b6-1626-167b-8d4c-172cb0b96631, command-id=01f089b6-1643-18a6-9095-52c976b4f47d) - Closing
[0m19:39:20.438609 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:39:20.537969 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m19:39:20.538921 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-1626-167b-8d4c-172cb0b96631) - Closing
[0m19:39:20.541778 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129ae15c0>]}
[0m19:39:20.542674 [error] [Thread-1 (]: 1 of 10 ERROR creating sql view model raw.bronze_airlines ...................... [[31mERROR[0m in 1.66s]
[0m19:39:20.543293 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m19:39:20.543677 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_flights
[0m19:39:20.544194 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m19:39:20.544686 [info ] [Thread-1 (]: 5 of 10 START sql view model raw.silver_flights ................................ [RUN]
[0m19:39:20.741132 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129b4ddf0>]}
[0m19:39:20.742312 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m19:39:20.743051 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129b69df0>]}
[0m19:39:20.744447 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m19:39:20.744048 [info ] [Thread-2 (]: 2 of 10 OK created sql view model raw.bronze_airports .......................... [[32mOK[0m in 1.86s]
[0m19:39:20.745510 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_flights
[0m19:39:20.745208 [info ] [Thread-3 (]: 3 of 10 OK created sql view model raw.bronze_flights ........................... [[32mOK[0m in 1.86s]
[0m19:39:20.746109 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m19:39:20.749381 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m19:39:20.749864 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m19:39:20.750217 [debug] [Thread-2 (]: Began running node model.flights_dbt.stg_airlines
[0m19:39:20.750838 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airlines
[0m19:39:20.751347 [info ] [Thread-2 (]: 6 of 10 START sql table model raw.stg_airlines ................................. [RUN]
[0m19:39:20.751823 [info ] [Thread-3 (]: 7 of 10 SKIP relation raw.silver_airlines ...................................... [[33mSKIP[0m]
[0m19:39:20.752489 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.stg_airlines) - Creating connection
[0m19:39:20.752826 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airlines
[0m19:39:20.753100 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_flights
[0m19:39:20.753342 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.stg_airlines'
[0m19:39:20.753613 [debug] [Thread-3 (]: Began running node model.flights_dbt.silver_airports
[0m19:39:20.755703 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:39:20.755986 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.stg_airlines
[0m19:39:20.756333 [info ] [Thread-3 (]: 8 of 10 START sql view model raw.silver_airports ............................... [RUN]
[0m19:39:20.758427 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`silver_flights`
[0m19:39:20.761062 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.stg_airlines"
[0m19:39:20.761511 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m19:39:20.762201 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m19:39:20.762519 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m19:39:20.762885 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.silver_airports
[0m19:39:20.765080 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m19:39:20.765431 [debug] [Thread-2 (]: Began executing node model.flights_dbt.stg_airlines
[0m19:39:20.767524 [debug] [Thread-2 (]: MATERIALIZING TABLE
[0m19:39:20.767818 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m19:39:20.769146 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.stg_airlines"
[0m19:39:20.769415 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

[0m19:39:20.769672 [debug] [Thread-3 (]: Began executing node model.flights_dbt.silver_airports
[0m19:39:20.769960 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:39:20.774624 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.stg_airlines"
[0m19:39:20.775616 [debug] [Thread-2 (]: On model.flights_dbt.stg_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.stg_airlines"} */

  
    
        create or replace table `flight_db`.`raw`.`stg_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
  1            as airline_id,
  'Test Air'   as airline_name
  
[0m19:39:20.776412 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:39:20.779593 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:39:20.780434 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`silver_airports`
[0m19:39:20.780879 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m19:39:20.781198 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m19:39:20.781369 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`bronze_airports`
  )

[0m19:39:20.781521 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:39:21.589966 [debug] [Thread-4 (]: SQL status: OK in 2.640 seconds
[0m19:39:21.591544 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b6-1625-1ca8-b211-8d0f829904ad, command-id=01f089b6-1643-1cff-9ee4-d0a5bde76371) - Closing
[0m19:39:21.596619 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:39:21.612808 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m19:39:21.613131 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-1625-1ca8-b211-8d0f829904ad) - Closing
[0m19:39:21.654171 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-1749-16d7-bf7c-d00c184a36bb) - Created
[0m19:39:21.736849 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-1756-14ac-a84c-3414595289fb) - Created
[0m19:39:21.823037 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12984ce90>]}
[0m19:39:21.824479 [info ] [Thread-4 (]: 4 of 10 OK created sql table model raw.my_first_dbt_model ...................... [[32mOK[0m in 2.94s]
[0m19:39:21.825160 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m19:39:21.825818 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m19:39:21.826324 [info ] [Thread-4 (]: 9 of 10 START sql view model raw.my_second_dbt_model ........................... [RUN]
[0m19:39:21.827031 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m19:39:21.827434 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m19:39:21.827745 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m19:39:21.833248 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m19:39:21.834048 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m19:39:21.836083 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m19:39:21.836922 [debug] [Thread-4 (]: Creating view `flight_db`.`raw`.`my_second_dbt_model`
[0m19:39:21.837494 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m19:39:21.838110 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m19:39:21.838377 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`raw`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`raw`.`my_first_dbt_model`
where id = 1
  )

[0m19:39:21.838607 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:39:21.880067 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-176b-1e99-aff5-e4ad719e98c5) - Created
[0m19:39:22.403340 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b6-1786-152a-9644-ce188ec4630c
[0m19:39:22.405586 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: Close
[0m19:39:22.406003 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-176b-1e99-aff5-e4ad719e98c5) - Closing
[0m19:39:22.490930 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-17c9-1192-aa45-75db6fde4b7d) - Created
[0m19:39:22.509776 [debug] [Thread-3 (]: SQL status: OK in 1.730 seconds
[0m19:39:22.511060 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b6-1756-14ac-a84c-3414595289fb, command-id=01f089b6-1773-19cd-8651-5e8a82b8b608) - Closing
[0m19:39:22.511915 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:39:22.622968 [debug] [Thread-3 (]: On model.flights_dbt.silver_airports: Close
[0m19:39:22.623983 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-1756-14ac-a84c-3414595289fb) - Closing
[0m19:39:22.630121 [debug] [Thread-1 (]: Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:39:22.822074 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1299cf770>]}
[0m19:39:22.823088 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129b69df0>]}
[0m19:39:22.824206 [error] [Thread-1 (]: 5 of 10 ERROR creating sql view model raw.silver_flights ....................... [[31mERROR[0m in 2.28s]
[0m19:39:22.824811 [info ] [Thread-3 (]: 8 of 10 OK created sql view model raw.silver_airports .......................... [[32mOK[0m in 2.06s]
[0m19:39:22.825585 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_flights
[0m19:39:22.826049 [debug] [Thread-3 (]: Finished running node model.flights_dbt.silver_airports
[0m19:39:22.826565 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_flights' to be skipped because of status 'error'.  Reason: Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql.
[0m19:39:22.827274 [debug] [Thread-1 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m19:39:22.827623 [info ] [Thread-1 (]: 10 of 10 SKIP relation raw.gold_flight_metrics ................................. [[33mSKIP[0m]
[0m19:39:22.827966 [debug] [Thread-1 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m19:39:23.464561 [debug] [Thread-4 (]: SQL status: OK in 1.630 seconds
[0m19:39:23.466298 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b6-17c9-1192-aa45-75db6fde4b7d, command-id=01f089b6-17e3-102a-bcad-6ac91541eab5) - Closing
[0m19:39:23.467161 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:39:23.468066 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m19:39:23.468326 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-17c9-1192-aa45-75db6fde4b7d) - Closing
[0m19:39:23.659191 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1299cf410>]}
[0m19:39:23.660896 [info ] [Thread-4 (]: 9 of 10 OK created sql view model raw.my_second_dbt_model ...................... [[32mOK[0m in 1.83s]
[0m19:39:23.661585 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m19:39:24.136364 [debug] [Thread-2 (]: SQL status: OK in 3.360 seconds
[0m19:39:24.137805 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b6-1749-16d7-bf7c-d00c184a36bb, command-id=01f089b6-1764-148b-a1be-de641277d7f8) - Closing
[0m19:39:24.138829 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:39:24.140197 [debug] [Thread-2 (]: On model.flights_dbt.stg_airlines: Close
[0m19:39:24.140484 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-1749-16d7-bf7c-d00c184a36bb) - Closing
[0m19:39:24.346513 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ae76515-01c0-4f3f-875e-efd963d4647d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129b69f10>]}
[0m19:39:24.348146 [info ] [Thread-2 (]: 6 of 10 OK created sql table model raw.stg_airlines ............................ [[32mOK[0m in 3.59s]
[0m19:39:24.348838 [debug] [Thread-2 (]: Finished running node model.flights_dbt.stg_airlines
[0m19:39:24.350811 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:39:24.351204 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:39:24.351760 [info ] [MainThread]: 
[0m19:39:24.352096 [info ] [MainThread]: Finished running 3 table models, 7 view models in 0 hours 0 minutes and 8.33 seconds (8.33s).
[0m19:39:24.353862 [debug] [MainThread]: Command end result
[0m19:39:24.385500 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:39:24.386962 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:39:24.391526 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:39:24.391696 [info ] [MainThread]: 
[0m19:39:24.391881 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m19:39:24.392012 [info ] [MainThread]: 
[0m19:39:24.392187 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m19:39:24.392359 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:39:24.392486 [info ] [MainThread]: 
[0m19:39:24.392617 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:39:24.392722 [info ] [MainThread]: 
[0m19:39:24.392853 [error] [MainThread]: [31mFailure in model silver_flights (models/silver/silver_flights.sql)[0m
[0m19:39:24.393008 [error] [MainThread]:   Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:39:24.393128 [info ] [MainThread]: 
[0m19:39:24.393250 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_flights.sql
[0m19:39:24.393359 [info ] [MainThread]: 
[0m19:39:24.393493 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=10
[0m19:39:24.396455 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.375412, "process_in_blocks": "0", "process_kernel_time": 0.276234, "process_mem_max_rss": "240156672", "process_out_blocks": "0", "process_user_time": 2.344435}
[0m19:39:24.396704 [debug] [MainThread]: Command `dbt run` failed at 19:39:24.396657 after 9.38 seconds
[0m19:39:24.396895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129b480b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1299cf710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1299ce6f0>]}
[0m19:39:24.397049 [debug] [MainThread]: Flushing usage events
[0m19:39:25.143077 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:40:22.023269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b43620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b5b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b5bb10>]}


============================== 19:40:22.025616 | cd8e9d2e-2d87-442e-9064-fd13b15f2b9b ==============================
[0m19:40:22.025616 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:40:22.025846 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'printer_width': '80', 'profiles_dir': '/Users/artakerqeli/.dbt', 'empty': 'False', 'quiet': 'False', 'partial_parse': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'use_colors': 'True', 'indirect_selection': 'eager', 'fail_fast': 'False', 'warn_error': 'None', 'no_print': 'None', 'debug': 'False'}
[0m19:40:22.330875 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:40:22.331148 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:40:22.331259 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:40:22.777706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e87360>]}
[0m19:40:22.797029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d66ad0>]}
[0m19:40:22.797257 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:40:22.852042 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:40:22.910149 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:40:22.910299 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:40:22.913327 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:40:22.928086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa32e50>]}
[0m19:40:22.963337 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:40:22.964359 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:40:22.969533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f633890>]}
[0m19:40:22.969704 [info ] [MainThread]: Found 10 models, 4 data tests, 7 sources, 686 macros
[0m19:40:22.969805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc1be70>]}
[0m19:40:22.970647 [info ] [MainThread]: 
[0m19:40:22.970773 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:40:22.970860 [info ] [MainThread]: 
[0m19:40:22.971055 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:40:22.971149 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:40:22.973660 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m19:40:22.973770 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m19:40:22.978067 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m19:40:22.978217 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m19:40:22.978308 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:40:23.703466 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-3c44-189e-beee-0d3af0b0df18) - Created
[0m19:40:24.278601 [debug] [ThreadPool]: SQL status: OK in 1.300 seconds
[0m19:40:24.290565 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b6-3c44-189e-beee-0d3af0b0df18, command-id=01f089b6-3c5f-1da5-a554-d7f625072292) - Closing
[0m19:40:24.291134 [debug] [ThreadPool]: On list_flight_db: Close
[0m19:40:24.291363 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-3c44-189e-beee-0d3af0b0df18) - Closing
[0m19:40:24.657459 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:40:24.658001 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:40:24.670238 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:40:24.670660 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:40:24.670928 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:40:25.339987 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-3d3f-1b41-8f1a-970e8e1fe367) - Created
[0m19:40:25.825594 [debug] [ThreadPool]: SQL status: OK in 1.150 seconds
[0m19:40:25.830255 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b6-3d3f-1b41-8f1a-970e8e1fe367, command-id=01f089b6-3d5d-1d51-bdf7-f0c8fac71dc7) - Closing
[0m19:40:25.831419 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:40:25.831747 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-3d3f-1b41-8f1a-970e8e1fe367) - Closing
[0m19:40:26.014656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcefc70>]}
[0m19:40:26.019612 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m19:40:26.020067 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m19:40:26.020404 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m19:40:26.020700 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m19:40:26.021304 [info ] [Thread-3 (]: 3 of 10 START sql view model raw.bronze_flights ................................ [RUN]
[0m19:40:26.021769 [info ] [Thread-2 (]: 2 of 10 START sql view model raw.bronze_airports ............................... [RUN]
[0m19:40:26.022245 [info ] [Thread-1 (]: 1 of 10 START sql view model raw.bronze_airlines ............................... [RUN]
[0m19:40:26.022645 [info ] [Thread-4 (]: 4 of 10 START sql table model raw.my_first_dbt_model ........................... [RUN]
[0m19:40:26.023250 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m19:40:26.023692 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m19:40:26.024069 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m19:40:26.024417 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m19:40:26.024696 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m19:40:26.024958 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m19:40:26.025193 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m19:40:26.025416 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m19:40:26.025681 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m19:40:26.025926 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m19:40:26.026142 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m19:40:26.026339 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m19:40:26.035616 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m19:40:26.038694 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m19:40:26.039970 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m19:40:26.042097 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m19:40:26.043182 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m19:40:26.043453 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m19:40:26.043658 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m19:40:26.043827 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m19:40:26.058898 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:40:26.061751 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m19:40:26.062623 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:40:26.063602 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:40:26.064559 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:40:26.064793 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:40:26.065013 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:40:26.065218 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:40:26.065398 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd05cd0>]}
[0m19:40:26.065531 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fbc2db0>]}
[0m19:40:26.065660 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fbc2e60>]}
[0m19:40:26.065783 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fdcb110>]}
[0m19:40:26.083100 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m19:40:26.089197 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m19:40:26.089513 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m19:40:26.089795 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m19:40:26.090072 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m19:40:26.090385 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m19:40:26.090667 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m19:40:26.090874 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m19:40:26.091068 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`raw`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:40:26.091229 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:40:26.091466 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m19:40:26.091591 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m19:40:26.091741 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m19:40:26.091874 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:40:26.091966 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m19:40:26.092094 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

[0m19:40:26.092278 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m19:40:26.092383 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:40:26.092513 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:40:26.830097 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-3e22-15c3-a88b-759f85e534e0) - Created
[0m19:40:26.835220 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-3e22-1262-b942-2b7a6daee362) - Created
[0m19:40:26.866966 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-3e26-1608-b119-81fa9b3c1de2) - Created
[0m19:40:26.998162 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-3e39-16a7-a22f-12101651c08e) - Created
[0m19:40:27.245310 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b6-3e3d-1472-b980-11e541a2f382
[0m19:40:27.247889 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m19:40:27.248584 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-3e22-1262-b942-2b7a6daee362) - Closing
[0m19:40:27.476319 [debug] [Thread-3 (]: SQL status: OK in 1.380 seconds
[0m19:40:27.479959 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b6-3e26-1608-b119-81fa9b3c1de2, command-id=01f089b6-3e47-108b-a88c-8a828e4ff59d) - Closing
[0m19:40:27.492431 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:40:27.495401 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m19:40:27.495806 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-3e26-1608-b119-81fa9b3c1de2) - Closing
[0m19:40:27.504213 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:40:27.609019 [debug] [Thread-2 (]: SQL status: OK in 1.520 seconds
[0m19:40:27.610496 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b6-3e39-16a7-a22f-12101651c08e, command-id=01f089b6-3e56-1551-98b1-85673cca650c) - Closing
[0m19:40:27.611298 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:40:27.705951 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m19:40:27.707144 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-3e39-16a7-a22f-12101651c08e) - Closing
[0m19:40:27.710035 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe04fa0>]}
[0m19:40:27.710944 [error] [Thread-1 (]: 1 of 10 ERROR creating sql view model raw.bronze_airlines ...................... [[31mERROR[0m in 1.68s]
[0m19:40:27.711558 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m19:40:27.711962 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_flights
[0m19:40:27.712478 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m19:40:27.712986 [info ] [Thread-1 (]: 5 of 10 START sql view model raw.silver_flights ................................ [RUN]
[0m19:40:27.915434 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd86a50>]}
[0m19:40:27.916621 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m19:40:27.917305 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11006aab0>]}
[0m19:40:27.918623 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m19:40:27.918240 [info ] [Thread-3 (]: 3 of 10 OK created sql view model raw.bronze_flights ........................... [[32mOK[0m in 1.89s]
[0m19:40:27.919669 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_flights
[0m19:40:27.919380 [info ] [Thread-2 (]: 2 of 10 OK created sql view model raw.bronze_airports .......................... [[32mOK[0m in 1.89s]
[0m19:40:27.920265 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m19:40:27.922365 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m19:40:27.924001 [debug] [Thread-3 (]: Began running node model.flights_dbt.stg_airlines
[0m19:40:27.929757 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airlines
[0m19:40:27.930686 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m19:40:27.931218 [info ] [Thread-3 (]: 6 of 10 START sql table model raw.stg_airlines ................................. [RUN]
[0m19:40:27.931697 [info ] [Thread-2 (]: 7 of 10 SKIP relation raw.silver_airlines ...................................... [[33mSKIP[0m]
[0m19:40:27.932433 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.stg_airlines) - Creating connection
[0m19:40:27.932826 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airlines
[0m19:40:27.933253 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.stg_airlines'
[0m19:40:27.933618 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airports
[0m19:40:27.933975 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.stg_airlines
[0m19:40:27.934462 [info ] [Thread-2 (]: 8 of 10 START sql view model raw.silver_airports ............................... [RUN]
[0m19:40:27.934805 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_flights
[0m19:40:27.937781 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.stg_airlines"
[0m19:40:27.938241 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m19:40:27.940207 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:40:27.940539 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m19:40:27.941349 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`silver_flights`
[0m19:40:27.941627 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.silver_airports
[0m19:40:27.942236 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m19:40:27.942458 [debug] [Thread-3 (]: Began executing node model.flights_dbt.stg_airlines
[0m19:40:27.944248 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m19:40:27.946058 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m19:40:27.947278 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.stg_airlines"
[0m19:40:27.947594 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m19:40:27.947788 [debug] [Thread-2 (]: Began executing node model.flights_dbt.silver_airports
[0m19:40:27.948004 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

[0m19:40:27.949178 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:40:27.949394 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:40:27.949562 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.stg_airlines"
[0m19:40:27.950164 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`silver_airports`
[0m19:40:27.950514 [debug] [Thread-3 (]: On model.flights_dbt.stg_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.stg_airlines"} */

  
    
        create or replace table `flight_db`.`raw`.`stg_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
  1            as airline_id,
  'Test Air'   as airline_name
  
[0m19:40:27.951050 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m19:40:27.951234 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:40:27.951537 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m19:40:27.951778 [debug] [Thread-2 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`bronze_airports`
  )

[0m19:40:27.951957 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:40:28.594757 [debug] [Thread-4 (]: SQL status: OK in 2.500 seconds
[0m19:40:28.597332 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b6-3e22-15c3-a88b-759f85e534e0, command-id=01f089b6-3e3d-15a1-b2a3-25fa42647894) - Closing
[0m19:40:28.601931 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:40:28.614136 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m19:40:28.614390 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-3e22-15c3-a88b-759f85e534e0) - Closing
[0m19:40:28.681518 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-3f3c-1549-a730-032a2068204e) - Created
[0m19:40:28.688709 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-3f3d-1f0b-9722-f70f6724c17d) - Created
[0m19:40:28.710004 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-3f40-1f7a-a4df-7a2206df16d1) - Created
[0m19:40:28.799096 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fb6bc50>]}
[0m19:40:28.800941 [info ] [Thread-4 (]: 4 of 10 OK created sql table model raw.my_first_dbt_model ...................... [[32mOK[0m in 2.77s]
[0m19:40:28.801804 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m19:40:28.802475 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m19:40:28.802995 [info ] [Thread-4 (]: 9 of 10 START sql view model raw.my_second_dbt_model ........................... [RUN]
[0m19:40:28.803643 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m19:40:28.804014 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m19:40:28.804318 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m19:40:28.808144 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m19:40:28.809085 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m19:40:28.811831 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m19:40:28.812881 [debug] [Thread-4 (]: Creating view `flight_db`.`raw`.`my_second_dbt_model`
[0m19:40:28.813577 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m19:40:28.814418 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m19:40:28.814746 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`raw`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`raw`.`my_first_dbt_model`
where id = 1
  )

[0m19:40:28.815025 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:40:29.144279 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b6-3f5c-1fb0-9cf9-c2a496f8dbd8
[0m19:40:29.146718 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: Close
[0m19:40:29.147175 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-3f40-1f7a-a4df-7a2206df16d1) - Closing
[0m19:40:29.300907 [debug] [Thread-2 (]: SQL status: OK in 1.350 seconds
[0m19:40:29.302676 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b6-3f3c-1549-a730-032a2068204e, command-id=01f089b6-3f56-1ff8-8a4e-b9ce4f18a98d) - Closing
[0m19:40:29.303664 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:40:29.346122 [debug] [Thread-2 (]: On model.flights_dbt.silver_airports: Close
[0m19:40:29.347058 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-3f3c-1549-a730-032a2068204e) - Closing
[0m19:40:29.352795 [debug] [Thread-1 (]: Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:40:29.473753 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-3fb5-1304-87da-1bb848288da8) - Created
[0m19:40:29.535916 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcf00b0>]}
[0m19:40:29.536727 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd868d0>]}
[0m19:40:29.537852 [error] [Thread-1 (]: 5 of 10 ERROR creating sql view model raw.silver_flights ....................... [[31mERROR[0m in 1.82s]
[0m19:40:29.538567 [info ] [Thread-2 (]: 8 of 10 OK created sql view model raw.silver_airports .......................... [[32mOK[0m in 1.60s]
[0m19:40:29.539057 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_flights
[0m19:40:29.539472 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airports
[0m19:40:29.540017 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_flights' to be skipped because of status 'error'.  Reason: Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql.
[0m19:40:29.540880 [debug] [Thread-1 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m19:40:29.541230 [info ] [Thread-1 (]: 10 of 10 SKIP relation raw.gold_flight_metrics ................................. [[33mSKIP[0m]
[0m19:40:29.541550 [debug] [Thread-1 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m19:40:30.135884 [debug] [Thread-4 (]: SQL status: OK in 1.320 seconds
[0m19:40:30.138381 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b6-3fb5-1304-87da-1bb848288da8, command-id=01f089b6-3fd0-138c-a1e5-2c33d9a4eb30) - Closing
[0m19:40:30.139473 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:40:30.140566 [debug] [Thread-4 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m19:40:30.140921 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-3fb5-1304-87da-1bb848288da8) - Closing
[0m19:40:30.329733 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109fb0ef0>]}
[0m19:40:30.331535 [info ] [Thread-4 (]: 9 of 10 OK created sql view model raw.my_second_dbt_model ...................... [[32mOK[0m in 1.53s]
[0m19:40:30.332650 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m19:40:30.433772 [debug] [Thread-3 (]: SQL status: OK in 2.480 seconds
[0m19:40:30.435581 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b6-3f3d-1f0b-9722-f70f6724c17d, command-id=01f089b6-3f59-1a38-88c2-0319ccf4863b) - Closing
[0m19:40:30.436750 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:40:30.438252 [debug] [Thread-3 (]: On model.flights_dbt.stg_airlines: Close
[0m19:40:30.438584 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-3f3d-1f0b-9722-f70f6724c17d) - Closing
[0m19:40:30.637814 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd8e9d2e-2d87-442e-9064-fd13b15f2b9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f80ae10>]}
[0m19:40:30.639622 [info ] [Thread-3 (]: 6 of 10 OK created sql table model raw.stg_airlines ............................ [[32mOK[0m in 2.71s]
[0m19:40:30.640698 [debug] [Thread-3 (]: Finished running node model.flights_dbt.stg_airlines
[0m19:40:30.643119 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:40:30.643558 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:40:30.644200 [info ] [MainThread]: 
[0m19:40:30.644546 [info ] [MainThread]: Finished running 3 table models, 7 view models in 0 hours 0 minutes and 7.67 seconds (7.67s).
[0m19:40:30.646289 [debug] [MainThread]: Command end result
[0m19:40:30.679728 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:40:30.681182 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:40:30.685765 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:40:30.685936 [info ] [MainThread]: 
[0m19:40:30.686119 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m19:40:30.686254 [info ] [MainThread]: 
[0m19:40:30.686427 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m19:40:30.686608 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:40:30.686737 [info ] [MainThread]: 
[0m19:40:30.686875 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:40:30.686989 [info ] [MainThread]: 
[0m19:40:30.687123 [error] [MainThread]: [31mFailure in model silver_flights (models/silver/silver_flights.sql)[0m
[0m19:40:30.687281 [error] [MainThread]:   Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:40:30.687402 [info ] [MainThread]: 
[0m19:40:30.687539 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_flights.sql
[0m19:40:30.687646 [info ] [MainThread]: 
[0m19:40:30.687775 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=10
[0m19:40:30.690896 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.703576, "process_in_blocks": "0", "process_kernel_time": 0.269878, "process_mem_max_rss": "243646464", "process_out_blocks": "0", "process_user_time": 2.258764}
[0m19:40:30.691109 [debug] [MainThread]: Command `dbt run` failed at 19:40:30.691067 after 8.70 seconds
[0m19:40:30.691280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108998890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcf1fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcf3050>]}
[0m19:40:30.691420 [debug] [MainThread]: Flushing usage events
[0m19:40:31.416426 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:41:17.555881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10801f620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109afb890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109afbb10>]}


============================== 19:41:17.558277 | 75a7a789-2438-4d66-841b-0e7bfc37175c ==============================
[0m19:41:17.558277 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:41:17.558511 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'target_path': 'None', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'warn_error': 'None', 'quiet': 'False', 'introspect': 'True', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'cache_selected_only': 'False', 'printer_width': '80', 'use_colors': 'True', 'use_experimental_parser': 'False', 'static_parser': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'write_json': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True'}
[0m19:41:17.863895 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:41:17.864105 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:41:17.864201 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:41:18.315332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108363360>]}
[0m19:41:18.334714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b602ad0>]}
[0m19:41:18.334960 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:41:18.387863 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:41:18.446436 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:41:18.446605 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:41:18.449639 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:41:18.464653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a00ae50>]}
[0m19:41:18.500192 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:41:18.501811 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:41:18.508672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c0b890>]}
[0m19:41:18.508834 [info ] [MainThread]: Found 10 models, 4 data tests, 7 sources, 686 macros
[0m19:41:18.508931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a1f3e70>]}
[0m19:41:18.509724 [info ] [MainThread]: 
[0m19:41:18.509842 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:41:18.509925 [info ] [MainThread]: 
[0m19:41:18.510114 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:41:18.510215 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:41:18.512738 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db) - Creating connection
[0m19:41:18.512850 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db'
[0m19:41:18.517003 [debug] [ThreadPool]: Using databricks connection "list_flight_db"
[0m19:41:18.517125 [debug] [ThreadPool]: On list_flight_db: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db"} */

    show databases
  
[0m19:41:18.517216 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:41:19.192377 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-5d58-1151-962b-1680203e5148) - Created
[0m19:41:19.533443 [debug] [ThreadPool]: SQL status: OK in 1.020 seconds
[0m19:41:19.543280 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b6-5d58-1151-962b-1680203e5148, command-id=01f089b6-5d74-18be-963b-6b12e569cef7) - Closing
[0m19:41:19.543818 [debug] [ThreadPool]: On list_flight_db: Close
[0m19:41:19.544045 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-5d58-1151-962b-1680203e5148) - Closing
[0m19:41:19.773765 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:41:19.774578 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:41:19.787167 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:41:19.787660 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:41:19.788013 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:41:20.522580 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-5e1f-1881-ad22-690b91b0e7b5) - Created
[0m19:41:21.132967 [debug] [ThreadPool]: SQL status: OK in 1.340 seconds
[0m19:41:21.137499 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b6-5e1f-1881-ad22-690b91b0e7b5, command-id=01f089b6-5e3d-1fee-a4fa-cb6f735db62b) - Closing
[0m19:41:21.139110 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:41:21.139648 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-5e1f-1881-ad22-690b91b0e7b5) - Closing
[0m19:41:21.365163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a2c7c70>]}
[0m19:41:21.370482 [debug] [Thread-2 (]: Began running node model.flights_dbt.bronze_airports
[0m19:41:21.370926 [debug] [Thread-4 (]: Began running node model.flights_dbt.my_first_dbt_model
[0m19:41:21.371248 [debug] [Thread-1 (]: Began running node model.flights_dbt.bronze_airlines
[0m19:41:21.371531 [debug] [Thread-3 (]: Began running node model.flights_dbt.bronze_flights
[0m19:41:21.372140 [info ] [Thread-2 (]: 2 of 10 START sql view model raw.bronze_airports ............................... [RUN]
[0m19:41:21.372579 [info ] [Thread-4 (]: 4 of 10 START sql table model raw.my_first_dbt_model ........................... [RUN]
[0m19:41:21.373011 [info ] [Thread-1 (]: 1 of 10 START sql view model raw.bronze_airlines ............................... [RUN]
[0m19:41:21.373415 [info ] [Thread-3 (]: 3 of 10 START sql view model raw.bronze_flights ................................ [RUN]
[0m19:41:21.374059 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airports) - Creating connection
[0m19:41:21.374470 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_first_dbt_model) - Creating connection
[0m19:41:21.374815 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_airlines) - Creating connection
[0m19:41:21.375153 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.bronze_flights) - Creating connection
[0m19:41:21.375432 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airports'
[0m19:41:21.375681 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.flights_dbt.my_first_dbt_model'
[0m19:41:21.375907 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_airlines'
[0m19:41:21.376126 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.bronze_flights'
[0m19:41:21.376400 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.bronze_airports
[0m19:41:21.376672 [debug] [Thread-4 (]: Began compiling node model.flights_dbt.my_first_dbt_model
[0m19:41:21.376912 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.bronze_airlines
[0m19:41:21.377153 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.bronze_flights
[0m19:41:21.386703 [debug] [Thread-4 (]: Writing injected SQL for node "model.flights_dbt.my_first_dbt_model"
[0m19:41:21.390217 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.bronze_airports"
[0m19:41:21.391557 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.bronze_airlines"
[0m19:41:21.393625 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.bronze_flights"
[0m19:41:21.394761 [debug] [Thread-2 (]: Began executing node model.flights_dbt.bronze_airports
[0m19:41:21.395036 [debug] [Thread-3 (]: Began executing node model.flights_dbt.bronze_flights
[0m19:41:21.395263 [debug] [Thread-1 (]: Began executing node model.flights_dbt.bronze_airlines
[0m19:41:21.395456 [debug] [Thread-4 (]: Began executing node model.flights_dbt.my_first_dbt_model
[0m19:41:21.405059 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:41:21.406081 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:41:21.406956 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:41:21.416530 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m19:41:21.414287 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:41:21.416784 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:41:21.417010 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:41:21.417426 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a2dcc50>]}
[0m19:41:21.417216 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:41:21.417597 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a19a620>]}
[0m19:41:21.417724 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a19aa40>]}
[0m19:41:21.423953 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`bronze_airports`
[0m19:41:21.424091 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a3e4730>]}
[0m19:41:21.424506 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`bronze_airlines`
[0m19:41:21.428981 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.bronze_airports"
[0m19:41:21.429334 [debug] [Thread-3 (]: Creating view `flight_db`.`raw`.`bronze_flights`
[0m19:41:21.435962 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.bronze_airlines"
[0m19:41:21.443599 [debug] [Thread-4 (]: Writing runtime sql for node "model.flights_dbt.my_first_dbt_model"
[0m19:41:21.443944 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.bronze_flights"
[0m19:41:21.444468 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.bronze_flights"
[0m19:41:21.444577 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.bronze_airlines"
[0m19:41:21.444675 [debug] [Thread-4 (]: Using databricks connection "model.flights_dbt.my_first_dbt_model"
[0m19:41:21.444800 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_flights`
  
  as (
    SELECT
    flight_id,
    origin,
    destination AS dest,  -- or the correct column name
    airline,
    arr_time
FROM `flight_db`.`raw`.`flights`
  )

[0m19:41:21.444891 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.bronze_airports"
[0m19:41:21.445016 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

[0m19:41:21.445154 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_first_dbt_model"} */

  
    
        create or replace table `flight_db`.`raw`.`my_first_dbt_model`
      
      
  using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:41:21.445271 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:41:21.445385 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`airports`
  )

[0m19:41:21.445501 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:41:21.445618 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:41:21.445785 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:41:22.172993 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-5f1f-17c9-b2bc-3bd4062f76ad) - Created
[0m19:41:22.185360 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-5f21-14d9-8c85-47e9d2f07882) - Created
[0m19:41:22.258333 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-5f29-1654-a3ae-243a55eed34b) - Created
[0m19:41:22.269667 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-5f2a-1fc9-b97e-75ce1c2d15d9) - Created
[0m19:41:22.753913 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */

  
  
  create or replace view `flight_db`.`raw`.`bronze_airlines`
  
  as (
    create or replace view `flight_db`.`raw`.`bronze_airlines` as
----^^^
select
    airline,
    name,
    country
from `flight_db`.`raw`.`airlines`
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b6-5f47-1003-bc00-ab9158ef010b
[0m19:41:22.755591 [debug] [Thread-1 (]: On model.flights_dbt.bronze_airlines: Close
[0m19:41:22.756000 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-5f29-1654-a3ae-243a55eed34b) - Closing
[0m19:41:22.962290 [debug] [Thread-1 (]: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:41:22.965416 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ac7d470>]}
[0m19:41:22.966223 [error] [Thread-1 (]: 1 of 10 ERROR creating sql view model raw.bronze_airlines ...................... [[31mERROR[0m in 1.59s]
[0m19:41:22.966781 [debug] [Thread-1 (]: Finished running node model.flights_dbt.bronze_airlines
[0m19:41:22.967126 [debug] [Thread-1 (]: Began running node model.flights_dbt.silver_flights
[0m19:41:22.967606 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.bronze_airlines' to be skipped because of status 'error'.  Reason: Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql.
[0m19:41:22.968101 [info ] [Thread-1 (]: 5 of 10 START sql view model raw.silver_flights ................................ [RUN]
[0m19:41:22.969665 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_flights) - Creating connection
[0m19:41:22.969997 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.silver_flights'
[0m19:41:22.970260 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.silver_flights
[0m19:41:22.972667 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.silver_flights"
[0m19:41:22.973280 [debug] [Thread-1 (]: Began executing node model.flights_dbt.silver_flights
[0m19:41:22.974856 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:41:22.975679 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`silver_flights`
[0m19:41:22.976206 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.silver_flights"
[0m19:41:22.976565 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.silver_flights"
[0m19:41:22.976791 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

[0m19:41:22.976992 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:41:23.120883 [debug] [Thread-3 (]: SQL status: OK in 1.680 seconds
[0m19:41:23.122635 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b6-5f1f-17c9-b2bc-3bd4062f76ad, command-id=01f089b6-5f39-120e-a3f1-c6ee6ad142b6) - Closing
[0m19:41:23.137685 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:41:23.141504 [debug] [Thread-3 (]: On model.flights_dbt.bronze_flights: Close
[0m19:41:23.141801 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-5f1f-17c9-b2bc-3bd4062f76ad) - Closing
[0m19:41:23.142103 [debug] [Thread-2 (]: SQL status: OK in 1.700 seconds
[0m19:41:23.143144 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b6-5f21-14d9-8c85-47e9d2f07882, command-id=01f089b6-5f41-1d29-8d50-3496f4fb5a18) - Closing
[0m19:41:23.143661 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:41:23.357636 [debug] [Thread-2 (]: On model.flights_dbt.bronze_airports: Close
[0m19:41:23.358719 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-5f21-14d9-8c85-47e9d2f07882) - Closing
[0m19:41:23.566544 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a2cecf0>]}
[0m19:41:23.567572 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a2ce6f0>]}
[0m19:41:23.568823 [info ] [Thread-3 (]: 3 of 10 OK created sql view model raw.bronze_flights ........................... [[32mOK[0m in 2.19s]
[0m19:41:23.570128 [debug] [Thread-3 (]: Finished running node model.flights_dbt.bronze_flights
[0m19:41:23.569568 [info ] [Thread-2 (]: 2 of 10 OK created sql view model raw.bronze_airports .......................... [[32mOK[0m in 2.19s]
[0m19:41:23.570693 [debug] [Thread-3 (]: Began running node model.flights_dbt.stg_airlines
[0m19:41:23.571313 [debug] [Thread-2 (]: Finished running node model.flights_dbt.bronze_airports
[0m19:41:23.571760 [info ] [Thread-3 (]: 6 of 10 START sql table model raw.stg_airlines ................................. [RUN]
[0m19:41:23.572150 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airlines
[0m19:41:23.572826 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.stg_airlines) - Creating connection
[0m19:41:23.573359 [info ] [Thread-2 (]: 7 of 10 SKIP relation raw.silver_airlines ...................................... [[33mSKIP[0m]
[0m19:41:23.573725 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.flights_dbt.stg_airlines'
[0m19:41:23.574043 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airlines
[0m19:41:23.574321 [debug] [Thread-3 (]: Began compiling node model.flights_dbt.stg_airlines
[0m19:41:23.574626 [debug] [Thread-2 (]: Began running node model.flights_dbt.silver_airports
[0m19:41:23.578765 [debug] [Thread-3 (]: Writing injected SQL for node "model.flights_dbt.stg_airlines"
[0m19:41:23.579309 [info ] [Thread-2 (]: 8 of 10 START sql view model raw.silver_airports ............................... [RUN]
[0m19:41:23.579950 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.silver_airports) - Creating connection
[0m19:41:23.580273 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.flights_dbt.silver_airports'
[0m19:41:23.580527 [debug] [Thread-2 (]: Began compiling node model.flights_dbt.silver_airports
[0m19:41:23.585005 [debug] [Thread-2 (]: Writing injected SQL for node "model.flights_dbt.silver_airports"
[0m19:41:23.585517 [debug] [Thread-3 (]: Began executing node model.flights_dbt.stg_airlines
[0m19:41:23.587445 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m19:41:23.588964 [debug] [Thread-3 (]: Writing runtime sql for node "model.flights_dbt.stg_airlines"
[0m19:41:23.589307 [debug] [Thread-2 (]: Began executing node model.flights_dbt.silver_airports
[0m19:41:23.593458 [debug] [Thread-3 (]: Using databricks connection "model.flights_dbt.stg_airlines"
[0m19:41:23.594341 [debug] [Thread-3 (]: On model.flights_dbt.stg_airlines: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.stg_airlines"} */

  
    
        create or replace table `flight_db`.`raw`.`stg_airlines`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
  1            as airline_id,
  'Test Air'   as airline_name
  
[0m19:41:23.595203 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:41:23.599899 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m19:41:23.600716 [debug] [Thread-2 (]: Creating view `flight_db`.`raw`.`silver_airports`
[0m19:41:23.601205 [debug] [Thread-2 (]: Writing runtime sql for node "model.flights_dbt.silver_airports"
[0m19:41:23.601579 [debug] [Thread-2 (]: Using databricks connection "model.flights_dbt.silver_airports"
[0m19:41:23.601819 [debug] [Thread-2 (]: On model.flights_dbt.silver_airports: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_airports"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_airports`
  
  as (
    select
    airport_id,
    iata_code,
    city,
    state,
    country
from `flight_db`.`raw`.`bronze_airports`
  )

[0m19:41:23.602005 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:41:23.653784 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-6000-1af0-9f28-f258c51ca972) - Created
[0m19:41:24.044475 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */

  
  
  create or replace view `flight_db`.`raw`.`silver_flights`
  
  as (
    create or replace view `flight_db`.`raw`.`silver_flights` as
----^^^
select
    f.flight_id,
    f.airline,
    f.origin,
    f.dest,
    f.arr_time
from `flight_db`.`raw`.`flights` f
  )

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f089b6-601c-1e36-b40c-9072a536d913
[0m19:41:24.047058 [debug] [Thread-1 (]: On model.flights_dbt.silver_flights: Close
[0m19:41:24.047732 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-6000-1af0-9f28-f258c51ca972) - Closing
[0m19:41:24.283265 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-6060-1769-974e-6c757eb5322e) - Created
[0m19:41:24.289204 [debug] [Thread-1 (]: Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:41:24.290005 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10829eb70>]}
[0m19:41:24.290864 [error] [Thread-1 (]: 5 of 10 ERROR creating sql view model raw.silver_flights ....................... [[31mERROR[0m in 1.32s]
[0m19:41:24.291410 [debug] [Thread-1 (]: Finished running node model.flights_dbt.silver_flights
[0m19:41:24.291918 [debug] [Thread-7 (]: Marking all children of 'model.flights_dbt.silver_flights' to be skipped because of status 'error'.  Reason: Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql.
[0m19:41:24.391613 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-606e-16f2-b4c7-7ac0e5c17f98) - Created
[0m19:41:24.541683 [debug] [Thread-4 (]: SQL status: OK in 3.100 seconds
[0m19:41:24.543479 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b6-5f2a-1fc9-b97e-75ce1c2d15d9, command-id=01f089b6-5f4b-147d-8c6c-1151db7dab81) - Closing
[0m19:41:24.547349 [debug] [Thread-4 (]: Applying tags to relation None
[0m19:41:24.559488 [debug] [Thread-4 (]: On model.flights_dbt.my_first_dbt_model: Close
[0m19:41:24.559718 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-5f2a-1fc9-b97e-75ce1c2d15d9) - Closing
[0m19:41:24.760572 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119de66f0>]}
[0m19:41:24.762311 [info ] [Thread-4 (]: 4 of 10 OK created sql table model raw.my_first_dbt_model ...................... [[32mOK[0m in 3.39s]
[0m19:41:24.763249 [debug] [Thread-4 (]: Finished running node model.flights_dbt.my_first_dbt_model
[0m19:41:24.763798 [debug] [Thread-1 (]: Began running node model.flights_dbt.my_second_dbt_model
[0m19:41:24.764313 [info ] [Thread-1 (]: 9 of 10 START sql view model raw.my_second_dbt_model ........................... [RUN]
[0m19:41:24.764879 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.flights_dbt.my_second_dbt_model) - Creating connection
[0m19:41:24.765186 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.flights_dbt.my_second_dbt_model'
[0m19:41:24.765458 [debug] [Thread-1 (]: Began compiling node model.flights_dbt.my_second_dbt_model
[0m19:41:24.770166 [debug] [Thread-1 (]: Writing injected SQL for node "model.flights_dbt.my_second_dbt_model"
[0m19:41:24.771747 [debug] [Thread-1 (]: Began executing node model.flights_dbt.my_second_dbt_model
[0m19:41:24.774626 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m19:41:24.775827 [debug] [Thread-1 (]: Creating view `flight_db`.`raw`.`my_second_dbt_model`
[0m19:41:24.776611 [debug] [Thread-1 (]: Writing runtime sql for node "model.flights_dbt.my_second_dbt_model"
[0m19:41:24.777529 [debug] [Thread-1 (]: Using databricks connection "model.flights_dbt.my_second_dbt_model"
[0m19:41:24.777857 [debug] [Thread-1 (]: On model.flights_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.my_second_dbt_model"} */

  
  
  create or replace view `flight_db`.`raw`.`my_second_dbt_model`
  
  as (
    -- Use the `ref` function to select from other models

select *
from `flight_db`.`raw`.`my_first_dbt_model`
where id = 1
  )

[0m19:41:24.778118 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:41:24.895354 [debug] [Thread-2 (]: SQL status: OK in 1.290 seconds
[0m19:41:24.897057 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b6-6060-1769-974e-6c757eb5322e, command-id=01f089b6-607b-163c-8fa8-d9b28ea26f87) - Closing
[0m19:41:24.898032 [debug] [Thread-2 (]: Applying tags to relation None
[0m19:41:24.899045 [debug] [Thread-2 (]: On model.flights_dbt.silver_airports: Close
[0m19:41:24.899378 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-6060-1769-974e-6c757eb5322e) - Closing
[0m19:41:25.089307 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ac38950>]}
[0m19:41:25.091216 [info ] [Thread-2 (]: 8 of 10 OK created sql view model raw.silver_airports .......................... [[32mOK[0m in 1.51s]
[0m19:41:25.092049 [debug] [Thread-2 (]: Finished running node model.flights_dbt.silver_airports
[0m19:41:25.092733 [debug] [Thread-4 (]: Began running node model.flights_dbt.gold_flight_metrics
[0m19:41:25.093172 [info ] [Thread-4 (]: 10 of 10 SKIP relation raw.gold_flight_metrics ................................. [[33mSKIP[0m]
[0m19:41:25.093639 [debug] [Thread-4 (]: Finished running node model.flights_dbt.gold_flight_metrics
[0m19:41:25.426798 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-610e-1618-bc13-889fc4af293e) - Created
[0m19:41:26.136290 [debug] [Thread-1 (]: SQL status: OK in 1.360 seconds
[0m19:41:26.137792 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b6-610e-1618-bc13-889fc4af293e, command-id=01f089b6-612a-1503-a48e-a3d508461ccd) - Closing
[0m19:41:26.138721 [debug] [Thread-1 (]: Applying tags to relation None
[0m19:41:26.139709 [debug] [Thread-1 (]: On model.flights_dbt.my_second_dbt_model: Close
[0m19:41:26.140003 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-610e-1618-bc13-889fc4af293e) - Closing
[0m19:41:26.331336 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ac62750>]}
[0m19:41:26.333164 [info ] [Thread-1 (]: 9 of 10 OK created sql view model raw.my_second_dbt_model ...................... [[32mOK[0m in 1.57s]
[0m19:41:26.334113 [debug] [Thread-1 (]: Finished running node model.flights_dbt.my_second_dbt_model
[0m19:41:26.398751 [debug] [Thread-3 (]: SQL status: OK in 2.800 seconds
[0m19:41:26.401291 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b6-606e-16f2-b4c7-7ac0e5c17f98, command-id=01f089b6-609d-19f2-90d7-a62ca2a37f1f) - Closing
[0m19:41:26.402734 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:41:26.404409 [debug] [Thread-3 (]: On model.flights_dbt.stg_airlines: Close
[0m19:41:26.404744 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-606e-16f2-b4c7-7ac0e5c17f98) - Closing
[0m19:41:26.612305 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75a7a789-2438-4d66-841b-0e7bfc37175c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a2cfb30>]}
[0m19:41:26.613869 [info ] [Thread-3 (]: 6 of 10 OK created sql table model raw.stg_airlines ............................ [[32mOK[0m in 3.04s]
[0m19:41:26.614426 [debug] [Thread-3 (]: Finished running node model.flights_dbt.stg_airlines
[0m19:41:26.616243 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:41:26.616572 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:41:26.617079 [info ] [MainThread]: 
[0m19:41:26.617390 [info ] [MainThread]: Finished running 3 table models, 7 view models in 0 hours 0 minutes and 8.11 seconds (8.11s).
[0m19:41:26.618976 [debug] [MainThread]: Command end result
[0m19:41:26.644208 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:41:26.645345 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:41:26.649443 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:41:26.649582 [info ] [MainThread]: 
[0m19:41:26.649727 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m19:41:26.649849 [info ] [MainThread]: 
[0m19:41:26.650002 [error] [MainThread]: [31mFailure in model bronze_airlines (models/bronze/bronze_airlines.sql)[0m
[0m19:41:26.650160 [error] [MainThread]:   Database Error in model bronze_airlines (models/bronze/bronze_airlines.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.bronze_airlines"} */
  
    
    
    create or replace view `flight_db`.`raw`.`bronze_airlines`
    
    as (
      create or replace view `flight_db`.`raw`.`bronze_airlines` as
  ----^^^
  select
      airline,
      name,
      country
  from `flight_db`.`raw`.`airlines`
    )
  
  compiled code at target/run/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:41:26.650295 [info ] [MainThread]: 
[0m19:41:26.650423 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/bronze/bronze_airlines.sql
[0m19:41:26.650528 [info ] [MainThread]: 
[0m19:41:26.650658 [error] [MainThread]: [31mFailure in model silver_flights (models/silver/silver_flights.sql)[0m
[0m19:41:26.650806 [error] [MainThread]:   Database Error in model silver_flights (models/silver/silver_flights.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 8, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "model.flights_dbt.silver_flights"} */
  
    
    
    create or replace view `flight_db`.`raw`.`silver_flights`
    
    as (
      create or replace view `flight_db`.`raw`.`silver_flights` as
  ----^^^
  select
      f.flight_id,
      f.airline,
      f.origin,
      f.dest,
      f.arr_time
  from `flight_db`.`raw`.`flights` f
    )
  
  compiled code at target/run/flights_dbt/models/silver/silver_flights.sql
[0m19:41:26.650921 [info ] [MainThread]: 
[0m19:41:26.651045 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/silver/silver_flights.sql
[0m19:41:26.651149 [info ] [MainThread]: 
[0m19:41:26.651265 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=10
[0m19:41:26.654229 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.129445, "process_in_blocks": "0", "process_kernel_time": 0.271683, "process_mem_max_rss": "231276544", "process_out_blocks": "0", "process_user_time": 2.294345}
[0m19:41:26.654468 [debug] [MainThread]: Command `dbt run` failed at 19:41:26.654425 after 9.13 seconds
[0m19:41:26.654671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b3deb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bda2b70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10829eb70>]}
[0m19:41:26.654820 [debug] [MainThread]: Flushing usage events
[0m19:41:27.403490 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:42:24.835297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cab620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106cc7890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106cc7b10>]}


============================== 19:42:24.838608 | 3a6ecdfc-1e57-4411-9081-598bc7c7dc9c ==============================
[0m19:42:24.838608 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:42:24.838956 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'target_path': 'None', 'indirect_selection': 'eager', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'static_parser': 'True', 'quiet': 'False', 'fail_fast': 'False', 'warn_error': 'None', 'log_cache_events': 'False', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'introspect': 'True', 'invocation_command': 'dbt test', 'write_json': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'no_print': 'None', 'empty': 'None'}
[0m19:42:25.152843 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:42:25.153046 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:42:25.153162 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:42:25.597232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3a6ecdfc-1e57-4411-9081-598bc7c7dc9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ff3360>]}
[0m19:42:25.616543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3a6ecdfc-1e57-4411-9081-598bc7c7dc9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ecead0>]}
[0m19:42:25.616766 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:42:25.666306 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:42:25.724196 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:42:25.724347 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:42:25.727307 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:42:25.741963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3a6ecdfc-1e57-4411-9081-598bc7c7dc9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db7ba50>]}
[0m19:42:25.777124 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:42:25.778690 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:42:25.792251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3a6ecdfc-1e57-4411-9081-598bc7c7dc9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de5eb70>]}
[0m19:42:25.792432 [info ] [MainThread]: Found 10 models, 4 data tests, 7 sources, 686 macros
[0m19:42:25.792535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a6ecdfc-1e57-4411-9081-598bc7c7dc9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de03bd0>]}
[0m19:42:25.793312 [info ] [MainThread]: 
[0m19:42:25.793429 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:42:25.793517 [info ] [MainThread]: 
[0m19:42:25.793712 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:42:25.793808 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:42:25.796412 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:42:25.796522 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:42:25.801233 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:42:25.801372 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:42:25.801473 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:42:26.545463 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-857d-1038-ab66-b4dc7c3214c7) - Created
[0m19:42:27.154588 [debug] [ThreadPool]: SQL status: OK in 1.350 seconds
[0m19:42:27.161906 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b6-857d-1038-ab66-b4dc7c3214c7, command-id=01f089b6-8598-1429-aefe-6ea73caee3b5) - Closing
[0m19:42:27.162578 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:42:27.162765 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-857d-1038-ab66-b4dc7c3214c7) - Closing
[0m19:42:27.384578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a6ecdfc-1e57-4411-9081-598bc7c7dc9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b29a0d0>]}
[0m19:42:27.388701 [debug] [Thread-2 (]: Began running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:42:27.389172 [debug] [Thread-1 (]: Began running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:42:27.389506 [debug] [Thread-4 (]: Began running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:42:27.389794 [debug] [Thread-3 (]: Began running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:42:27.390174 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m19:42:27.390529 [info ] [Thread-1 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m19:42:27.390942 [info ] [Thread-4 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m19:42:27.391368 [info ] [Thread-3 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m19:42:27.392018 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m19:42:27.392532 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m19:42:27.392968 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m19:42:27.393346 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m19:42:27.393653 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778'
[0m19:42:27.393932 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710'
[0m19:42:27.394192 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493'
[0m19:42:27.394447 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_first_dbt_model_id.16e066b321'
[0m19:42:27.394766 [debug] [Thread-2 (]: Began compiling node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:42:27.395039 [debug] [Thread-1 (]: Began compiling node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:42:27.395279 [debug] [Thread-4 (]: Began compiling node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:42:27.395513 [debug] [Thread-3 (]: Began compiling node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:42:27.411081 [debug] [Thread-1 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:42:27.415709 [debug] [Thread-2 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:42:27.419706 [debug] [Thread-4 (]: Writing injected SQL for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:42:27.421759 [debug] [Thread-3 (]: Writing injected SQL for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:42:27.422687 [debug] [Thread-2 (]: Began executing node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:42:27.422901 [debug] [Thread-3 (]: Began executing node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:42:27.423079 [debug] [Thread-4 (]: Began executing node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:42:27.423231 [debug] [Thread-1 (]: Began executing node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:42:27.432187 [debug] [Thread-2 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:42:27.433453 [debug] [Thread-3 (]: Writing runtime sql for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:42:27.434514 [debug] [Thread-4 (]: Writing runtime sql for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:42:27.435629 [debug] [Thread-1 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:42:27.436087 [debug] [Thread-2 (]: Using databricks connection "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:42:27.436222 [debug] [Thread-3 (]: Using databricks connection "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:42:27.436405 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:42:27.436551 [debug] [Thread-4 (]: Using databricks connection "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:42:27.436733 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:42:27.436877 [debug] [Thread-1 (]: Using databricks connection "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:42:27.437027 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:42:27.437191 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:42:27.437343 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:42:27.437504 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:42:27.437758 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:42:27.437996 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:42:28.199838 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-867a-1767-af53-822cdcbac58b) - Created
[0m19:42:28.264954 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-8682-18b9-897f-d71ca24d13a8) - Created
[0m19:42:28.266756 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-8683-16b9-8392-d0e4e8b505db) - Created
[0m19:42:28.336343 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-8688-17f8-bd13-2af9a6933a90) - Created
[0m19:42:29.210213 [debug] [Thread-2 (]: SQL status: OK in 1.770 seconds
[0m19:42:29.215758 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b6-8682-18b9-897f-d71ca24d13a8, command-id=01f089b6-869f-1887-8f8c-f5cc266ef5be) - Closing
[0m19:42:29.220305 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: Close
[0m19:42:29.221038 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-8682-18b9-897f-d71ca24d13a8) - Closing
[0m19:42:29.485346 [info ] [Thread-2 (]: 2 of 4 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 2.09s]
[0m19:42:29.486368 [debug] [Thread-2 (]: Finished running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:42:29.542907 [debug] [Thread-1 (]: SQL status: OK in 2.100 seconds
[0m19:42:29.546703 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b6-8688-17f8-bd13-2af9a6933a90, command-id=01f089b6-86a8-1cea-b967-59f4e4ee54d5) - Closing
[0m19:42:29.550101 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m19:42:29.550641 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-8688-17f8-bd13-2af9a6933a90) - Closing
[0m19:42:29.551467 [debug] [Thread-4 (]: SQL status: OK in 2.110 seconds
[0m19:42:29.554547 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b6-8683-16b9-8392-d0e4e8b505db, command-id=01f089b6-86a1-10f7-bd8e-db12ebefbd6c) - Closing
[0m19:42:29.573285 [debug] [Thread-3 (]: SQL status: OK in 2.140 seconds
[0m19:42:29.575732 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b6-867a-1767-af53-822cdcbac58b, command-id=01f089b6-8698-1a33-8dd3-6f9dd7441100) - Closing
[0m19:42:29.773449 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m19:42:29.774566 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-8683-16b9-8392-d0e4e8b505db) - Closing
[0m19:42:29.984388 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: Close
[0m19:42:29.985212 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-867a-1767-af53-822cdcbac58b) - Closing
[0m19:42:30.179106 [error] [Thread-1 (]: 1 of 4 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 2.79s]
[0m19:42:30.179955 [info ] [Thread-4 (]: 4 of 4 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 2.79s]
[0m19:42:30.181240 [debug] [Thread-1 (]: Finished running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:42:30.180605 [info ] [Thread-3 (]: 3 of 4 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 2.79s]
[0m19:42:30.181859 [debug] [Thread-4 (]: Finished running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:42:30.182525 [debug] [Thread-3 (]: Finished running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:42:30.184351 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:42:30.184694 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:42:30.185123 [info ] [MainThread]: 
[0m19:42:30.185438 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 4.39 seconds (4.39s).
[0m19:42:30.186533 [debug] [MainThread]: Command end result
[0m19:42:30.224981 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:42:30.225970 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:42:30.229438 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:42:30.229582 [info ] [MainThread]: 
[0m19:42:30.229742 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:42:30.229858 [info ] [MainThread]: 
[0m19:42:30.230014 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m19:42:30.230157 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m19:42:30.230253 [info ] [MainThread]: 
[0m19:42:30.230378 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m19:42:30.230483 [info ] [MainThread]: 
[0m19:42:30.230600 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m19:42:30.233054 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 5.435614, "process_in_blocks": "0", "process_kernel_time": 0.248323, "process_mem_max_rss": "235290624", "process_out_blocks": "0", "process_user_time": 1.708855}
[0m19:42:30.233283 [debug] [MainThread]: Command `dbt test` failed at 19:42:30.233234 after 5.44 seconds
[0m19:42:30.233461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a7ae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dea8cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dea95a0>]}
[0m19:42:30.233610 [debug] [MainThread]: Flushing usage events
[0m19:42:30.882079 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:43:37.590667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104207620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105223890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105223b10>]}


============================== 19:43:37.592900 | 999fc73f-8a11-4a87-b07c-c6fda01a439a ==============================
[0m19:43:37.592900 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:43:37.593120 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'use_colors': 'True', 'invocation_command': 'dbt test', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'empty': 'None', 'no_print': 'None', 'version_check': 'True', 'quiet': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'printer_width': '80', 'partial_parse': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'debug': 'False', 'write_json': 'True', 'introspect': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt'}
[0m19:43:37.894396 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:43:37.894586 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:43:37.894680 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:43:38.351631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '999fc73f-8a11-4a87-b07c-c6fda01a439a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10454b360>]}
[0m19:43:38.371223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '999fc73f-8a11-4a87-b07c-c6fda01a439a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10642ead0>]}
[0m19:43:38.371458 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:43:38.419744 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:43:38.476969 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:43:38.477262 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/example/my_first_dbt_model.sql
[0m19:43:38.614587 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:43:38.619617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '999fc73f-8a11-4a87-b07c-c6fda01a439a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c36d350>]}
[0m19:43:38.656132 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:43:38.657135 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:43:38.666528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '999fc73f-8a11-4a87-b07c-c6fda01a439a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c44b4d0>]}
[0m19:43:38.666691 [info ] [MainThread]: Found 10 models, 4 data tests, 7 sources, 686 macros
[0m19:43:38.666791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '999fc73f-8a11-4a87-b07c-c6fda01a439a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c614910>]}
[0m19:43:38.667549 [info ] [MainThread]: 
[0m19:43:38.667670 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:43:38.667755 [info ] [MainThread]: 
[0m19:43:38.667943 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:43:38.668039 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:43:38.670689 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:43:38.670802 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:43:38.674924 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:43:38.675076 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:43:38.675180 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:43:39.415577 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-b0ec-10a9-9470-03898ea9d22f) - Created
[0m19:43:39.994661 [debug] [ThreadPool]: SQL status: OK in 1.320 seconds
[0m19:43:40.005546 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b6-b0ec-10a9-9470-03898ea9d22f, command-id=01f089b6-b10a-1735-81c0-b60d2e450301) - Closing
[0m19:43:40.006562 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:43:40.006857 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-b0ec-10a9-9470-03898ea9d22f) - Closing
[0m19:43:40.202113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '999fc73f-8a11-4a87-b07c-c6fda01a439a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c46b110>]}
[0m19:43:40.206994 [debug] [Thread-2 (]: Began running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:43:40.207451 [debug] [Thread-4 (]: Began running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:43:40.207792 [debug] [Thread-1 (]: Began running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:43:40.208077 [debug] [Thread-3 (]: Began running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:43:40.208464 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m19:43:40.208842 [info ] [Thread-4 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m19:43:40.209273 [info ] [Thread-1 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m19:43:40.209669 [info ] [Thread-3 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m19:43:40.210301 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m19:43:40.210727 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m19:43:40.211096 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m19:43:40.211433 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m19:43:40.211698 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778'
[0m19:43:40.211949 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493'
[0m19:43:40.212208 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710'
[0m19:43:40.212440 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_first_dbt_model_id.16e066b321'
[0m19:43:40.212761 [debug] [Thread-2 (]: Began compiling node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:43:40.213022 [debug] [Thread-4 (]: Began compiling node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:43:40.213254 [debug] [Thread-1 (]: Began compiling node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:43:40.213484 [debug] [Thread-3 (]: Began compiling node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:43:40.233117 [debug] [Thread-2 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:43:40.234988 [debug] [Thread-4 (]: Writing injected SQL for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:43:40.237670 [debug] [Thread-1 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:43:40.239681 [debug] [Thread-3 (]: Writing injected SQL for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:43:40.240490 [debug] [Thread-4 (]: Began executing node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:43:40.240728 [debug] [Thread-1 (]: Began executing node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:43:40.240933 [debug] [Thread-2 (]: Began executing node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:43:40.247060 [debug] [Thread-3 (]: Began executing node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:43:40.249997 [debug] [Thread-4 (]: Writing runtime sql for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:43:40.251061 [debug] [Thread-1 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:43:40.252220 [debug] [Thread-2 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:43:40.253202 [debug] [Thread-3 (]: Writing runtime sql for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:43:40.253591 [debug] [Thread-2 (]: Using databricks connection "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:43:40.253732 [debug] [Thread-1 (]: Using databricks connection "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:43:40.253920 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:43:40.254078 [debug] [Thread-4 (]: Using databricks connection "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:43:40.254228 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:43:40.254373 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:43:40.254483 [debug] [Thread-3 (]: Using databricks connection "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:43:40.254643 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:43:40.254798 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:43:40.255055 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:43:40.255252 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:43:40.255445 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:43:40.965844 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-b1d8-1173-a043-f36282a5552e) - Created
[0m19:43:40.977461 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-b1d9-1e5e-af3f-41e89275dc16) - Created
[0m19:43:41.007576 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-b1df-17b9-9594-1317ee4a1f8c) - Created
[0m19:43:41.028445 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-b1df-1e6a-8eb7-38810689a9f8) - Created
[0m19:43:41.293201 [debug] [Thread-2 (]: SQL status: OK in 1.040 seconds
[0m19:43:41.298127 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b6-b1d8-1173-a043-f36282a5552e, command-id=01f089b6-b1f5-1f6a-a548-91ffebca95a4) - Closing
[0m19:43:41.302248 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: Close
[0m19:43:41.302718 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-b1d8-1173-a043-f36282a5552e) - Closing
[0m19:43:41.339886 [debug] [Thread-3 (]: SQL status: OK in 1.080 seconds
[0m19:43:41.343087 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b6-b1df-17b9-9594-1317ee4a1f8c, command-id=01f089b6-b1fb-1dce-b713-0be759e71e39) - Closing
[0m19:43:41.374312 [debug] [Thread-4 (]: SQL status: OK in 1.120 seconds
[0m19:43:41.377535 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b6-b1d9-1e5e-af3f-41e89275dc16, command-id=01f089b6-b1fa-1f68-a53f-7ac7228541c3) - Closing
[0m19:43:41.399917 [debug] [Thread-1 (]: SQL status: OK in 1.140 seconds
[0m19:43:41.402584 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b6-b1df-1e6a-8eb7-38810689a9f8, command-id=01f089b6-b1fd-1614-aa87-7dbb820d1f28) - Closing
[0m19:43:41.562118 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: Close
[0m19:43:41.562849 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-b1df-17b9-9594-1317ee4a1f8c) - Closing
[0m19:43:41.767281 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m19:43:41.768343 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-b1d9-1e5e-af3f-41e89275dc16) - Closing
[0m19:43:41.956763 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m19:43:41.957516 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-b1df-1e6a-8eb7-38810689a9f8) - Closing
[0m19:43:42.148789 [info ] [Thread-2 (]: 2 of 4 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 1.94s]
[0m19:43:42.149528 [info ] [Thread-3 (]: 3 of 4 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 1.94s]
[0m19:43:42.151237 [debug] [Thread-2 (]: Finished running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:43:42.150244 [info ] [Thread-4 (]: 4 of 4 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 1.94s]
[0m19:43:42.151975 [debug] [Thread-3 (]: Finished running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:43:42.150736 [error] [Thread-1 (]: 1 of 4 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 1.94s]
[0m19:43:42.153120 [debug] [Thread-4 (]: Finished running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:43:42.153913 [debug] [Thread-1 (]: Finished running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:43:42.156365 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:43:42.156940 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:43:42.157642 [info ] [MainThread]: 
[0m19:43:42.158119 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 3.49 seconds (3.49s).
[0m19:43:42.159342 [debug] [MainThread]: Command end result
[0m19:43:42.194606 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:43:42.195662 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:43:42.199432 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:43:42.199592 [info ] [MainThread]: 
[0m19:43:42.199747 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:43:42.199865 [info ] [MainThread]: 
[0m19:43:42.200098 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m19:43:42.200273 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m19:43:42.200384 [info ] [MainThread]: 
[0m19:43:42.200519 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m19:43:42.200627 [info ] [MainThread]: 
[0m19:43:42.200757 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m19:43:42.203741 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 4.6437445, "process_in_blocks": "0", "process_kernel_time": 0.242969, "process_mem_max_rss": "248184832", "process_out_blocks": "0", "process_user_time": 1.889203}
[0m19:43:42.203944 [debug] [MainThread]: Command `dbt test` failed at 19:43:42.203905 after 4.64 seconds
[0m19:43:42.204114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fd6e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c452570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c452ba0>]}
[0m19:43:42.204269 [debug] [MainThread]: Flushing usage events
[0m19:43:42.762495 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:44:58.537689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e1f620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e3b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e3bb10>]}


============================== 19:44:58.539962 | e0a459f8-7b58-4bf6-82ab-15b05c975f53 ==============================
[0m19:44:58.539962 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:44:58.540186 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'log_format': 'default', 'partial_parse': 'True', 'version_check': 'True', 'cache_selected_only': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'target_path': 'None', 'empty': 'None', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'profiles_dir': '/Users/artakerqeli/.dbt', 'debug': 'False', 'fail_fast': 'False', 'introspect': 'True', 'write_json': 'True', 'invocation_command': 'dbt test', 'indirect_selection': 'eager', 'use_colors': 'True', 'quiet': 'False'}
[0m19:44:58.871071 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:44:58.871292 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:44:58.871451 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:44:59.341827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e0a459f8-7b58-4bf6-82ab-15b05c975f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106167360>]}
[0m19:44:59.362771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e0a459f8-7b58-4bf6-82ab-15b05c975f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108046ad0>]}
[0m19:44:59.363087 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:44:59.415937 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:44:59.475155 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:44:59.475479 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/example/my_first_dbt_model.sql
[0m19:44:59.618849 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:44:59.623948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e0a459f8-7b58-4bf6-82ab-15b05c975f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e075250>]}
[0m19:44:59.660466 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:44:59.661639 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:44:59.671089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e0a459f8-7b58-4bf6-82ab-15b05c975f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e0232f0>]}
[0m19:44:59.671270 [info ] [MainThread]: Found 10 models, 4 data tests, 7 sources, 686 macros
[0m19:44:59.671380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0a459f8-7b58-4bf6-82ab-15b05c975f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e224670>]}
[0m19:44:59.672147 [info ] [MainThread]: 
[0m19:44:59.672265 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:44:59.672349 [info ] [MainThread]: 
[0m19:44:59.672535 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:44:59.672629 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:44:59.675257 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:44:59.675397 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:44:59.680114 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:44:59.680302 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:44:59.680418 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:45:00.507672 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-e141-10dc-ae3a-c445a893f33a) - Created
[0m19:45:01.099333 [debug] [ThreadPool]: SQL status: OK in 1.420 seconds
[0m19:45:01.106325 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b6-e141-10dc-ae3a-c445a893f33a, command-id=01f089b6-e164-1a7c-aa2d-32dbafbd3ceb) - Closing
[0m19:45:01.106738 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:45:01.106860 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-e141-10dc-ae3a-c445a893f33a) - Closing
[0m19:45:01.316081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0a459f8-7b58-4bf6-82ab-15b05c975f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e0332b0>]}
[0m19:45:01.319164 [debug] [Thread-2 (]: Began running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:45:01.319630 [debug] [Thread-4 (]: Began running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:45:01.320036 [debug] [Thread-1 (]: Began running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:45:01.320399 [debug] [Thread-3 (]: Began running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:45:01.320784 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m19:45:01.321285 [info ] [Thread-4 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m19:45:01.321748 [info ] [Thread-1 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m19:45:01.322248 [info ] [Thread-3 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m19:45:01.323050 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m19:45:01.323637 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m19:45:01.324169 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m19:45:01.324713 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m19:45:01.325065 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778'
[0m19:45:01.325361 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493'
[0m19:45:01.325640 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710'
[0m19:45:01.325928 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_first_dbt_model_id.16e066b321'
[0m19:45:01.326246 [debug] [Thread-2 (]: Began compiling node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:45:01.326517 [debug] [Thread-4 (]: Began compiling node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:45:01.326766 [debug] [Thread-1 (]: Began compiling node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:45:01.327026 [debug] [Thread-3 (]: Began compiling node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:45:01.336015 [debug] [Thread-4 (]: Writing injected SQL for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:45:01.339754 [debug] [Thread-2 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:45:01.341353 [debug] [Thread-1 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:45:01.342796 [debug] [Thread-3 (]: Writing injected SQL for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:45:01.343471 [debug] [Thread-3 (]: Began executing node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:45:01.343623 [debug] [Thread-2 (]: Began executing node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:45:01.343748 [debug] [Thread-1 (]: Began executing node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:45:01.349627 [debug] [Thread-4 (]: Began executing node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:45:01.350487 [debug] [Thread-3 (]: Writing runtime sql for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:45:01.351411 [debug] [Thread-2 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:45:01.352376 [debug] [Thread-1 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:45:01.353227 [debug] [Thread-4 (]: Writing runtime sql for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:45:01.353742 [debug] [Thread-3 (]: Using databricks connection "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:45:01.353885 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:45:01.354009 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:45:01.354211 [debug] [Thread-4 (]: Using databricks connection "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:45:01.354323 [debug] [Thread-1 (]: Using databricks connection "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:45:01.354420 [debug] [Thread-2 (]: Using databricks connection "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:45:01.354555 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:45:01.354736 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:45:01.354869 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:45:01.354985 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:45:01.355089 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:45:01.355186 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:45:02.096208 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-e235-1269-8d79-3eeb4c930169) - Created
[0m19:45:02.132963 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-e23b-1100-86f5-483cf9e7eb4e) - Created
[0m19:45:02.193741 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-e23f-1c20-b603-a8f61b781ce5) - Created
[0m19:45:02.203632 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-e242-1680-a78e-7bb33f4bf67d) - Created
[0m19:45:02.425950 [debug] [Thread-4 (]: SQL status: OK in 1.070 seconds
[0m19:45:02.431043 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b6-e235-1269-8d79-3eeb4c930169, command-id=01f089b6-e24f-191d-8f98-7128c4bc45b5) - Closing
[0m19:45:02.434819 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m19:45:02.435301 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-e235-1269-8d79-3eeb4c930169) - Closing
[0m19:45:02.505404 [debug] [Thread-3 (]: SQL status: OK in 1.150 seconds
[0m19:45:02.508353 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b6-e23b-1100-86f5-483cf9e7eb4e, command-id=01f089b6-e256-11cc-b411-156903b4104e) - Closing
[0m19:45:02.576720 [debug] [Thread-1 (]: SQL status: OK in 1.220 seconds
[0m19:45:02.579568 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b6-e23f-1c20-b603-a8f61b781ce5, command-id=01f089b6-e25d-1601-87e8-078199587b24) - Closing
[0m19:45:02.621878 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: Close
[0m19:45:02.622855 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-e23b-1100-86f5-483cf9e7eb4e) - Closing
[0m19:45:02.645897 [debug] [Thread-2 (]: SQL status: OK in 1.290 seconds
[0m19:45:02.648645 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b6-e242-1680-a78e-7bb33f4bf67d, command-id=01f089b6-e261-1384-ae9a-1a049f15ad00) - Closing
[0m19:45:02.847227 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m19:45:02.848266 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-e23f-1c20-b603-a8f61b781ce5) - Closing
[0m19:45:03.080747 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: Close
[0m19:45:03.079884 [info ] [Thread-4 (]: 4 of 4 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 1.76s]
[0m19:45:03.081865 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-e242-1680-a78e-7bb33f4bf67d) - Closing
[0m19:45:03.082975 [debug] [Thread-4 (]: Finished running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:45:03.347495 [info ] [Thread-3 (]: 3 of 4 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 2.02s]
[0m19:45:03.348460 [error] [Thread-1 (]: 1 of 4 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 2.02s]
[0m19:45:03.349976 [debug] [Thread-3 (]: Finished running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:45:03.349310 [info ] [Thread-2 (]: 2 of 4 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 2.03s]
[0m19:45:03.350678 [debug] [Thread-1 (]: Finished running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:45:03.351511 [debug] [Thread-2 (]: Finished running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:45:03.353397 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:45:03.353765 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:45:03.354314 [info ] [MainThread]: 
[0m19:45:03.354689 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 3.68 seconds (3.68s).
[0m19:45:03.356119 [debug] [MainThread]: Command end result
[0m19:45:03.394019 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:45:03.395018 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:45:03.398439 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:45:03.398579 [info ] [MainThread]: 
[0m19:45:03.398738 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:45:03.398856 [info ] [MainThread]: 
[0m19:45:03.399004 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m19:45:03.399136 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m19:45:03.399235 [info ] [MainThread]: 
[0m19:45:03.399356 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m19:45:03.399458 [info ] [MainThread]: 
[0m19:45:03.399585 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m19:45:03.402722 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 4.897634, "process_in_blocks": "0", "process_kernel_time": 0.259256, "process_mem_max_rss": "235077632", "process_out_blocks": "0", "process_user_time": 2.038647}
[0m19:45:03.402910 [debug] [MainThread]: Command `dbt test` failed at 19:45:03.402872 after 4.90 seconds
[0m19:45:03.403077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106beee10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e02a620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e02aa40>]}
[0m19:45:03.403217 [debug] [MainThread]: Flushing usage events
[0m19:45:04.100068 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:45:47.362120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11222b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1139ff890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1139ffb10>]}


============================== 19:45:47.364391 | c518e4c6-73c8-4dcd-99fd-71a9d2f02931 ==============================
[0m19:45:47.364391 [info ] [MainThread]: Running with dbt=1.10.9
[0m19:45:47.364615 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'static_parser': 'True', 'log_path': '/Users/artakerqeli/flights_dbt/flights_dbt/logs', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt test', 'log_format': 'default', 'log_cache_events': 'False', 'write_json': 'True', 'no_print': 'None', 'introspect': 'True', 'profiles_dir': '/Users/artakerqeli/.dbt', 'use_experimental_parser': 'False', 'debug': 'False', 'target_path': 'None', 'quiet': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True', 'version_check': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m19:45:47.665358 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:45:47.665540 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:45:47.665642 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:45:48.110393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c518e4c6-73c8-4dcd-99fd-71a9d2f02931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112927360>]}
[0m19:45:48.130186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c518e4c6-73c8-4dcd-99fd-71a9d2f02931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115902ad0>]}
[0m19:45:48.130459 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m19:45:48.184976 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m19:45:48.244405 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:45:48.244700 [debug] [MainThread]: Partial parsing: updated file: flights_dbt://models/example/my_first_dbt_model.sql
[0m19:45:48.416996 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.flights_dbt.raw
[0m19:45:48.422232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c518e4c6-73c8-4dcd-99fd-71a9d2f02931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249d5250>]}
[0m19:45:48.460087 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:45:48.462422 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:45:48.472837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c518e4c6-73c8-4dcd-99fd-71a9d2f02931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249832f0>]}
[0m19:45:48.473074 [info ] [MainThread]: Found 10 models, 4 data tests, 7 sources, 686 macros
[0m19:45:48.473192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c518e4c6-73c8-4dcd-99fd-71a9d2f02931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124b6c670>]}
[0m19:45:48.474018 [info ] [MainThread]: 
[0m19:45:48.474175 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:45:48.474274 [info ] [MainThread]: 
[0m19:45:48.474487 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:45:48.474591 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:45:48.477594 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_flight_db_raw) - Creating connection
[0m19:45:48.477795 [debug] [ThreadPool]: Acquiring new databricks connection 'list_flight_db_raw'
[0m19:45:48.482383 [debug] [ThreadPool]: Using databricks connection "list_flight_db_raw"
[0m19:45:48.482567 [debug] [ThreadPool]: On list_flight_db_raw: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "connection_name": "list_flight_db_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'flight_db' 
  AND table_schema = 'raw'

  
[0m19:45:48.482687 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:45:49.156360 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-fe40-10cd-b5d1-410bd711b493) - Created
[0m19:45:49.783188 [debug] [ThreadPool]: SQL status: OK in 1.300 seconds
[0m19:45:49.794042 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f089b6-fe40-10cd-b5d1-410bd711b493, command-id=01f089b6-fe5f-1f4a-bc76-5252d58ba403) - Closing
[0m19:45:49.794710 [debug] [ThreadPool]: On list_flight_db_raw: Close
[0m19:45:49.794924 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f089b6-fe40-10cd-b5d1-410bd711b493) - Closing
[0m19:45:49.995189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c518e4c6-73c8-4dcd-99fd-71a9d2f02931', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249a32b0>]}
[0m19:45:50.001285 [debug] [Thread-2 (]: Began running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:45:50.002074 [debug] [Thread-1 (]: Began running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:45:50.002581 [debug] [Thread-3 (]: Began running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:45:50.003011 [debug] [Thread-4 (]: Began running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:45:50.003490 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m19:45:50.004002 [info ] [Thread-1 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m19:45:50.004444 [info ] [Thread-3 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m19:45:50.004952 [info ] [Thread-4 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m19:45:50.005836 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m19:45:50.006542 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m19:45:50.007082 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m19:45:50.007610 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m19:45:50.008036 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778'
[0m19:45:50.008419 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710'
[0m19:45:50.008813 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_first_dbt_model_id.16e066b321'
[0m19:45:50.009173 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493'
[0m19:45:50.009598 [debug] [Thread-2 (]: Began compiling node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:45:50.009935 [debug] [Thread-1 (]: Began compiling node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:45:50.010265 [debug] [Thread-3 (]: Began compiling node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:45:50.010637 [debug] [Thread-4 (]: Began compiling node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:45:50.027359 [debug] [Thread-1 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:45:50.031676 [debug] [Thread-2 (]: Writing injected SQL for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:45:50.037122 [debug] [Thread-3 (]: Writing injected SQL for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:45:50.039380 [debug] [Thread-4 (]: Writing injected SQL for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:45:50.040305 [debug] [Thread-4 (]: Began executing node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:45:50.046980 [debug] [Thread-3 (]: Began executing node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:45:50.052722 [debug] [Thread-2 (]: Began executing node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:45:50.055622 [debug] [Thread-4 (]: Writing runtime sql for node "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:45:50.056036 [debug] [Thread-3 (]: Writing runtime sql for node "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:45:50.057299 [debug] [Thread-2 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:45:50.057469 [debug] [Thread-1 (]: Began executing node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:45:50.058659 [debug] [Thread-1 (]: Writing runtime sql for node "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:45:50.058930 [debug] [Thread-2 (]: Using databricks connection "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"
[0m19:45:50.059093 [debug] [Thread-4 (]: Using databricks connection "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:45:50.059225 [debug] [Thread-3 (]: Using databricks connection "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"
[0m19:45:50.059378 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:45:50.059559 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:45:50.059686 [debug] [Thread-1 (]: Using databricks connection "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:45:50.059845 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `flight_db`.`raw`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:45:50.060004 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:45:50.060129 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:45:50.060275 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.5", "profile_name": "flights_dbt", "target_name": "dev", "node_id": "test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `flight_db`.`raw`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m19:45:50.060416 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:45:50.060719 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:45:50.785886 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-ff3b-1025-accf-9e8902234907) - Created
[0m19:45:50.813903 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-ff3d-1ed3-bf32-4c402d1b0f08) - Created
[0m19:45:50.816982 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-ff40-14b6-a42c-7f9520d7d4bc) - Created
[0m19:45:50.818705 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-ff40-12ff-9757-72da3b254fd4) - Created
[0m19:45:51.112508 [debug] [Thread-3 (]: SQL status: OK in 1.050 seconds
[0m19:45:51.118620 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f089b6-ff3b-1025-accf-9e8902234907, command-id=01f089b6-ff55-145a-a740-88dca4759595) - Closing
[0m19:45:51.123245 [debug] [Thread-3 (]: On test.flights_dbt.unique_my_first_dbt_model_id.16e066b321: Close
[0m19:45:51.123833 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f089b6-ff3b-1025-accf-9e8902234907) - Closing
[0m19:45:51.148549 [debug] [Thread-1 (]: SQL status: OK in 1.090 seconds
[0m19:45:51.152043 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f089b6-ff40-14b6-a42c-7f9520d7d4bc, command-id=01f089b6-ff5b-1102-b1a7-b7d910c51c35) - Closing
[0m19:45:51.202181 [debug] [Thread-2 (]: SQL status: OK in 1.140 seconds
[0m19:45:51.205561 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f089b6-ff40-12ff-9757-72da3b254fd4, command-id=01f089b6-ff5a-1e68-9460-e0cd936ffccf) - Closing
[0m19:45:51.252806 [debug] [Thread-4 (]: SQL status: OK in 1.190 seconds
[0m19:45:51.256086 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f089b6-ff3d-1ed3-bf32-4c402d1b0f08, command-id=01f089b6-ff5d-10f4-a26e-39e321706f7c) - Closing
[0m19:45:51.309005 [debug] [Thread-1 (]: On test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m19:45:51.310214 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f089b6-ff40-14b6-a42c-7f9520d7d4bc) - Closing
[0m19:45:51.513936 [debug] [Thread-2 (]: On test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778: Close
[0m19:45:51.514786 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f089b6-ff40-12ff-9757-72da3b254fd4) - Closing
[0m19:45:51.718724 [debug] [Thread-4 (]: On test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m19:45:51.719733 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f089b6-ff3d-1ed3-bf32-4c402d1b0f08) - Closing
[0m19:45:51.909694 [info ] [Thread-3 (]: 3 of 4 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 1.90s]
[0m19:45:51.910719 [error] [Thread-1 (]: 1 of 4 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 1.90s]
[0m19:45:51.912654 [debug] [Thread-3 (]: Finished running node test.flights_dbt.unique_my_first_dbt_model_id.16e066b321
[0m19:45:51.911612 [info ] [Thread-2 (]: 2 of 4 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 1.91s]
[0m19:45:51.913411 [debug] [Thread-1 (]: Finished running node test.flights_dbt.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:45:51.912113 [info ] [Thread-4 (]: 4 of 4 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 1.90s]
[0m19:45:51.914366 [debug] [Thread-2 (]: Finished running node test.flights_dbt.not_null_my_second_dbt_model_id.151b76d778
[0m19:45:51.914959 [debug] [Thread-4 (]: Finished running node test.flights_dbt.unique_my_second_dbt_model_id.57a0f8c493
[0m19:45:51.916800 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:45:51.917158 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:45:51.917626 [info ] [MainThread]: 
[0m19:45:51.917943 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 3.44 seconds (3.44s).
[0m19:45:51.919006 [debug] [MainThread]: Command end result
[0m19:45:51.954216 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/manifest.json
[0m19:45:51.955215 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/artakerqeli/flights_dbt/flights_dbt/target/semantic_manifest.json
[0m19:45:51.958951 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/artakerqeli/flights_dbt/flights_dbt/target/run_results.json
[0m19:45:51.959120 [info ] [MainThread]: 
[0m19:45:51.959291 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:45:51.959412 [info ] [MainThread]: 
[0m19:45:51.959571 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m19:45:51.959714 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m19:45:51.959817 [info ] [MainThread]: 
[0m19:45:51.959946 [info ] [MainThread]:   compiled code at target/compiled/flights_dbt/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m19:45:51.960051 [info ] [MainThread]: 
[0m19:45:51.960183 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m19:45:51.962621 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 4.631661, "process_in_blocks": "0", "process_kernel_time": 0.260522, "process_mem_max_rss": "232423424", "process_out_blocks": "0", "process_user_time": 2.00352}
[0m19:45:51.962812 [debug] [MainThread]: Command `dbt test` failed at 19:45:51.962772 after 4.63 seconds
[0m19:45:51.962974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1135b2e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12498a570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12498a830>]}
[0m19:45:51.963118 [debug] [MainThread]: Flushing usage events
[0m19:45:52.491465 [debug] [MainThread]: An error was encountered while trying to flush usage events
